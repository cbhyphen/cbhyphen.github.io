<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="me">
<meta name="dcterms.date" content="2023-11-15">

<title>cbhyphen.github.io - Quantizing an Object Detection Model in PyTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">cbhyphen.github.io</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">about</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cbhyphen" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Quantizing an Object Detection Model in PyTorch</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>me </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 15, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>At the time of writing this, quantization in PyTorch was relatively new to me. After reading about the advantages of quantization aware training, I wanted a deep dive on eager mode quantization and for something non-trivial. In my research I came across <a href="https://github.com/pytorch/pytorch/issues/31316">multiple discussions</a> online requesting either <a href="https://discuss.pytorch.org/t/slow-inference-time-on-quantized-faster-rcnn-model/115182">help</a> or <a href="https://discuss.pytorch.org/t/tutorial-on-quantizing-object-detection-model/102604">a tutorial</a> for quantizing the backbone of an object detection model (faster R-CNN in this case). As far as I could tell there was nothing out there and it got me wondering. Are there no tutorials because it’s too difficult or maybe not worth it? Surely there is some advantage with respect to memory requirements or latency (although maybe not better than GPU). In any case, this was the perfect excuse for a deep dive.</p>
<p>It’s worth mentioning that I ran into all sorts of issues on my early attempts. This post is polished and makes the whole process look linear but it really wasn’t. There were many attempts, breaks to research bugs, figuring out how to do a thing, reverting to simpler models, etc. I failed and failed, then finally began to understand things, and only then did it work.</p>
<section id="network-modifications" class="level3">
<h3 class="anchored" data-anchor-id="network-modifications">Network Modifications</h3>
<p>The first step in setting up the network for quantization is to create a modified <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L108">bottleneck block</a>. This isn’t obvious until you try to quantize the ResNet without it. You will get an error <code>.. out += identity .. Could not run 'aten::add.out' ..</code> which means that PyTorch isn’t able to quantize the skip connection using the <code>+=</code> operator in eager mode. <a href="https://discuss.pytorch.org/t/quantizing-an-existing-object-detector-with-resnet-backbone/134627">This discussion on the pytorch forums</a> was helpful for describing the error as well as how to fix it. The modified bottleneck block just uses <a href="https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.FloatFunctional.html">FloatFunctional</a> which has a quantized addition operator. I’m using ResNet 101 here but for much smaller networks you would want to modify the basic block. Also, the original bottleneck class reuses the ReLU layer which won’t work when fusing. Finding <a href="https://leimao.github.io/blog/PyTorch-Static-Quantization/">this blog post</a> about quantizing ResNet was helpful for realizing and avoiding that pitfall.</p>
<div class="cell" data-execution_count="244">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Any, Callable, List, Optional, Type, Union</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> Tensor</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.resnet <span class="im">import</span> conv1x1, conv3x3</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.ao.nn.quantized <span class="im">import</span> FloatFunctional</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BottleneckFloatFunctional(nn.Module):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># while original implementation places the stride at the first 1x1 convolution(self.conv1)</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># according to "Deep residual learning for image recognition" https://arxiv.org/abs/1512.03385.</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This variant is also known as ResNet V1.5 and improves accuracy according to</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    expansion: <span class="bu">int</span> <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        inplanes: <span class="bu">int</span>,</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        planes: <span class="bu">int</span>,</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        stride: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        downsample: Optional[nn.Module] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        groups: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        base_width: <span class="bu">int</span> <span class="op">=</span> <span class="dv">64</span>,</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        dilation: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        norm_layer: Optional[Callable[..., nn.Module]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> norm_layer <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>            norm_layer <span class="op">=</span> nn.BatchNorm2d</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        width <span class="op">=</span> <span class="bu">int</span>(planes <span class="op">*</span> (base_width <span class="op">/</span> <span class="fl">64.0</span>)) <span class="op">*</span> groups</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Both self.conv2 and self.downsample layers downsample the input when stride != 1</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> conv1x1(inplanes, width)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> norm_layer(width)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu1 <span class="op">=</span> nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> conv3x3(width, width, stride, groups, dilation)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> norm_layer(width)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu2 <span class="op">=</span> nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> conv1x1(width, planes <span class="op">*</span> <span class="va">self</span>.expansion)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn3 <span class="op">=</span> norm_layer(planes <span class="op">*</span> <span class="va">self</span>.expansion)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu3 <span class="op">=</span> nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.downsample <span class="op">=</span> downsample</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stride <span class="op">=</span> stride</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.f_add <span class="op">=</span> nn.quantized.FloatFunctional()</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        identity <span class="op">=</span> x</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn1(out)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu1(out)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv2(out)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn2(out)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu2(out)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv3(out)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn3(out)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.downsample <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>            identity <span class="op">=</span> <span class="va">self</span>.downsample(x)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.f_add.add(out, identity)</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu3(out)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can simply plug in that class to generate the ResNet. Even though the float functional operator was added, we can still load pretrained imagenet weights since the weights/submodules didn’t change. Note that the number of classes for the ResNet don’t matter here because we will extract intermediate layers and not the final fully connected layer (more on that ahead).</p>
<div class="cell" data-outputid="79cf93b3-b00d-4cf8-97eb-34627efa255e" data-execution_count="245">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.resnet <span class="im">import</span> ResNet, ResNet50_Weights, ResNet101_Weights</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> resnet101_ff():</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ResNet(block<span class="op">=</span>BottleneckFloatFunctional, layers<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">23</span>, <span class="dv">3</span>])</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>res101_backbone_ff <span class="op">=</span> resnet101_ff()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>res101_backbone_ff.load_state_dict(ResNet101_Weights.DEFAULT.get_state_dict(progress<span class="op">=</span><span class="va">True</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="245">
<pre><code>&lt;All keys matched successfully&gt;</code></pre>
</div>
</div>
<p>I’m assuming some familiarity with the R-CNN architecture but to refresh, the feature pyramid network uses output from consecutive layers of the ResNet backbone to extract semantically rich information at different resolutions. Torchvision has a <a href="https://github.com/pytorch/vision/blob/main/torchvision/ops/feature_pyramid_network.py#L36">convenience class for the FPN</a> that takes as input an OrderedDict containing the output of backbone layers (no final fully connected layer).</p>
<p>To get the layer outputs from the backbone, we can use a utility class <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/_utils.py#L13">IntermediateLayerGetter</a> (type <code>nn.ModuleDict</code>) that returns an <code>OrderedDict</code> of layer outputs given the layer names. There is also a <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/detection/backbone_utils.py#L13">convenience class</a> <code>BackboneWithFPN</code> which uses the intermediate layer getter but creating the FPN that way doesn’t allow the backbone to be quantized.</p>
<p>As an aside, it looks like the control flow (for loop) in <code>IntermediateLayerGetter</code> is one reason that the network is <a href="https://pytorch.org/docs/stable/fx.html#dynamic-control-flow">not symbolically traceable</a> and why graph FX mode quantization can’t be used here.</p>
<div class="cell" data-execution_count="246">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models._utils <span class="im">import</span> IntermediateLayerGetter</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>returned_layers <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]  <span class="co"># get all 4 layers</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>return_layers <span class="op">=</span> {<span class="ss">f"layer</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>: <span class="bu">str</span>(v) <span class="cf">for</span> v, k <span class="kw">in</span> <span class="bu">enumerate</span>(returned_layers)}  <span class="co"># {'layer1': 0, 'layer2': 1, ...}</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>res101_backbone_layers <span class="op">=</span> IntermediateLayerGetter(res101_backbone_ff, return_layers<span class="op">=</span>return_layers)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here’s an example of what the intermediate output looks like for a toy image.</p>
<div class="cell" data-outputid="82fa1733-3028-4069-cd13-5780c8203183" data-execution_count="247">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> res101_backbone_layers(torch.rand(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">200</span>, <span class="dv">200</span>))  <span class="co"># e.g. 200 x 200 image with 3 channels</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>[(k, v.shape) <span class="cf">for</span> k, v <span class="kw">in</span> out.items()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="247">
<pre><code>[('0', torch.Size([1, 256, 50, 50])),
 ('1', torch.Size([1, 512, 25, 25])),
 ('2', torch.Size([1, 1024, 13, 13])),
 ('3', torch.Size([1, 2048, 7, 7]))]</code></pre>
</div>
</div>
<p>Wrapping the module dict backbone in sequential layer with quant/dequant stubs causes issues downstream when quantizing, so we need to create an <code>nn.Module</code> wrapper class manually.</p>
<div class="cell" data-execution_count="248">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Callable, Dict, List, Optional, Union</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.ops.feature_pyramid_network <span class="im">import</span> ExtraFPNBlock, FeaturePyramidNetwork, LastLevelMaxPool</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QuantLayers(torch.nn.Module):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, layers_module_dict: torch.nn.ModuleDict):</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.quant <span class="op">=</span> torch.ao.quantization.QuantStub()</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dequant <span class="op">=</span> torch.ao.quantization.DeQuantStub()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> layers_module_dict</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.quant(x)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.layers(x)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k, v <span class="kw">in</span> out.items():</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>            out[k] <span class="op">=</span> <span class="va">self</span>.dequant(v)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next step is to create a modified backbone with FPN. This follows the <code>BackboneWithFPN</code> class but just uses the wrapped module dict above with quantized inputs and dequantized outputs so they can be fed to the FPN. The feature pyramid network also needs to know the exact dimensions of each output from the resnet backbone so we’ll get that below.</p>
<div class="cell" data-execution_count="249">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QuantBackboneWithFPN(nn.Module):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        backbone: nn.Module,</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        return_layers: Dict[<span class="bu">str</span>, <span class="bu">str</span>],</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        in_channels_list: List[<span class="bu">int</span>],</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        out_channels: <span class="bu">int</span>,</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        extra_blocks: Optional[ExtraFPNBlock] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        norm_layer: Optional[Callable[..., nn.Module]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> extra_blocks <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>            extra_blocks <span class="op">=</span> LastLevelMaxPool()</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.body <span class="op">=</span> QuantLayers(</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>            IntermediateLayerGetter(backbone, return_layers<span class="op">=</span>return_layers)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fpn <span class="op">=</span> FeaturePyramidNetwork(</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>            in_channels_list<span class="op">=</span>in_channels_list,</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>out_channels,</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>            extra_blocks<span class="op">=</span>extra_blocks,</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>            norm_layer<span class="op">=</span>norm_layer,</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_channels <span class="op">=</span> out_channels</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Tensor) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Tensor]:</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.body(x)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fpn(x)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="5acce49d-09f9-4f8f-fcf9-d3568c7a90bc" data-execution_count="250">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">#  there are several ways to get these dimensions</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># from backbone_utils.py: https://github.com/pytorch/vision/blob/main/torchvision/models/detection/backbone_utils.py#L145</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># in_channels_stage2 = res101_backbone_layers.inplanes // 8</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># in_channels_list = [in_channels_stage2 * 2 ** (i - 1) for i in returned_layers]</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>in_channels_list <span class="op">=</span> []</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k1, m1 <span class="kw">in</span> res101_backbone_layers.named_children():</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="st">'layer'</span> <span class="kw">in</span> k1:</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    in_channels_list.append((m1[<span class="op">-</span><span class="dv">1</span>].bn3.num_features))</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>in_channels_list</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="250">
<pre><code>[256, 512, 1024, 2048]</code></pre>
</div>
</div>
<p>Another important note is that regular <code>BatchNorm2d</code> is used here instead of <code>FrozenBatchNorm2d</code> because the latter isn’t quantizeable. Frozen batch norm is the recommended layer because batches are generally <a href="https://github.com/facebookresearch/maskrcnn-benchmark/issues/267">too small for good estimates of mean and variance</a> statistics. So using regular batch norm could be unstable and less performant if those layers aren’t frozen.</p>
<div class="cell" data-execution_count="251">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>res101_bb_fpn <span class="op">=</span> QuantBackboneWithFPN(</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    backbone<span class="op">=</span>res101_backbone_layers,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    return_layers<span class="op">=</span>return_layers,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    in_channels_list<span class="op">=</span>in_channels_list,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    out_channels<span class="op">=</span><span class="dv">256</span>,  <span class="co"># defined</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    extra_blocks<span class="op">=</span><span class="va">None</span>,  <span class="co"># defaults to LastLevelMaxPool, see above</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    norm_layer<span class="op">=</span><span class="va">None</span>,  <span class="co"># defaults to torch.nn.BatchNorm2d, see FeaturePyramidNetwork</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># res101_bb_fpn</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>At this point there should be quant/dequant stubs in the network similar to below.</p>
<pre><code>QuantBackboneWithFPN(
  (body): QuantLayers(
    (quant): QuantStub()
    (dequant): DeQuantStub()
    (layers): IntermediateLayerGetter(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      ...</code></pre>
<p>Once the backbone with FPN is created, it can be plugged into the <code>FasterRCNN</code> <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py#L43">torchvision class</a>. The number of classes is set to 2 for object or background (specific to the dataset used here).</p>
<div class="cell" data-execution_count="252">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.detection.faster_rcnn <span class="im">import</span> FasterRCNN</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>quant_rcnn_res101 <span class="op">=</span> FasterRCNN(res101_bb_fpn, num_classes<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># quant_rcnn_res101</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="layer-fusion-and-quantization-config" class="level3">
<h3 class="anchored" data-anchor-id="layer-fusion-and-quantization-config">Layer Fusion and Quantization Config</h3>
<p>Before training and subsequently converting the model, we can fuse specific sequences of modules in the backbone. Fusing compresses the model <a href="https://pytorch.org/tutorials/recipes/fuse.html">making it smaller and run faster</a>. Below sequences of <code>Conv2d-BatchNorm2d-ReLU</code> and <code>Conv2d-BatchNorm2d</code> are fused. After fusing you should see new fused modules in the network (shown below) like <code>ConvReLU2d</code> as well as <code>Identity</code>.</p>
<pre><code>(backbone): QuantBackboneWithFPN(
    (body): QuantLayers(
      (quant): QuantStub()
      (dequant): DeQuantStub()
      (layers): IntermediateLayerGetter(
        (conv1): ConvReLU2d(
          (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
          (1): ReLU(inplace=True)
        )
        (bn1): Identity()
        (relu): Identity()
        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        (layer1): Sequential(
          (0): BottleneckFloatFunctional(
            (conv1): ConvReLU2d(
              (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): ReLU(inplace=True)
            )
            (bn1): Identity()
            (relu1): Identity()
            ...</code></pre>
<div class="cell" data-execution_count="253">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.ao.quantization <span class="im">import</span> fuse_modules</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>quant_rcnn_res101.<span class="bu">eval</span>()</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># fuse stem</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>fuse_modules(quant_rcnn_res101.backbone.body.layers, [[<span class="st">'conv1'</span>, <span class="st">'bn1'</span>, <span class="st">'relu'</span>]], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># fuse blocks</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k1, m1 <span class="kw">in</span> quant_rcnn_res101.backbone.body.layers.named_children():</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"layer"</span> <span class="kw">in</span> k1:  <span class="co"># in sequential layer with blocks</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k2, m2 <span class="kw">in</span> m1.named_children():</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>            fuse_modules(m2, [[<span class="st">"conv1"</span>, <span class="st">"bn1"</span>, <span class="st">"relu1"</span>], [<span class="st">"conv2"</span>, <span class="st">"bn2"</span>, <span class="st">"relu2"</span>], [<span class="st">"conv3"</span>, <span class="st">"bn3"</span>]], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k3, m3 <span class="kw">in</span> m2.named_children():</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="st">"downsample"</span> <span class="kw">in</span> k3:  <span class="co"># fuse downsample</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>                    fuse_modules(m3, [[<span class="st">"0"</span>, <span class="st">"1"</span>]], inplace<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Before training the qconfig needs to be set but on the resnet backbone only. And again, because the batches are so small, batch norm gets frozen using the handy API found in <a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#quantization-aware-training">this PyTorch tutorial</a>. Last, I’ll freeze the stem and the first layer in the backbone since the pretrained imagenet weights were loaded. After preparation you should be able to see the observers in the network.</p>
<pre><code>(backbone): QuantBackboneWithFPN(
    (body): QuantLayers(
      (quant): QuantStub(
        (activation_post_process): FusedMovingAvgObsFakeQuantize(
          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
      (dequant): DeQuantStub()
      (layers): IntermediateLayerGetter(
        (conv1): ConvReLU2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)
          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(
            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False
            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))
          )
          (activation_post_process): FusedMovingAvgObsFakeQuantize(
            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
        (bn1): Identity()
        (relu): Identity()
        ...</code></pre>
<div class="cell" data-execution_count="254">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># for backend in torch.backends.quantized.supported_engines:</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">#     print(backend)</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>quant_rcnn_res101.train()</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>quant_rcnn_res101.backbone.body.qconfig <span class="op">=</span> torch.ao.quantization.get_default_qat_qconfig(<span class="st">'fbgemm'</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>quant_rcnn_res101_prepared <span class="op">=</span> torch.ao.quantization.prepare_qat(quant_rcnn_res101, inplace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>quant_rcnn_res101_prepared <span class="op">=</span> quant_rcnn_res101_prepared.<span class="bu">apply</span>(torch.nn.intrinsic.qat.freeze_bn_stats)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>quant_rcnn_res101_prepared.backbone.body.layers.conv1.weight.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, parameter <span class="kw">in</span> quant_rcnn_res101_prepared.backbone.body.named_parameters():</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> re.search(<span class="vs">r".layer1"</span>, name):</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>        parameter.requires_grad <span class="op">=</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="dataset-training-and-conversion" class="level3">
<h3 class="anchored" data-anchor-id="dataset-training-and-conversion">Dataset, Training, and Conversion</h3>
<p>I’ll be using the <a href="https://www.cis.upenn.edu/~jshi/ped_html/">PennFudan dataset</a> from the <a href="https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html">Torchvision object detection finetuning tutorial</a> for QAT. Most of the code below is borrowed from that tutorial with slight modifications since I don’t perform segmentation.</p>
<div class="cell" data-execution_count="255">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.io <span class="im">import</span> read_image</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.ops.boxes <span class="im">import</span> masks_to_boxes</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> tv_tensors</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms.v2 <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> v2 <span class="im">as</span> T</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PennFudanDataset(torch.utils.data.Dataset):</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, root, transforms):</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.root <span class="op">=</span> root</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transforms <span class="op">=</span> transforms</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># load all image files, sorting them to</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ensure that they are aligned</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.imgs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">sorted</span>(os.listdir(os.path.join(root, <span class="st">"PNGImages"</span>))))</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.masks <span class="op">=</span> <span class="bu">list</span>(<span class="bu">sorted</span>(os.listdir(os.path.join(root, <span class="st">"PedMasks"</span>))))</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># load images and masks</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        img_path <span class="op">=</span> os.path.join(<span class="va">self</span>.root, <span class="st">"PNGImages"</span>, <span class="va">self</span>.imgs[idx])</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        mask_path <span class="op">=</span> os.path.join(<span class="va">self</span>.root, <span class="st">"PedMasks"</span>, <span class="va">self</span>.masks[idx])</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> read_image(img_path)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> read_image(mask_path)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># instances are encoded as different colors</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>        obj_ids <span class="op">=</span> torch.unique(mask)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># first id is the background, so remove it</span></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>        obj_ids <span class="op">=</span> obj_ids[<span class="dv">1</span>:]</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>        num_objs <span class="op">=</span> <span class="bu">len</span>(obj_ids)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># split the color-encoded mask into a set</span></span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># of binary masks</span></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>        masks <span class="op">=</span> (mask <span class="op">==</span> obj_ids[:, <span class="va">None</span>, <span class="va">None</span>]).to(dtype<span class="op">=</span>torch.uint8)</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get bounding box coordinates for each mask</span></span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>        boxes <span class="op">=</span> masks_to_boxes(masks)</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># there is only one class</span></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> torch.ones((num_objs,), dtype<span class="op">=</span>torch.int64)</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>        image_id <span class="op">=</span> idx</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>        area <span class="op">=</span> (boxes[:, <span class="dv">3</span>] <span class="op">-</span> boxes[:, <span class="dv">1</span>]) <span class="op">*</span> (boxes[:, <span class="dv">2</span>] <span class="op">-</span> boxes[:, <span class="dv">0</span>])</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># suppose all instances are not crowd</span></span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>        iscrowd <span class="op">=</span> torch.zeros((num_objs,), dtype<span class="op">=</span>torch.int64)</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Wrap sample and targets into torchvision tv_tensors:</span></span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> tv_tensors.Image(img)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> {}</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>        target[<span class="st">"boxes"</span>] <span class="op">=</span> tv_tensors.BoundingBoxes(boxes, <span class="bu">format</span><span class="op">=</span><span class="st">"XYXY"</span>, canvas_size<span class="op">=</span>F.get_size(img))</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>        target[<span class="st">"masks"</span>] <span class="op">=</span> tv_tensors.Mask(masks)</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>        target[<span class="st">"labels"</span>] <span class="op">=</span> labels</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>        target[<span class="st">"image_id"</span>] <span class="op">=</span> image_id</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>        target[<span class="st">"area"</span>] <span class="op">=</span> area</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>        target[<span class="st">"iscrowd"</span>] <span class="op">=</span> iscrowd</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.transforms <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>            img, target <span class="op">=</span> <span class="va">self</span>.transforms(img, target)</span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> img, target</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.imgs)</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_transform(train):</span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a>    transforms <span class="op">=</span> []</span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> train:</span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>        transforms.append(T.RandomHorizontalFlip(<span class="fl">0.5</span>))</span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a>    transforms.append(T.ToDtype(torch.<span class="bu">float</span>, scale<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>    transforms.append(T.ToPureTensor())</span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> T.Compose(transforms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="105">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>os.system(<span class="st">"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py"</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>os.system(<span class="st">"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py"</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>os.system(<span class="st">"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py"</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>os.system(<span class="st">"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py"</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>os.system(<span class="st">"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py"</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget https:<span class="op">//</span>www.cis.upenn.edu<span class="op">/~</span>jshi<span class="op">/</span>ped_html<span class="op">/</span>PennFudanPed.<span class="bu">zip</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>unzip PennFudanPed.<span class="bu">zip</span> <span class="op">-</span>d .<span class="op">/</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="256">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> utils</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> engine <span class="im">import</span> train_one_epoch, evaluate</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co"># train on the GPU or on the CPU, if a GPU is not available</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span>) <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> torch.device(<span class="st">'cpu'</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># our dataset has two classes only - background and person</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co"># use our dataset and defined transformations</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> PennFudanDataset(<span class="st">'PennFudanPed'</span>, get_transform(train<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>dataset_test <span class="op">=</span> PennFudanDataset(<span class="st">'PennFudanPed'</span>, get_transform(train<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="co"># split the dataset in train and test set</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> torch.randperm(<span class="bu">len</span>(dataset)).tolist()</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> torch.utils.data.Subset(dataset, indices[:<span class="op">-</span><span class="dv">50</span>])</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>dataset_test <span class="op">=</span> torch.utils.data.Subset(dataset_test, indices[<span class="op">-</span><span class="dv">50</span>:])</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="co"># define training and validation data loaders</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    dataset,</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    collate_fn<span class="op">=</span>utils.collate_fn</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>data_loader_test <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>    dataset_test,</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>    collate_fn<span class="op">=</span>utils.collate_fn</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="8a662a4f-be5e-416c-ed31-e3847da6c976" data-execution_count="257">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># move model to the right device</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>quant_rcnn_res101_prepared.to(device)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># construct an optimizer</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> [p <span class="cf">for</span> p <span class="kw">in</span> quant_rcnn_res101_prepared.parameters() <span class="cf">if</span> p.requires_grad]</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    params,</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span><span class="fl">0.005</span>,</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    momentum<span class="op">=</span><span class="fl">0.9</span>,</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.0005</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="co"># and a learning rate scheduler</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>lr_scheduler <span class="op">=</span> torch.optim.lr_scheduler.StepLR(</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    optimizer,</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    step_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>    gamma<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="co"># let's train it for 10 epochs</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># train for one epoch, printing every 10 iterations</span></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>    train_one_epoch(quant_rcnn_res101_prepared, optimizer, data_loader, device, epoch, print_freq<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># update the learning rate</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>    lr_scheduler.step()</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluate on the test dataset</span></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>    evaluate(quant_rcnn_res101_prepared, data_loader_test, device<span class="op">=</span>device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: [0]  [ 0/60]  eta: 0:01:05  lr: 0.000090  loss: 1.6602 (1.6602)  loss_classifier: 0.9224 (0.9224)  loss_box_reg: 0.0023 (0.0023)  loss_objectness: 0.7214 (0.7214)  loss_rpn_box_reg: 0.0140 (0.0140)  time: 1.0922  data: 0.2625  max mem: 9188
Epoch: [0]  [10/60]  eta: 0:00:40  lr: 0.000936  loss: 0.7143 (0.9616)  loss_classifier: 0.0847 (0.3113)  loss_box_reg: 0.0019 (0.0041)  loss_objectness: 0.6638 (0.6250)  loss_rpn_box_reg: 0.0206 (0.0212)  time: 0.8087  data: 0.0288  max mem: 9188
Epoch: [0]  [20/60]  eta: 0:00:31  lr: 0.001783  loss: 0.5990 (0.7357)  loss_classifier: 0.1087 (0.2302)  loss_box_reg: 0.0221 (0.0527)  loss_objectness: 0.3577 (0.4293)  loss_rpn_box_reg: 0.0206 (0.0235)  time: 0.7719  data: 0.0054  max mem: 9188
Epoch: [0]  [30/60]  eta: 0:00:23  lr: 0.002629  loss: 0.3487 (0.5963)  loss_classifier: 0.1135 (0.1929)  loss_box_reg: 0.0837 (0.0640)  loss_objectness: 0.0951 (0.3145)  loss_rpn_box_reg: 0.0212 (0.0250)  time: 0.7637  data: 0.0054  max mem: 9188
Epoch: [0]  [40/60]  eta: 0:00:15  lr: 0.003476  loss: 0.3345 (0.5256)  loss_classifier: 0.1140 (0.1790)  loss_box_reg: 0.0874 (0.0739)  loss_objectness: 0.0602 (0.2496)  loss_rpn_box_reg: 0.0201 (0.0230)  time: 0.7614  data: 0.0054  max mem: 9188
Epoch: [0]  [50/60]  eta: 0:00:07  lr: 0.004323  loss: 0.2655 (0.4923)  loss_classifier: 0.1164 (0.1727)  loss_box_reg: 0.0991 (0.0861)  loss_objectness: 0.0439 (0.2110)  loss_rpn_box_reg: 0.0160 (0.0226)  time: 0.7539  data: 0.0054  max mem: 9188
Epoch: [0]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2582 (0.4716)  loss_classifier: 0.1061 (0.1660)  loss_box_reg: 0.1086 (0.0956)  loss_objectness: 0.0400 (0.1876)  loss_rpn_box_reg: 0.0109 (0.0223)  time: 0.7506  data: 0.0055  max mem: 9188
Epoch: [0] Total time: 0:00:46 (0.7705 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:26  model_time: 0.2997 (0.2997)  evaluator_time: 0.0087 (0.0087)  time: 0.5225  data: 0.2121  max mem: 9188
Test:  [49/50]  eta: 0:00:00  model_time: 0.2671 (0.2676)  evaluator_time: 0.0043 (0.0045)  time: 0.2761  data: 0.0031  max mem: 9188
Test: Total time: 0:00:14 (0.2843 s / it)
Averaged stats: model_time: 0.2671 (0.2676)  evaluator_time: 0.0043 (0.0045)
Accumulating evaluation results...
DONE (t=0.04s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.021
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.089
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.022
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.002
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.077
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.325
Epoch: [1]  [ 0/60]  eta: 0:01:00  lr: 0.005000  loss: 0.2305 (0.2305)  loss_classifier: 0.0670 (0.0670)  loss_box_reg: 0.1148 (0.1148)  loss_objectness: 0.0354 (0.0354)  loss_rpn_box_reg: 0.0133 (0.0133)  time: 1.0080  data: 0.2090  max mem: 9188
Epoch: [1]  [10/60]  eta: 0:00:38  lr: 0.005000  loss: 0.3300 (0.3228)  loss_classifier: 0.1011 (0.1051)  loss_box_reg: 0.1396 (0.1461)  loss_objectness: 0.0468 (0.0494)  loss_rpn_box_reg: 0.0231 (0.0222)  time: 0.7770  data: 0.0242  max mem: 9188
Epoch: [1]  [20/60]  eta: 0:00:30  lr: 0.005000  loss: 0.2922 (0.2905)  loss_classifier: 0.0966 (0.0966)  loss_box_reg: 0.1377 (0.1319)  loss_objectness: 0.0367 (0.0438)  loss_rpn_box_reg: 0.0156 (0.0183)  time: 0.7568  data: 0.0057  max mem: 9188
Epoch: [1]  [30/60]  eta: 0:00:23  lr: 0.005000  loss: 0.2693 (0.2873)  loss_classifier: 0.0876 (0.0962)  loss_box_reg: 0.1191 (0.1302)  loss_objectness: 0.0337 (0.0430)  loss_rpn_box_reg: 0.0101 (0.0179)  time: 0.7612  data: 0.0057  max mem: 9188
Epoch: [1]  [40/60]  eta: 0:00:15  lr: 0.005000  loss: 0.2052 (0.2968)  loss_classifier: 0.0669 (0.1035)  loss_box_reg: 0.1018 (0.1287)  loss_objectness: 0.0299 (0.0464)  loss_rpn_box_reg: 0.0101 (0.0182)  time: 0.7637  data: 0.0057  max mem: 9188
Epoch: [1]  [50/60]  eta: 0:00:07  lr: 0.005000  loss: 0.1873 (0.2863)  loss_classifier: 0.0641 (0.0982)  loss_box_reg: 0.0945 (0.1282)  loss_objectness: 0.0212 (0.0416)  loss_rpn_box_reg: 0.0164 (0.0183)  time: 0.7661  data: 0.0057  max mem: 9188
Epoch: [1]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2432 (0.2833)  loss_classifier: 0.0731 (0.0959)  loss_box_reg: 0.1244 (0.1310)  loss_objectness: 0.0187 (0.0376)  loss_rpn_box_reg: 0.0170 (0.0188)  time: 0.7676  data: 0.0057  max mem: 9188
Epoch: [1] Total time: 0:00:46 (0.7698 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:24  model_time: 0.2893 (0.2893)  evaluator_time: 0.0060 (0.0060)  time: 0.4938  data: 0.1962  max mem: 9188
Test:  [49/50]  eta: 0:00:00  model_time: 0.2744 (0.2684)  evaluator_time: 0.0021 (0.0024)  time: 0.2816  data: 0.0031  max mem: 9188
Test: Total time: 0:00:14 (0.2826 s / it)
Averaged stats: model_time: 0.2744 (0.2684)  evaluator_time: 0.0021 (0.0024)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.696
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.040
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.113
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.131
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.389
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.408
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.360
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.421
Epoch: [2]  [ 0/60]  eta: 0:01:02  lr: 0.005000  loss: 0.1664 (0.1664)  loss_classifier: 0.0622 (0.0622)  loss_box_reg: 0.0806 (0.0806)  loss_objectness: 0.0133 (0.0133)  loss_rpn_box_reg: 0.0103 (0.0103)  time: 1.0334  data: 0.2054  max mem: 9188
Epoch: [2]  [10/60]  eta: 0:00:40  lr: 0.005000  loss: 0.1824 (0.2101)  loss_classifier: 0.0578 (0.0609)  loss_box_reg: 0.1048 (0.1166)  loss_objectness: 0.0102 (0.0110)  loss_rpn_box_reg: 0.0180 (0.0216)  time: 0.8014  data: 0.0237  max mem: 9188
Epoch: [2]  [20/60]  eta: 0:00:31  lr: 0.005000  loss: 0.1885 (0.2088)  loss_classifier: 0.0615 (0.0680)  loss_box_reg: 0.1048 (0.1126)  loss_objectness: 0.0076 (0.0093)  loss_rpn_box_reg: 0.0180 (0.0190)  time: 0.7785  data: 0.0057  max mem: 9188
Epoch: [2]  [30/60]  eta: 0:00:23  lr: 0.005000  loss: 0.1895 (0.2282)  loss_classifier: 0.0627 (0.0734)  loss_box_reg: 0.1084 (0.1228)  loss_objectness: 0.0067 (0.0110)  loss_rpn_box_reg: 0.0160 (0.0211)  time: 0.7754  data: 0.0058  max mem: 9188
Epoch: [2]  [40/60]  eta: 0:00:15  lr: 0.005000  loss: 0.2135 (0.2191)  loss_classifier: 0.0580 (0.0679)  loss_box_reg: 0.1133 (0.1171)  loss_objectness: 0.0069 (0.0131)  loss_rpn_box_reg: 0.0182 (0.0210)  time: 0.7747  data: 0.0058  max mem: 9188
Epoch: [2]  [50/60]  eta: 0:00:07  lr: 0.005000  loss: 0.1913 (0.2131)  loss_classifier: 0.0502 (0.0647)  loss_box_reg: 0.1150 (0.1163)  loss_objectness: 0.0074 (0.0126)  loss_rpn_box_reg: 0.0145 (0.0195)  time: 0.7779  data: 0.0066  max mem: 9188
Epoch: [2]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2049 (0.2168)  loss_classifier: 0.0544 (0.0649)  loss_box_reg: 0.1379 (0.1196)  loss_objectness: 0.0077 (0.0120)  loss_rpn_box_reg: 0.0157 (0.0202)  time: 0.7743  data: 0.0071  max mem: 9188
Epoch: [2] Total time: 0:00:46 (0.7830 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:26  model_time: 0.3114 (0.3114)  evaluator_time: 0.0047 (0.0047)  time: 0.5266  data: 0.2082  max mem: 9188
Test:  [49/50]  eta: 0:00:00  model_time: 0.2667 (0.2690)  evaluator_time: 0.0020 (0.0022)  time: 0.2730  data: 0.0034  max mem: 9188
Test: Total time: 0:00:14 (0.2832 s / it)
Averaged stats: model_time: 0.2667 (0.2690)  evaluator_time: 0.0020 (0.0022)
Accumulating evaluation results...
DONE (t=0.03s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.426
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.941
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.275
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.003
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.249
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.446
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.216
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.535
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.540
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.380
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.561
Epoch: [3]  [ 0/60]  eta: 0:00:59  lr: 0.000500  loss: 0.1269 (0.1269)  loss_classifier: 0.0302 (0.0302)  loss_box_reg: 0.0771 (0.0771)  loss_objectness: 0.0064 (0.0064)  loss_rpn_box_reg: 0.0132 (0.0132)  time: 0.9940  data: 0.2165  max mem: 9188
Epoch: [3]  [10/60]  eta: 0:00:40  lr: 0.000500  loss: 0.1716 (0.2328)  loss_classifier: 0.0517 (0.0684)  loss_box_reg: 0.1076 (0.1371)  loss_objectness: 0.0037 (0.0082)  loss_rpn_box_reg: 0.0132 (0.0190)  time: 0.8007  data: 0.0249  max mem: 9188
Epoch: [3]  [20/60]  eta: 0:00:31  lr: 0.000500  loss: 0.1716 (0.2058)  loss_classifier: 0.0479 (0.0585)  loss_box_reg: 0.1076 (0.1229)  loss_objectness: 0.0041 (0.0069)  loss_rpn_box_reg: 0.0143 (0.0175)  time: 0.7782  data: 0.0058  max mem: 9188
Epoch: [3]  [30/60]  eta: 0:00:23  lr: 0.000500  loss: 0.1613 (0.1940)  loss_classifier: 0.0434 (0.0556)  loss_box_reg: 0.1042 (0.1161)  loss_objectness: 0.0051 (0.0067)  loss_rpn_box_reg: 0.0131 (0.0157)  time: 0.7731  data: 0.0059  max mem: 9188
Epoch: [3]  [40/60]  eta: 0:00:15  lr: 0.000500  loss: 0.1840 (0.1919)  loss_classifier: 0.0563 (0.0555)  loss_box_reg: 0.1063 (0.1140)  loss_objectness: 0.0051 (0.0064)  loss_rpn_box_reg: 0.0132 (0.0160)  time: 0.7706  data: 0.0062  max mem: 9188
Epoch: [3]  [50/60]  eta: 0:00:07  lr: 0.000500  loss: 0.1402 (0.1820)  loss_classifier: 0.0444 (0.0527)  loss_box_reg: 0.0820 (0.1080)  loss_objectness: 0.0044 (0.0060)  loss_rpn_box_reg: 0.0132 (0.0153)  time: 0.7652  data: 0.0061  max mem: 9188
Epoch: [3]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1260 (0.1743)  loss_classifier: 0.0367 (0.0501)  loss_box_reg: 0.0732 (0.1045)  loss_objectness: 0.0022 (0.0055)  loss_rpn_box_reg: 0.0080 (0.0143)  time: 0.7578  data: 0.0055  max mem: 9188
Epoch: [3] Total time: 0:00:46 (0.7759 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:26  model_time: 0.3235 (0.3235)  evaluator_time: 0.0042 (0.0042)  time: 0.5317  data: 0.2016  max mem: 9188
Test:  [49/50]  eta: 0:00:00  model_time: 0.2647 (0.2682)  evaluator_time: 0.0016 (0.0018)  time: 0.2752  data: 0.0030  max mem: 9188
Test: Total time: 0:00:14 (0.2819 s / it)
Averaged stats: model_time: 0.2647 (0.2682)  evaluator_time: 0.0016 (0.0018)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.501
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.938
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.453
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.314
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.523
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.575
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.575
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.380
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.599
Epoch: [4]  [ 0/60]  eta: 0:00:58  lr: 0.000500  loss: 0.1874 (0.1874)  loss_classifier: 0.0379 (0.0379)  loss_box_reg: 0.1311 (0.1311)  loss_objectness: 0.0055 (0.0055)  loss_rpn_box_reg: 0.0129 (0.0129)  time: 0.9828  data: 0.2060  max mem: 9188
Epoch: [4]  [10/60]  eta: 0:00:39  lr: 0.000500  loss: 0.1874 (0.1985)  loss_classifier: 0.0483 (0.0520)  loss_box_reg: 0.1261 (0.1253)  loss_objectness: 0.0042 (0.0041)  loss_rpn_box_reg: 0.0143 (0.0171)  time: 0.7883  data: 0.0238  max mem: 9188
Epoch: [4]  [20/60]  eta: 0:00:31  lr: 0.000500  loss: 0.1580 (0.1767)  loss_classifier: 0.0424 (0.0474)  loss_box_reg: 0.0927 (0.1102)  loss_objectness: 0.0040 (0.0045)  loss_rpn_box_reg: 0.0133 (0.0147)  time: 0.7684  data: 0.0057  max mem: 9188
Epoch: [4]  [30/60]  eta: 0:00:23  lr: 0.000500  loss: 0.1519 (0.1718)  loss_classifier: 0.0421 (0.0475)  loss_box_reg: 0.0991 (0.1055)  loss_objectness: 0.0038 (0.0048)  loss_rpn_box_reg: 0.0093 (0.0140)  time: 0.7741  data: 0.0059  max mem: 9188
Epoch: [4]  [40/60]  eta: 0:00:15  lr: 0.000500  loss: 0.1401 (0.1661)  loss_classifier: 0.0371 (0.0455)  loss_box_reg: 0.0776 (0.1031)  loss_objectness: 0.0038 (0.0046)  loss_rpn_box_reg: 0.0092 (0.0129)  time: 0.7815  data: 0.0058  max mem: 9188
Epoch: [4]  [50/60]  eta: 0:00:07  lr: 0.000500  loss: 0.1408 (0.1695)  loss_classifier: 0.0360 (0.0473)  loss_box_reg: 0.0843 (0.1031)  loss_objectness: 0.0038 (0.0054)  loss_rpn_box_reg: 0.0112 (0.0136)  time: 0.7709  data: 0.0057  max mem: 9188
Epoch: [4]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1481 (0.1657)  loss_classifier: 0.0366 (0.0461)  loss_box_reg: 0.0888 (0.1014)  loss_objectness: 0.0026 (0.0052)  loss_rpn_box_reg: 0.0119 (0.0130)  time: 0.7635  data: 0.0058  max mem: 9188
Epoch: [4] Total time: 0:00:46 (0.7779 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3073 (0.3073)  evaluator_time: 0.0039 (0.0039)  time: 0.5077  data: 0.1946  max mem: 9188
Test:  [49/50]  eta: 0:00:00  model_time: 0.2605 (0.2693)  evaluator_time: 0.0016 (0.0019)  time: 0.2714  data: 0.0030  max mem: 9188
Test: Total time: 0:00:14 (0.2829 s / it)
Averaged stats: model_time: 0.2605 (0.2693)  evaluator_time: 0.0016 (0.0019)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.939
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.017
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.242
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.242
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.570
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.571
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.320
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.595
Epoch: [5]  [ 0/60]  eta: 0:00:59  lr: 0.000500  loss: 0.1179 (0.1179)  loss_classifier: 0.0285 (0.0285)  loss_box_reg: 0.0738 (0.0738)  loss_objectness: 0.0035 (0.0035)  loss_rpn_box_reg: 0.0120 (0.0120)  time: 0.9861  data: 0.2058  max mem: 9188
Epoch: [5]  [10/60]  eta: 0:00:39  lr: 0.000500  loss: 0.1691 (0.1664)  loss_classifier: 0.0493 (0.0478)  loss_box_reg: 0.1028 (0.1006)  loss_objectness: 0.0039 (0.0043)  loss_rpn_box_reg: 0.0109 (0.0137)  time: 0.7915  data: 0.0246  max mem: 9188
Epoch: [5]  [20/60]  eta: 0:00:31  lr: 0.000500  loss: 0.1691 (0.1752)  loss_classifier: 0.0499 (0.0500)  loss_box_reg: 0.1028 (0.1053)  loss_objectness: 0.0042 (0.0052)  loss_rpn_box_reg: 0.0109 (0.0149)  time: 0.7731  data: 0.0062  max mem: 9188
Epoch: [5]  [30/60]  eta: 0:00:23  lr: 0.000500  loss: 0.1550 (0.1697)  loss_classifier: 0.0483 (0.0486)  loss_box_reg: 0.0918 (0.1023)  loss_objectness: 0.0046 (0.0052)  loss_rpn_box_reg: 0.0104 (0.0136)  time: 0.7696  data: 0.0057  max mem: 9188
Epoch: [5]  [40/60]  eta: 0:00:15  lr: 0.000500  loss: 0.1439 (0.1698)  loss_classifier: 0.0439 (0.0489)  loss_box_reg: 0.0912 (0.1030)  loss_objectness: 0.0028 (0.0046)  loss_rpn_box_reg: 0.0097 (0.0133)  time: 0.7672  data: 0.0054  max mem: 9188
Epoch: [5]  [50/60]  eta: 0:00:07  lr: 0.000500  loss: 0.1373 (0.1627)  loss_classifier: 0.0387 (0.0469)  loss_box_reg: 0.0841 (0.0988)  loss_objectness: 0.0029 (0.0045)  loss_rpn_box_reg: 0.0078 (0.0125)  time: 0.7658  data: 0.0054  max mem: 9188
Epoch: [5]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1196 (0.1647)  loss_classifier: 0.0331 (0.0475)  loss_box_reg: 0.0802 (0.1003)  loss_objectness: 0.0030 (0.0047)  loss_rpn_box_reg: 0.0071 (0.0121)  time: 0.7703  data: 0.0056  max mem: 9188
Epoch: [5] Total time: 0:00:46 (0.7769 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3016 (0.3016)  evaluator_time: 0.0036 (0.0036)  time: 0.5064  data: 0.1989  max mem: 9188
Test:  [49/50]  eta: 0:00:00  model_time: 0.2680 (0.2723)  evaluator_time: 0.0015 (0.0018)  time: 0.2758  data: 0.0029  max mem: 9188
Test: Total time: 0:00:14 (0.2859 s / it)
Averaged stats: model_time: 0.2680 (0.2723)  evaluator_time: 0.0015 (0.0018)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.517
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.964
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.484
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.006
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.315
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.539
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.256
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.583
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.586
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.067
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.608
Epoch: [6]  [ 0/60]  eta: 0:00:59  lr: 0.000050  loss: 0.1945 (0.1945)  loss_classifier: 0.0693 (0.0693)  loss_box_reg: 0.1114 (0.1114)  loss_objectness: 0.0028 (0.0028)  loss_rpn_box_reg: 0.0110 (0.0110)  time: 0.9962  data: 0.2050  max mem: 9188
Epoch: [6]  [10/60]  eta: 0:00:39  lr: 0.000050  loss: 0.1817 (0.1884)  loss_classifier: 0.0551 (0.0548)  loss_box_reg: 0.1114 (0.1192)  loss_objectness: 0.0029 (0.0025)  loss_rpn_box_reg: 0.0103 (0.0117)  time: 0.7963  data: 0.0238  max mem: 9188
Epoch: [6]  [20/60]  eta: 0:00:31  lr: 0.000050  loss: 0.1631 (0.1723)  loss_classifier: 0.0446 (0.0494)  loss_box_reg: 0.1005 (0.1094)  loss_objectness: 0.0026 (0.0025)  loss_rpn_box_reg: 0.0089 (0.0110)  time: 0.7784  data: 0.0056  max mem: 9188
Epoch: [6]  [30/60]  eta: 0:00:23  lr: 0.000050  loss: 0.1194 (0.1552)  loss_classifier: 0.0393 (0.0450)  loss_box_reg: 0.0682 (0.0973)  loss_objectness: 0.0021 (0.0027)  loss_rpn_box_reg: 0.0071 (0.0102)  time: 0.7767  data: 0.0055  max mem: 9188
Epoch: [6]  [40/60]  eta: 0:00:15  lr: 0.000050  loss: 0.1135 (0.1558)  loss_classifier: 0.0380 (0.0451)  loss_box_reg: 0.0677 (0.0974)  loss_objectness: 0.0027 (0.0030)  loss_rpn_box_reg: 0.0074 (0.0103)  time: 0.7690  data: 0.0055  max mem: 9188
Epoch: [6]  [50/60]  eta: 0:00:07  lr: 0.000050  loss: 0.1423 (0.1599)  loss_classifier: 0.0434 (0.0465)  loss_box_reg: 0.0839 (0.0993)  loss_objectness: 0.0035 (0.0035)  loss_rpn_box_reg: 0.0110 (0.0106)  time: 0.7677  data: 0.0057  max mem: 9188
Epoch: [6]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1605 (0.1649)  loss_classifier: 0.0487 (0.0483)  loss_box_reg: 0.0996 (0.1019)  loss_objectness: 0.0027 (0.0040)  loss_rpn_box_reg: 0.0074 (0.0107)  time: 0.7675  data: 0.0056  max mem: 9188
Epoch: [6] Total time: 0:00:46 (0.7791 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3030 (0.3030)  evaluator_time: 0.0038 (0.0038)  time: 0.5109  data: 0.2020  max mem: 9188
Test:  [49/50]  eta: 0:00:00  model_time: 0.2680 (0.2678)  evaluator_time: 0.0015 (0.0018)  time: 0.2732  data: 0.0031  max mem: 9188
Test: Total time: 0:00:14 (0.2814 s / it)
Averaged stats: model_time: 0.2680 (0.2678)  evaluator_time: 0.0015 (0.0018)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.521
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.948
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.470
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.306
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.544
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.269
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.591
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.596
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.617
Epoch: [7]  [ 0/60]  eta: 0:00:58  lr: 0.000050  loss: 0.0945 (0.0945)  loss_classifier: 0.0253 (0.0253)  loss_box_reg: 0.0563 (0.0563)  loss_objectness: 0.0073 (0.0073)  loss_rpn_box_reg: 0.0056 (0.0056)  time: 0.9705  data: 0.1853  max mem: 9188
Epoch: [7]  [10/60]  eta: 0:00:39  lr: 0.000050  loss: 0.1117 (0.1174)  loss_classifier: 0.0318 (0.0328)  loss_box_reg: 0.0728 (0.0726)  loss_objectness: 0.0033 (0.0046)  loss_rpn_box_reg: 0.0056 (0.0075)  time: 0.7892  data: 0.0219  max mem: 9188
Epoch: [7]  [20/60]  eta: 0:00:31  lr: 0.000050  loss: 0.1672 (0.1506)  loss_classifier: 0.0427 (0.0432)  loss_box_reg: 0.1006 (0.0924)  loss_objectness: 0.0033 (0.0049)  loss_rpn_box_reg: 0.0100 (0.0101)  time: 0.7759  data: 0.0056  max mem: 9188
Epoch: [7]  [30/60]  eta: 0:00:23  lr: 0.000050  loss: 0.1998 (0.1628)  loss_classifier: 0.0521 (0.0470)  loss_box_reg: 0.1177 (0.1005)  loss_objectness: 0.0039 (0.0048)  loss_rpn_box_reg: 0.0111 (0.0105)  time: 0.7782  data: 0.0055  max mem: 9188
Epoch: [7]  [40/60]  eta: 0:00:15  lr: 0.000050  loss: 0.1580 (0.1646)  loss_classifier: 0.0507 (0.0471)  loss_box_reg: 0.1028 (0.1019)  loss_objectness: 0.0040 (0.0047)  loss_rpn_box_reg: 0.0098 (0.0109)  time: 0.7750  data: 0.0056  max mem: 9188
Epoch: [7]  [50/60]  eta: 0:00:07  lr: 0.000050  loss: 0.1286 (0.1654)  loss_classifier: 0.0416 (0.0473)  loss_box_reg: 0.0761 (0.1023)  loss_objectness: 0.0022 (0.0050)  loss_rpn_box_reg: 0.0091 (0.0108)  time: 0.7680  data: 0.0062  max mem: 9188
Epoch: [7]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1352 (0.1616)  loss_classifier: 0.0416 (0.0464)  loss_box_reg: 0.0761 (0.0996)  loss_objectness: 0.0024 (0.0047)  loss_rpn_box_reg: 0.0087 (0.0108)  time: 0.7615  data: 0.0061  max mem: 9188
Epoch: [7] Total time: 0:00:46 (0.7777 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.2944 (0.2944)  evaluator_time: 0.0038 (0.0038)  time: 0.5057  data: 0.2053  max mem: 9188
Test:  [49/50]  eta: 0:00:00  model_time: 0.2690 (0.2697)  evaluator_time: 0.0015 (0.0017)  time: 0.2784  data: 0.0030  max mem: 9188
Test: Total time: 0:00:14 (0.2834 s / it)
Averaged stats: model_time: 0.2690 (0.2697)  evaluator_time: 0.0015 (0.0017)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.511
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.960
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.464
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.020
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.168
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.536
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.267
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.581
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.585
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.360
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.608
Epoch: [8]  [ 0/60]  eta: 0:01:02  lr: 0.000050  loss: 0.1875 (0.1875)  loss_classifier: 0.0613 (0.0613)  loss_box_reg: 0.1094 (0.1094)  loss_objectness: 0.0076 (0.0076)  loss_rpn_box_reg: 0.0092 (0.0092)  time: 1.0381  data: 0.2319  max mem: 9188
Epoch: [8]  [10/60]  eta: 0:00:39  lr: 0.000050  loss: 0.1625 (0.1392)  loss_classifier: 0.0334 (0.0383)  loss_box_reg: 0.1082 (0.0888)  loss_objectness: 0.0020 (0.0028)  loss_rpn_box_reg: 0.0103 (0.0093)  time: 0.7846  data: 0.0262  max mem: 9188
Epoch: [8]  [20/60]  eta: 0:00:31  lr: 0.000050  loss: 0.1625 (0.1732)  loss_classifier: 0.0377 (0.0486)  loss_box_reg: 0.1059 (0.1091)  loss_objectness: 0.0022 (0.0040)  loss_rpn_box_reg: 0.0105 (0.0114)  time: 0.7634  data: 0.0056  max mem: 9188
Epoch: [8]  [30/60]  eta: 0:00:23  lr: 0.000050  loss: 0.1233 (0.1589)  loss_classifier: 0.0372 (0.0447)  loss_box_reg: 0.0807 (0.0999)  loss_objectness: 0.0024 (0.0035)  loss_rpn_box_reg: 0.0093 (0.0107)  time: 0.7660  data: 0.0055  max mem: 9188
Epoch: [8]  [40/60]  eta: 0:00:15  lr: 0.000050  loss: 0.1437 (0.1624)  loss_classifier: 0.0351 (0.0458)  loss_box_reg: 0.0866 (0.1019)  loss_objectness: 0.0024 (0.0037)  loss_rpn_box_reg: 0.0096 (0.0110)  time: 0.7712  data: 0.0056  max mem: 9188
Epoch: [8]  [50/60]  eta: 0:00:07  lr: 0.000050  loss: 0.1505 (0.1609)  loss_classifier: 0.0417 (0.0462)  loss_box_reg: 0.1007 (0.1003)  loss_objectness: 0.0022 (0.0037)  loss_rpn_box_reg: 0.0095 (0.0108)  time: 0.7742  data: 0.0058  max mem: 9188
Epoch: [8]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1266 (0.1577)  loss_classifier: 0.0403 (0.0455)  loss_box_reg: 0.0737 (0.0980)  loss_objectness: 0.0022 (0.0036)  loss_rpn_box_reg: 0.0070 (0.0106)  time: 0.7697  data: 0.0057  max mem: 9188
Epoch: [8] Total time: 0:00:46 (0.7761 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:24  model_time: 0.2885 (0.2885)  evaluator_time: 0.0034 (0.0034)  time: 0.4929  data: 0.1990  max mem: 9188
Test:  [49/50]  eta: 0:00:00  model_time: 0.2648 (0.2672)  evaluator_time: 0.0016 (0.0017)  time: 0.2731  data: 0.0031  max mem: 9188
Test: Total time: 0:00:14 (0.2808 s / it)
Averaged stats: model_time: 0.2648 (0.2672)  evaluator_time: 0.0016 (0.0017)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.529
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.962
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.499
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.004
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.330
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.553
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.597
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.603
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.067
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.480
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.623
Epoch: [9]  [ 0/60]  eta: 0:01:02  lr: 0.000005  loss: 0.1457 (0.1457)  loss_classifier: 0.0373 (0.0373)  loss_box_reg: 0.0974 (0.0974)  loss_objectness: 0.0019 (0.0019)  loss_rpn_box_reg: 0.0091 (0.0091)  time: 1.0399  data: 0.2021  max mem: 9188
Epoch: [9]  [10/60]  eta: 0:00:39  lr: 0.000005  loss: 0.1672 (0.1505)  loss_classifier: 0.0467 (0.0432)  loss_box_reg: 0.0974 (0.0931)  loss_objectness: 0.0023 (0.0036)  loss_rpn_box_reg: 0.0091 (0.0107)  time: 0.7953  data: 0.0234  max mem: 9188
Epoch: [9]  [20/60]  eta: 0:00:31  lr: 0.000005  loss: 0.1563 (0.1567)  loss_classifier: 0.0475 (0.0462)  loss_box_reg: 0.0904 (0.0967)  loss_objectness: 0.0026 (0.0035)  loss_rpn_box_reg: 0.0070 (0.0103)  time: 0.7706  data: 0.0062  max mem: 9188
Epoch: [9]  [30/60]  eta: 0:00:23  lr: 0.000005  loss: 0.1367 (0.1544)  loss_classifier: 0.0420 (0.0449)  loss_box_reg: 0.0846 (0.0953)  loss_objectness: 0.0027 (0.0040)  loss_rpn_box_reg: 0.0086 (0.0103)  time: 0.7776  data: 0.0071  max mem: 9188
Epoch: [9]  [40/60]  eta: 0:00:15  lr: 0.000005  loss: 0.1362 (0.1518)  loss_classifier: 0.0395 (0.0437)  loss_box_reg: 0.0834 (0.0941)  loss_objectness: 0.0028 (0.0038)  loss_rpn_box_reg: 0.0109 (0.0102)  time: 0.7749  data: 0.0063  max mem: 9188
Epoch: [9]  [50/60]  eta: 0:00:07  lr: 0.000005  loss: 0.1520 (0.1632)  loss_classifier: 0.0409 (0.0471)  loss_box_reg: 0.0889 (0.1008)  loss_objectness: 0.0034 (0.0044)  loss_rpn_box_reg: 0.0088 (0.0109)  time: 0.7710  data: 0.0057  max mem: 9188
Epoch: [9]  [59/60]  eta: 0:00:00  lr: 0.000005  loss: 0.1608 (0.1608)  loss_classifier: 0.0415 (0.0465)  loss_box_reg: 0.0889 (0.0990)  loss_objectness: 0.0028 (0.0044)  loss_rpn_box_reg: 0.0113 (0.0109)  time: 0.7794  data: 0.0059  max mem: 9188
Epoch: [9] Total time: 0:00:46 (0.7830 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:25  model_time: 0.3008 (0.3008)  evaluator_time: 0.0039 (0.0039)  time: 0.5143  data: 0.2076  max mem: 9188
Test:  [49/50]  eta: 0:00:00  model_time: 0.2704 (0.2700)  evaluator_time: 0.0016 (0.0018)  time: 0.2762  data: 0.0031  max mem: 9188
Test: Total time: 0:00:14 (0.2838 s / it)
Averaged stats: model_time: 0.2704 (0.2700)  evaluator_time: 0.0016 (0.0018)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.516
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.954
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.508
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.004
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.295
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.542
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.590
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.596
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.067
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.400
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.619</code></pre>
</div>
</div>
<p>Now to convert model and save the model. Make sure to put the model on CPU before conversion or you will get an error. After conversion you should see quantized modules like <code>QuantizedConvReLU2d</code>.</p>
<pre><code>(backbone): QuantBackboneWithFPN(
    (body): QuantLayers(
      (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([56]), dtype=torch.quint8)
      (dequant): DeQuantize()
      (layers): IntermediateLayerGetter(
        (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.3137107789516449, zero_point=0, padding=(3, 3))
        (bn1): Identity()
        (relu): Identity()
        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        (layer1): Sequential(
          (0): BottleneckFloatFunctional(
            (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.15175238251686096, zero_point=0)
            (bn1): Identity()
            (relu1): Identity()
            ...</code></pre>
<div class="cell" data-execution_count="258">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>quant_rcnn_res101_prepared.<span class="bu">eval</span>()</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>quant_rcnn_res101_prepared.to(torch.device(<span class="st">'cpu'</span>))</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>quant_rcnn_res101_converted <span class="op">=</span> torch.ao.quantization.convert(quant_rcnn_res101_prepared, inplace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>quant_model_101_path <span class="op">=</span> <span class="st">"/content/quant_model_101.pth"</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>torch.save(quant_rcnn_res101_converted.state_dict(), quant_model_101_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For comparison I’ll generate the same network without any modifications made for quantization (including fusion). Then we can compare model sizes and latency. Note that this is just comparing latency on the CPU, if the float model was on GPU it could be significantly faster depending upon the hardware.</p>
<div class="cell" data-execution_count="259">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.detection.backbone_utils <span class="im">import</span> BackboneWithFPN</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.resnet <span class="im">import</span> Bottleneck</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>res101_backbone <span class="op">=</span> ResNet(block<span class="op">=</span>Bottleneck, layers<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">23</span>, <span class="dv">3</span>])</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>res101_backbone.load_state_dict(ResNet101_Weights.DEFAULT.get_state_dict(progress<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>rcnn_res101 <span class="op">=</span> FasterRCNN(</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    BackboneWithFPN(</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>        backbone<span class="op">=</span>res101_backbone,</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        return_layers<span class="op">=</span>return_layers,</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        in_channels_list<span class="op">=</span>in_channels_list,</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        out_channels<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        extra_blocks<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>        norm_layer<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    num_classes<span class="op">=</span><span class="dv">2</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>rcnn_res101.<span class="bu">eval</span>()</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>rcnn_res101.to(torch.device(<span class="st">'cpu'</span>))</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>model_101_path <span class="op">=</span> <span class="st">"/content/model_101.pth"</span></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>torch.save(rcnn_res101.state_dict(), model_101_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="8c62572f-00d9-4232-dc23-b773ee8d9560" data-execution_count="260">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'size of quantized model: </span><span class="sc">{</span><span class="bu">round</span>(os.path.getsize(<span class="st">"/content/quant_model_101.pth"</span>) <span class="op">/</span> <span class="fl">1e6</span>)<span class="sc">}</span><span class="ss"> MB'</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'size of float model: </span><span class="sc">{</span><span class="bu">round</span>(os.path.getsize(<span class="st">"/content/model_101.pth"</span>) <span class="op">/</span> <span class="fl">1e6</span>)<span class="sc">}</span><span class="ss"> MB'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>size of quantized model: 115 MB
size of float model: 242 MB</code></pre>
</div>
</div>
<div class="cell" data-outputid="98dc7aac-fa3b-4b0b-84a3-91b884091185" data-execution_count="261">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> time <span class="im">import</span> perf_counter</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># just grab one test image/batch</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>images, targets <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(data_loader_test))</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>images <span class="op">=</span> <span class="bu">list</span>(img.to(torch.device(<span class="st">'cpu'</span>)) <span class="cf">for</span> img <span class="kw">in</span> images)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> perf_counter()</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>    __ <span class="op">=</span> quant_rcnn_res101_converted(images)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"quant model avg time: </span><span class="sc">{</span>(perf_counter() <span class="op">-</span> start) <span class="op">/</span> n<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> perf_counter()</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    __ <span class="op">=</span> rcnn_res101(images)</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"float model (cpu) avg time: </span><span class="sc">{</span>(perf_counter() <span class="op">-</span> start) <span class="op">/</span> n<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>quant model avg time: 1.84
float model (cpu) avg time: 2.39</code></pre>
</div>
</div>
<p>I believe a fully quantized model would be even smaller and faster by comparison. In this case, while we did quantize the backbone for the RCNN, it only accounted for roughly 70% of the model parameters. So a significant number of float operations still occur after the quantized backbone.</p>
<div class="cell" data-outputid="295969a9-9b2b-4e81-ecb5-3d4810233e30" data-execution_count="262">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>num_model_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> rcnn_res101.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>num_backbone_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> rcnn_res101.backbone.body.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"total number of parameters in model: </span><span class="sc">{</span>num_model_params<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"total number of parameters in backbone: </span><span class="sc">{</span>num_backbone_params<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ratio of quantized parameters: </span><span class="sc">{</span>num_backbone_params <span class="op">/</span> num_model_params<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>total number of parameters in model: 60344409
total number of parameters in backbone: 42500160
ratio of quantized parameters: 0.70</code></pre>
</div>
</div>
<p>We can also profile each model to see where each spends the most time during a forward pass.</p>
<div class="cell" data-outputid="0f9a21ce-218e-4c31-e302-ed60ef2d9d2b" data-execution_count="263">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.profiler <span class="im">import</span> profile, record_function, ProfilerActivity</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> profile(activities<span class="op">=</span>[ProfilerActivity.CPU], record_shapes<span class="op">=</span><span class="va">False</span>) <span class="im">as</span> prof:</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> record_function(<span class="st">"model_inference"</span>):</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>        quant_rcnn_res101_converted(images)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prof.key_averages().table(sort_by<span class="op">=</span><span class="st">"cpu_time_total"</span>, row_limit<span class="op">=</span><span class="dv">10</span>))</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> profile(activities<span class="op">=</span>[ProfilerActivity.CPU], record_shapes<span class="op">=</span><span class="va">False</span>) <span class="im">as</span> prof:</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> record_function(<span class="st">"model_inference"</span>):</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>        rcnn_res101(images)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prof.key_averages().table(sort_by<span class="op">=</span><span class="st">"cpu_time_total"</span>, row_limit<span class="op">=</span><span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                  model_inference         1.55%      31.980ms       100.00%        2.068s        2.068s             1  
           quantized::conv2d_relu        34.88%     721.174ms        35.12%     726.108ms      10.837ms            67  
                     aten::conv2d         0.01%     277.000us        22.65%     468.216ms      20.357ms            23  
                aten::convolution         0.02%     463.000us        22.64%     468.117ms      20.353ms            23  
               aten::_convolution         0.02%     478.000us        22.62%     467.654ms      20.333ms            23  
         aten::mkldnn_convolution        22.44%     463.959ms        22.48%     464.897ms      20.213ms            23  
           torchvision::roi_align        11.18%     231.104ms        14.49%     299.518ms      74.879ms             4  
                quantized::conv2d        14.12%     291.885ms        14.17%     292.962ms       7.918ms            37  
                      aten::relu_         3.83%      79.098ms         3.93%      81.327ms       2.140ms            38  
                      aten::clone         0.07%       1.494ms         3.40%      70.212ms       1.463ms            48  
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.068s

---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                  model_inference         3.11%      72.709ms       100.00%        2.341s        2.341s             1  
                     aten::conv2d         0.03%     765.000us        70.93%        1.660s      13.073ms           127  
                aten::convolution         0.12%       2.885ms        70.90%        1.660s      13.067ms           127  
               aten::_convolution         0.09%       2.153ms        70.78%        1.657s      13.045ms           127  
         aten::mkldnn_convolution        70.41%        1.648s        70.68%        1.655s      13.028ms           127  
           torchvision::roi_align        10.81%     253.069ms        10.89%     254.852ms      63.713ms             4  
                 aten::batch_norm         0.02%     443.000us         4.26%      99.738ms     959.019us           104  
     aten::_batch_norm_impl_index         0.04%     942.000us         4.24%      99.295ms     954.760us           104  
          aten::native_batch_norm         4.04%      94.653ms         4.19%      98.097ms     943.240us           104  
                     aten::linear         0.00%      76.000us         2.97%      69.608ms      17.402ms             4  
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.341s
</code></pre>
</div>
</div>
<p>The following loads the saved quantized model. It’s important that the same process of <a href="https://discuss.pytorch.org/t/how-to-load-quantized-model-for-inference/140283">fusing, preparing, and converting</a> be done before loading weights since quantization significantly alters the network. For sake of completeness, we can look at a prediction from the partially quantized RCNN.</p>
<div class="cell" data-execution_count="264">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>quant_model_loaded <span class="op">=</span> FasterRCNN(</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    QuantBackboneWithFPN(</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>        backbone<span class="op">=</span>resnet101_ff(),</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        return_layers<span class="op">=</span>return_layers,</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        in_channels_list<span class="op">=</span>in_channels_list,</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        out_channels<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        extra_blocks<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        norm_layer<span class="op">=</span><span class="va">None</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    num_classes<span class="op">=</span><span class="dv">2</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>quant_model_loaded.<span class="bu">eval</span>()</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>fuse_modules(quant_model_loaded.backbone.body.layers, [[<span class="st">'conv1'</span>, <span class="st">'bn1'</span>, <span class="st">'relu'</span>]], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k1, m1 <span class="kw">in</span> quant_model_loaded.backbone.body.layers.named_children():</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"layer"</span> <span class="kw">in</span> k1:  <span class="co"># in sequential layer with blocks</span></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k2, m2 <span class="kw">in</span> m1.named_children():</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>            fuse_modules(m2, [[<span class="st">"conv1"</span>, <span class="st">"bn1"</span>, <span class="st">"relu1"</span>], [<span class="st">"conv2"</span>, <span class="st">"bn2"</span>, <span class="st">"relu2"</span>], [<span class="st">"conv3"</span>, <span class="st">"bn3"</span>]], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k3, m3 <span class="kw">in</span> m2.named_children():</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="st">"downsample"</span> <span class="kw">in</span> k3:  <span class="co"># fuse downsample</span></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>                    fuse_modules(m3, [[<span class="st">"0"</span>, <span class="st">"1"</span>]], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>quant_model_loaded.train()</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>quant_model_loaded.backbone.body.qconfig <span class="op">=</span> torch.quantization.get_default_qconfig(<span class="st">'fbgemm'</span>)</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>torch.quantization.prepare_qat(quant_model_loaded, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>torch.quantization.convert(quant_model_loaded, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>quant_model_loaded.<span class="bu">eval</span>()</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>quant_model_loaded.load_state_dict(torch.load(quant_model_101_path, map_location<span class="op">=</span>torch.device(<span class="st">'cpu'</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="d1210437-97af-4667-b4d8-8208f6b7df35" data-execution_count="267">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.utils <span class="im">import</span> draw_bounding_boxes, draw_segmentation_masks</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> read_image(<span class="st">"PennFudanPed/PNGImages/FudanPed00022.png"</span>)  <span class="co"># 7, 22</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>eval_transform <span class="op">=</span> get_transform(train<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> eval_transform(image)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convert RGBA -&gt; RGB and move to device</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x[:<span class="dv">3</span>, ...].to(torch.device(<span class="st">'cpu'</span>))</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> quant_model_loaded([x, ])</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> predictions[<span class="dv">0</span>]</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>threshold <span class="op">=</span> <span class="fl">0.80</span></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> (<span class="fl">255.0</span> <span class="op">*</span> (image <span class="op">-</span> image.<span class="bu">min</span>()) <span class="op">/</span> (image.<span class="bu">max</span>() <span class="op">-</span> image.<span class="bu">min</span>())).to(torch.uint8)</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> image[:<span class="dv">3</span>, ...]</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>pred_labels <span class="op">=</span> [<span class="ss">f"pedestrian: </span><span class="sc">{</span>score<span class="sc">:.3f}</span><span class="ss">"</span> <span class="cf">for</span> label, score <span class="kw">in</span> <span class="bu">zip</span>(pred[<span class="st">"labels"</span>], pred[<span class="st">"scores"</span>]) <span class="cf">if</span> score <span class="op">&gt;</span> threshold]</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>pred_boxes <span class="op">=</span> pred[<span class="st">"boxes"</span>].<span class="bu">long</span>()[pred[<span class="st">"scores"</span>] <span class="op">&gt;</span> threshold]</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>output_image <span class="op">=</span> draw_bounding_boxes(image, pred_boxes, pred_labels, colors<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a><span class="co"># masks = (pred["masks"] &gt; 0.7).squeeze(1)</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a><span class="co"># output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors="blue")</span></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">12</span>))</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>plt.imshow(output_image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="267">
<pre><code>&lt;matplotlib.image.AxesImage at 0x7e43de5482e0&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="rcnn_quantization_files/figure-html/cell-24-output-2.png" class="img-fluid"></p>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>