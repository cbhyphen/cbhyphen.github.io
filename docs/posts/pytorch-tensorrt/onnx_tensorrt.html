<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="chris">
<meta name="dcterms.date" content="2024-05-27">

<title>cbhyphen.github.io - Speeding up Inference with TensorRT</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">cbhyphen.github.io</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">about</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cbhyphen" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Speeding up Inference with TensorRT</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>chris </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 27, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>follow up post to pytorch quantization … can we make it faster with GPU and TensorRT</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># random colab error: "A UTF-8 locale is required. Got ANSI_X3.4-1968"</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># https://github.com/googlecolab/colabtools/issues/3409</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> locale</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>locale.getpreferredencoding <span class="op">=</span> <span class="kw">lambda</span>: <span class="st">"UTF-8"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>get FasterRCNN as before with a resnet101 backbone…</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.resnet <span class="im">import</span> ResNet, Bottleneck, ResNet101_Weights</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models._utils <span class="im">import</span> IntermediateLayerGetter</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.detection.backbone_utils <span class="im">import</span> BackboneWithFPN</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.detection.faster_rcnn <span class="im">import</span> FasterRCNN</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> resnet_101():</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    resnet <span class="op">=</span> ResNet(block<span class="op">=</span>Bottleneck, layers<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">23</span>, <span class="dv">3</span>])</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    resnet.load_state_dict(ResNet101_Weights.DEFAULT.get_state_dict(progress<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> resnet</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>resnet <span class="op">=</span> resnet_101()</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co"># same as before, get intermediate layers and their output dimensions</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>returned_layers <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>return_layers <span class="op">=</span> {<span class="ss">f"layer</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>: <span class="bu">str</span>(v) <span class="cf">for</span> v, k <span class="kw">in</span> <span class="bu">enumerate</span>(returned_layers)}</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>in_channels_list <span class="op">=</span> []</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k1, m1 <span class="kw">in</span> resnet.named_children():</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'layer'</span> <span class="kw">in</span> k1:</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        in_channels_list.append((m1[<span class="op">-</span><span class="dv">1</span>].bn3.num_features))</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>rcnn <span class="op">=</span> FasterRCNN(</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    BackboneWithFPN(</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        backbone<span class="op">=</span>resnet,</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        return_layers<span class="op">=</span>return_layers,</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        in_channels_list<span class="op">=</span>in_channels_list,</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        out_channels<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        extra_blocks<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        norm_layer<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    num_classes<span class="op">=</span><span class="dv">2</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>rcnn.<span class="bu">eval</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>time the RCNN on both CPU and GPU. I don’t recall what the specs were the last time I used colab to profile the inference time so I’ll document that here as well. I’m using a T4 GPU and the following CPU</p>
<div class="cell" data-outputid="fa2236d5-0840-479b-abff-b1bae1ea82d8" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># !cat /proc/cpuinfo  | grep 'name' | uniq</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>lscpu <span class="op">|</span> grep <span class="st">'name'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz</code></pre>
</div>
</div>
<div class="cell" data-outputid="fc5a4674-a2ca-49d8-dbc9-db204c3a9e6f" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>nvidia<span class="op">-</span>smi <span class="op">-</span>L</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>GPU 0: NVIDIA L4 (UUID: GPU-393b8fe1-1ca8-7aaf-94b9-04eef8e2fda5)</code></pre>
</div>
</div>
<div class="cell" data-outputid="0f904955-6634-4ed8-cd69-3f27840ada77" data-execution_count="109">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># random image</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> torch.rand(<span class="dv">3</span>, <span class="dv">200</span>, <span class="dv">200</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># put on CPU</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>rcnn.to(torch.device(<span class="st">'cpu'</span>))</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>image_cpu <span class="op">=</span> image.to(torch.device(<span class="st">'cpu'</span>))</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    cpu_time <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>o rcnn([image_cpu])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.47 s ± 137 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre>
</div>
</div>
<div class="cell" data-outputid="26a679b0-e7f7-4185-f34c-bc6b294e984e" data-execution_count="110">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> copy <span class="im">import</span> deepcopy</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># on GPU</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>rcnn_gpu <span class="op">=</span> deepcopy(rcnn).to(torch.device(<span class="st">'cuda'</span>))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># rcnn.to(torch.device('cuda'))</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>image_gpu <span class="op">=</span> image.to(torch.device(<span class="st">'cuda'</span>))</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    gpu_time <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>o rcnn_gpu([image_gpu])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>37.9 ms ± 235 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre>
</div>
</div>
<p>we can also test with half precision…</p>
<div class="cell" data-outputid="fc71979d-2d72-4c39-f0fd-4e5d22769230" data-execution_count="111">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>rcnn_gpu_half <span class="op">=</span> rcnn_gpu.half().to(torch.device(<span class="st">'cuda'</span>))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>input_half <span class="op">=</span> image_gpu.half()</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    gpu_half_time <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>o rcnn_gpu_half([input_half])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>29.1 ms ± 1.13 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre>
</div>
</div>
<p>also re-clock the quantized model using FX Graph Mode since it’s performance is also CPU specific</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.ao.quantization <span class="im">import</span> quantize_fx</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.ao.quantization.qconfig_mapping <span class="im">import</span> get_default_qconfig_mapping</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>quant_rcnn <span class="op">=</span> deepcopy(rcnn)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>qconfig_mapping <span class="op">=</span> get_default_qconfig_mapping(<span class="st">"fbgemm"</span>)  <span class="co"># "qnnpack"</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co"># assume calibrated already</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>quant_rcnn.<span class="bu">eval</span>()</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>quant_rcnn.to(torch.device(<span class="st">'cpu'</span>))</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># prepare and quantize</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>example_input <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">200</span>, <span class="dv">200</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>quant_rcnn.backbone <span class="op">=</span> quantize_fx.prepare_fx(quant_rcnn.backbone, qconfig_mapping, example_input)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>quant_rcnn.backbone <span class="op">=</span> quantize_fx.convert_fx(quant_rcnn.backbone)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>script_module <span class="op">=</span> torch.jit.script(quant_rcnn)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>script_module.save(<span class="st">"./quant_rcnn.pt"</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>quant_rcnn_jit <span class="op">=</span> torch.jit.load(<span class="st">"./quant_rcnn.pt"</span>, map_location<span class="op">=</span>torch.device(<span class="st">'cpu'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="da6c8f97-c132-47c0-d677-b81839aa1003" data-execution_count="112">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># warmup</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> warnings.catch_warnings():</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    warnings.simplefilter(<span class="st">'ignore'</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        __ <span class="op">=</span> quant_rcnn_jit([image_cpu])</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    quant_time <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>o quant_rcnn_jit([image_cpu])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>652 ms ± 81 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre>
</div>
</div>
<p>Below I convert the float model to onnx. I went through onnx because that used to be the preferred way of converting to TensorRT. However, the onnx conversion didn’t play well with the <code>trtexec</code> command line utility for TensorRT regardless of the torch to onnx exporter used. Below the <a href="https://pytorch.org/docs/stable/onnx_torchscript.html">old torch script onnx converter</a> is used but the <a href="https://pytorch.org/docs/stable/onnx_dynamo.html">newer ‘dynamo’ converter</a> also had issues. Thankfully PyTorch has a very easy TensorRT API now, but I keep the ONNX model and evaluate it to see if a simple conversion offers any benefits.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install onnx</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install onnxruntime</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="bba7b721-159f-4c2b-ce53-ae5ca800489b" data-execution_count="8">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> onnx</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>torch.onnx.export(</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    deepcopy(rcnn),</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># onnx wants a tuple of 2 or bombs:  https://github.com/zhiqwang/yolort/issues/485</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    ([torch.randn(<span class="dv">3</span>, <span class="dv">200</span>, <span class="dv">200</span>)], ),</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"rcnn.onnx"</span>,</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># do_constant_folding=True,</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    opset_version <span class="op">=</span> <span class="dv">11</span>,</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co"># make sure the onnx proto is valid</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>rcnn_onnx <span class="op">=</span> onnx.load(<span class="st">"rcnn.onnx"</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>onnx.checker.check_model(rcnn_onnx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4009: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  (torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i], dtype=torch.float32)).float()))
/usr/local/lib/python3.10/dist-packages/torchvision/ops/boxes.py:166: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))
/usr/local/lib/python3.10/dist-packages/torchvision/ops/boxes.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))
/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1559: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert condition, message
/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/transform.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(s, dtype=torch.float32, device=boxes.device)
/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/transform.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  / torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)
/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset9.py:5858: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.
  warnings.warn(</code></pre>
</div>
</div>
<p>run inference on onnx model, make sure outputs are as expected, then clock-it…</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> onnxruntime</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>ort_session <span class="op">=</span> onnxruntime.InferenceSession(<span class="st">"rcnn.onnx"</span>, providers<span class="op">=</span>[<span class="st">"CPUExecutionProvider"</span>])</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># good to make sure inputs are as expected: '[i.name for i in ort_session.get_inputs()]'</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"># onnx wants numpy tensor not torch tensor</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> to_numpy(tensor):</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tensor.detach().cpu().numpy() <span class="cf">if</span> tensor.requires_grad <span class="cf">else</span> tensor.cpu().numpy()</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co"># get a prediction.  onnx doesn't need a list input like torch model does</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>ort_inputs <span class="op">=</span> {ort_session.get_inputs()[<span class="dv">0</span>].name: to_numpy(image)}</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>ort_outs <span class="op">=</span> ort_session.run(<span class="va">None</span>, ort_inputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="c63ab32c-123a-421b-e5ed-2695362647f0" data-execution_count="22">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># onxx outputs are list of three arrays corresponding to 'boxes', 'labels', and 'scores'</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"onnx out shapes: "</span>, [arr.shape <span class="cf">for</span> arr <span class="kw">in</span> ort_outs])</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># quant model out is tuple of (losses, outputs)</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>torch_outs <span class="op">=</span> __[<span class="dv">1</span>][<span class="dv">0</span>]</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"torch out shapes: "</span>, [torch_outs[k].shape <span class="cf">for</span> k <span class="kw">in</span> torch_outs])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>onnx out shapes:  [(100, 4), (100,), (100,)]
torch out shapes:  [torch.Size([100, 4]), torch.Size([100]), torch.Size([100])]</code></pre>
</div>
</div>
<div class="cell" data-outputid="45740ebf-76f6-47eb-8f1c-dfec0fa29f55" data-execution_count="113">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>onnx_time <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>o ort_session.run(<span class="va">None</span>, ort_inputs)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co"># sess = onnxruntime.InferenceSession('rcnn.onnx', providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider'])</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># onnx_trt_time = %timeit -o sess.run(None, ort_inputs)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.05 s ± 114 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># more steps for using trtexec which has issues with rcnn input shape</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !sudo apt-get install tensorrt</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install tensorrt</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># !ls /usr/src/tensorrt/bin  # make sure trtexec is there</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co"># !/usr/src/tensorrt/bin/trtexec --onnx=rcnn.onnx --saveEngine=rcnn_engine_pytorch.trt</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>use the handy <code>torch-tensorrt</code> package…</p>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>python <span class="op">-</span>m pip install torch<span class="op">-</span>tensorrt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>rcnn.to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch_tensorrt</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co"># need to wrap rcnn inputs in list</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> [[torch.randn(<span class="dv">3</span>, <span class="dv">200</span>, <span class="dv">200</span>).to(<span class="st">"cuda"</span>)]]  <span class="co"># .half()]</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>trt_model <span class="op">=</span> torch_tensorrt.<span class="bu">compile</span>(</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    deepcopy(rcnn),</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    ir<span class="op">=</span><span class="st">"torch_compile"</span>,</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># frontend api below complains about input shape</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># backend="torch_tensorrt",</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>inputs,</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    enabled_precisions<span class="op">=</span>{torch.float32},  <span class="co">#  {torch.half}</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    debug<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    workspace_size<span class="op">=</span><span class="dv">20</span> <span class="op">&lt;&lt;</span> <span class="dv">30</span>,</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    min_block_size<span class="op">=</span><span class="dv">7</span>,</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    torch_executed_ops<span class="op">=</span>{},</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="45273ee3-5d63-4633-8c3a-63ea3fb13571" data-execution_count="93">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># contrary to docs, first run actually compiles model</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># https://pytorch.org/TensorRT/tutorials/_rendered_examples/dynamo/torch_compile_resnet_example.html#torch-compile-resnet</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> trt_model(<span class="op">*</span>inputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Streaming output truncated to the last 5000 lines.
    %_tensor_constant140 : [num_users=1] = get_attr[target=_tensor_constant140]
    %_tensor_constant141 : [num_users=1] = get_attr[target=_tensor_constant141]
    %_native_batch_norm_legit_no_training_69 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_69, %_param_constant208, %_param_constant209, %_tensor_constant140, %_tensor_constant141, 0.1, 1e-05), kwargs = {})
    %getitem_209 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_69, 0), kwargs = {})
    %add_28 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_209, %relu_63), kwargs = {})
    %relu_66 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_28,), kwargs = {})
    %_param_constant210 : [num_users=1] = get_attr[target=_param_constant210]
    %convolution_70 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_66, %_param_constant210, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant211 : [num_users=1] = get_attr[target=_param_constant211]
    %_param_constant212 : [num_users=1] = get_attr[target=_param_constant212]
    %_tensor_constant142 : [num_users=1] = get_attr[target=_tensor_constant142]
    %_tensor_constant143 : [num_users=1] = get_attr[target=_tensor_constant143]
    %_native_batch_norm_legit_no_training_70 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_70, %_param_constant211, %_param_constant212, %_tensor_constant142, %_tensor_constant143, 0.1, 1e-05), kwargs = {})
    %getitem_212 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_70, 0), kwargs = {})
    %relu_67 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_212,), kwargs = {})
    %_param_constant213 : [num_users=1] = get_attr[target=_param_constant213]
    %convolution_71 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_67, %_param_constant213, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant214 : [num_users=1] = get_attr[target=_param_constant214]
    %_param_constant215 : [num_users=1] = get_attr[target=_param_constant215]
    %_tensor_constant144 : [num_users=1] = get_attr[target=_tensor_constant144]
    %_tensor_constant145 : [num_users=1] = get_attr[target=_tensor_constant145]
    %_native_batch_norm_legit_no_training_71 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_71, %_param_constant214, %_param_constant215, %_tensor_constant144, %_tensor_constant145, 0.1, 1e-05), kwargs = {})
    %getitem_215 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_71, 0), kwargs = {})
    %relu_68 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_215,), kwargs = {})
    %_param_constant216 : [num_users=1] = get_attr[target=_param_constant216]
    %convolution_72 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_68, %_param_constant216, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant217 : [num_users=1] = get_attr[target=_param_constant217]
    %_param_constant218 : [num_users=1] = get_attr[target=_param_constant218]
    %_tensor_constant146 : [num_users=1] = get_attr[target=_tensor_constant146]
    %_tensor_constant147 : [num_users=1] = get_attr[target=_tensor_constant147]
    %_native_batch_norm_legit_no_training_72 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_72, %_param_constant217, %_param_constant218, %_tensor_constant146, %_tensor_constant147, 0.1, 1e-05), kwargs = {})
    %getitem_218 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_72, 0), kwargs = {})
    %add_29 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_218, %relu_66), kwargs = {})
    %relu_69 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_29,), kwargs = {})
    %_param_constant219 : [num_users=1] = get_attr[target=_param_constant219]
    %convolution_73 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_69, %_param_constant219, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant220 : [num_users=1] = get_attr[target=_param_constant220]
    %_param_constant221 : [num_users=1] = get_attr[target=_param_constant221]
    %_tensor_constant148 : [num_users=1] = get_attr[target=_tensor_constant148]
    %_tensor_constant149 : [num_users=1] = get_attr[target=_tensor_constant149]
    %_native_batch_norm_legit_no_training_73 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_73, %_param_constant220, %_param_constant221, %_tensor_constant148, %_tensor_constant149, 0.1, 1e-05), kwargs = {})
    %getitem_221 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_73, 0), kwargs = {})
    %relu_70 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_221,), kwargs = {})
    %_param_constant222 : [num_users=1] = get_attr[target=_param_constant222]
    %convolution_74 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_70, %_param_constant222, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant223 : [num_users=1] = get_attr[target=_param_constant223]
    %_param_constant224 : [num_users=1] = get_attr[target=_param_constant224]
    %_tensor_constant150 : [num_users=1] = get_attr[target=_tensor_constant150]
    %_tensor_constant151 : [num_users=1] = get_attr[target=_tensor_constant151]
    %_native_batch_norm_legit_no_training_74 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_74, %_param_constant223, %_param_constant224, %_tensor_constant150, %_tensor_constant151, 0.1, 1e-05), kwargs = {})
    %getitem_224 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_74, 0), kwargs = {})
    %relu_71 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_224,), kwargs = {})
    %_param_constant225 : [num_users=1] = get_attr[target=_param_constant225]
    %convolution_75 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_71, %_param_constant225, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant226 : [num_users=1] = get_attr[target=_param_constant226]
    %_param_constant227 : [num_users=1] = get_attr[target=_param_constant227]
    %_tensor_constant152 : [num_users=1] = get_attr[target=_tensor_constant152]
    %_tensor_constant153 : [num_users=1] = get_attr[target=_tensor_constant153]
    %_native_batch_norm_legit_no_training_75 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_75, %_param_constant226, %_param_constant227, %_tensor_constant152, %_tensor_constant153, 0.1, 1e-05), kwargs = {})
    %getitem_227 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_75, 0), kwargs = {})
    %add_30 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_227, %relu_69), kwargs = {})
    %relu_72 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_30,), kwargs = {})
    %_param_constant228 : [num_users=1] = get_attr[target=_param_constant228]
    %convolution_76 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_72, %_param_constant228, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant229 : [num_users=1] = get_attr[target=_param_constant229]
    %_param_constant230 : [num_users=1] = get_attr[target=_param_constant230]
    %_tensor_constant154 : [num_users=1] = get_attr[target=_tensor_constant154]
    %_tensor_constant155 : [num_users=1] = get_attr[target=_tensor_constant155]
    %_native_batch_norm_legit_no_training_76 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_76, %_param_constant229, %_param_constant230, %_tensor_constant154, %_tensor_constant155, 0.1, 1e-05), kwargs = {})
    %getitem_230 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_76, 0), kwargs = {})
    %relu_73 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_230,), kwargs = {})
    %_param_constant231 : [num_users=1] = get_attr[target=_param_constant231]
    %convolution_77 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_73, %_param_constant231, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant232 : [num_users=1] = get_attr[target=_param_constant232]
    %_param_constant233 : [num_users=1] = get_attr[target=_param_constant233]
    %_tensor_constant156 : [num_users=1] = get_attr[target=_tensor_constant156]
    %_tensor_constant157 : [num_users=1] = get_attr[target=_tensor_constant157]
    %_native_batch_norm_legit_no_training_77 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_77, %_param_constant232, %_param_constant233, %_tensor_constant156, %_tensor_constant157, 0.1, 1e-05), kwargs = {})
    %getitem_233 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_77, 0), kwargs = {})
    %relu_74 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_233,), kwargs = {})
    %_param_constant234 : [num_users=1] = get_attr[target=_param_constant234]
    %convolution_78 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_74, %_param_constant234, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant235 : [num_users=1] = get_attr[target=_param_constant235]
    %_param_constant236 : [num_users=1] = get_attr[target=_param_constant236]
    %_tensor_constant158 : [num_users=1] = get_attr[target=_tensor_constant158]
    %_tensor_constant159 : [num_users=1] = get_attr[target=_tensor_constant159]
    %_native_batch_norm_legit_no_training_78 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_78, %_param_constant235, %_param_constant236, %_tensor_constant158, %_tensor_constant159, 0.1, 1e-05), kwargs = {})
    %getitem_236 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_78, 0), kwargs = {})
    %add_31 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_236, %relu_72), kwargs = {})
    %relu_75 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_31,), kwargs = {})
    %_param_constant237 : [num_users=1] = get_attr[target=_param_constant237]
    %convolution_79 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_75, %_param_constant237, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant238 : [num_users=1] = get_attr[target=_param_constant238]
    %_param_constant239 : [num_users=1] = get_attr[target=_param_constant239]
    %_tensor_constant160 : [num_users=1] = get_attr[target=_tensor_constant160]
    %_tensor_constant161 : [num_users=1] = get_attr[target=_tensor_constant161]
    %_native_batch_norm_legit_no_training_79 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_79, %_param_constant238, %_param_constant239, %_tensor_constant160, %_tensor_constant161, 0.1, 1e-05), kwargs = {})
    %getitem_239 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_79, 0), kwargs = {})
    %relu_76 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_239,), kwargs = {})
    %_param_constant240 : [num_users=1] = get_attr[target=_param_constant240]
    %convolution_80 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_76, %_param_constant240, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant241 : [num_users=1] = get_attr[target=_param_constant241]
    %_param_constant242 : [num_users=1] = get_attr[target=_param_constant242]
    %_tensor_constant162 : [num_users=1] = get_attr[target=_tensor_constant162]
    %_tensor_constant163 : [num_users=1] = get_attr[target=_tensor_constant163]
    %_native_batch_norm_legit_no_training_80 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_80, %_param_constant241, %_param_constant242, %_tensor_constant162, %_tensor_constant163, 0.1, 1e-05), kwargs = {})
    %getitem_242 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_80, 0), kwargs = {})
    %relu_77 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_242,), kwargs = {})
    %_param_constant243 : [num_users=1] = get_attr[target=_param_constant243]
    %convolution_81 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_77, %_param_constant243, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant244 : [num_users=1] = get_attr[target=_param_constant244]
    %_param_constant245 : [num_users=1] = get_attr[target=_param_constant245]
    %_tensor_constant164 : [num_users=1] = get_attr[target=_tensor_constant164]
    %_tensor_constant165 : [num_users=1] = get_attr[target=_tensor_constant165]
    %_native_batch_norm_legit_no_training_81 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_81, %_param_constant244, %_param_constant245, %_tensor_constant164, %_tensor_constant165, 0.1, 1e-05), kwargs = {})
    %getitem_245 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_81, 0), kwargs = {})
    %add_32 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_245, %relu_75), kwargs = {})
    %relu_78 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_32,), kwargs = {})
    %_param_constant246 : [num_users=1] = get_attr[target=_param_constant246]
    %convolution_82 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_78, %_param_constant246, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant247 : [num_users=1] = get_attr[target=_param_constant247]
    %_param_constant248 : [num_users=1] = get_attr[target=_param_constant248]
    %_tensor_constant166 : [num_users=1] = get_attr[target=_tensor_constant166]
    %_tensor_constant167 : [num_users=1] = get_attr[target=_tensor_constant167]
    %_native_batch_norm_legit_no_training_82 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_82, %_param_constant247, %_param_constant248, %_tensor_constant166, %_tensor_constant167, 0.1, 1e-05), kwargs = {})
    %getitem_248 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_82, 0), kwargs = {})
    %relu_79 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_248,), kwargs = {})
    %_param_constant249 : [num_users=1] = get_attr[target=_param_constant249]
    %convolution_83 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_79, %_param_constant249, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant250 : [num_users=1] = get_attr[target=_param_constant250]
    %_param_constant251 : [num_users=1] = get_attr[target=_param_constant251]
    %_tensor_constant168 : [num_users=1] = get_attr[target=_tensor_constant168]
    %_tensor_constant169 : [num_users=1] = get_attr[target=_tensor_constant169]
    %_native_batch_norm_legit_no_training_83 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_83, %_param_constant250, %_param_constant251, %_tensor_constant168, %_tensor_constant169, 0.1, 1e-05), kwargs = {})
    %getitem_251 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_83, 0), kwargs = {})
    %relu_80 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_251,), kwargs = {})
    %_param_constant252 : [num_users=1] = get_attr[target=_param_constant252]
    %convolution_84 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_80, %_param_constant252, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant253 : [num_users=1] = get_attr[target=_param_constant253]
    %_param_constant254 : [num_users=1] = get_attr[target=_param_constant254]
    %_tensor_constant170 : [num_users=1] = get_attr[target=_tensor_constant170]
    %_tensor_constant171 : [num_users=1] = get_attr[target=_tensor_constant171]
    %_native_batch_norm_legit_no_training_84 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_84, %_param_constant253, %_param_constant254, %_tensor_constant170, %_tensor_constant171, 0.1, 1e-05), kwargs = {})
    %getitem_254 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_84, 0), kwargs = {})
    %add_33 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_254, %relu_78), kwargs = {})
    %relu_81 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_33,), kwargs = {})
    %_param_constant255 : [num_users=1] = get_attr[target=_param_constant255]
    %convolution_85 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_81, %_param_constant255, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant256 : [num_users=1] = get_attr[target=_param_constant256]
    %_param_constant257 : [num_users=1] = get_attr[target=_param_constant257]
    %_tensor_constant172 : [num_users=1] = get_attr[target=_tensor_constant172]
    %_tensor_constant173 : [num_users=1] = get_attr[target=_tensor_constant173]
    %_native_batch_norm_legit_no_training_85 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_85, %_param_constant256, %_param_constant257, %_tensor_constant172, %_tensor_constant173, 0.1, 1e-05), kwargs = {})
    %getitem_257 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_85, 0), kwargs = {})
    %relu_82 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_257,), kwargs = {})
    %_param_constant258 : [num_users=1] = get_attr[target=_param_constant258]
    %convolution_86 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_82, %_param_constant258, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant259 : [num_users=1] = get_attr[target=_param_constant259]
    %_param_constant260 : [num_users=1] = get_attr[target=_param_constant260]
    %_tensor_constant174 : [num_users=1] = get_attr[target=_tensor_constant174]
    %_tensor_constant175 : [num_users=1] = get_attr[target=_tensor_constant175]
    %_native_batch_norm_legit_no_training_86 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_86, %_param_constant259, %_param_constant260, %_tensor_constant174, %_tensor_constant175, 0.1, 1e-05), kwargs = {})
    %getitem_260 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_86, 0), kwargs = {})
    %relu_83 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_260,), kwargs = {})
    %_param_constant261 : [num_users=1] = get_attr[target=_param_constant261]
    %convolution_87 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_83, %_param_constant261, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant262 : [num_users=1] = get_attr[target=_param_constant262]
    %_param_constant263 : [num_users=1] = get_attr[target=_param_constant263]
    %_tensor_constant176 : [num_users=1] = get_attr[target=_tensor_constant176]
    %_tensor_constant177 : [num_users=1] = get_attr[target=_tensor_constant177]
    %_native_batch_norm_legit_no_training_87 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_87, %_param_constant262, %_param_constant263, %_tensor_constant176, %_tensor_constant177, 0.1, 1e-05), kwargs = {})
    %getitem_263 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_87, 0), kwargs = {})
    %add_34 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_263, %relu_81), kwargs = {})
    %relu_84 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_34,), kwargs = {})
    %_param_constant264 : [num_users=1] = get_attr[target=_param_constant264]
    %convolution_88 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_84, %_param_constant264, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant265 : [num_users=1] = get_attr[target=_param_constant265]
    %_param_constant266 : [num_users=1] = get_attr[target=_param_constant266]
    %_tensor_constant178 : [num_users=1] = get_attr[target=_tensor_constant178]
    %_tensor_constant179 : [num_users=1] = get_attr[target=_tensor_constant179]
    %_native_batch_norm_legit_no_training_88 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_88, %_param_constant265, %_param_constant266, %_tensor_constant178, %_tensor_constant179, 0.1, 1e-05), kwargs = {})
    %getitem_266 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_88, 0), kwargs = {})
    %relu_85 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_266,), kwargs = {})
    %_param_constant267 : [num_users=1] = get_attr[target=_param_constant267]
    %convolution_89 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_85, %_param_constant267, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant268 : [num_users=1] = get_attr[target=_param_constant268]
    %_param_constant269 : [num_users=1] = get_attr[target=_param_constant269]
    %_tensor_constant180 : [num_users=1] = get_attr[target=_tensor_constant180]
    %_tensor_constant181 : [num_users=1] = get_attr[target=_tensor_constant181]
    %_native_batch_norm_legit_no_training_89 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_89, %_param_constant268, %_param_constant269, %_tensor_constant180, %_tensor_constant181, 0.1, 1e-05), kwargs = {})
    %getitem_269 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_89, 0), kwargs = {})
    %relu_86 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_269,), kwargs = {})
    %_param_constant270 : [num_users=1] = get_attr[target=_param_constant270]
    %convolution_90 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_86, %_param_constant270, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant271 : [num_users=1] = get_attr[target=_param_constant271]
    %_param_constant272 : [num_users=1] = get_attr[target=_param_constant272]
    %_tensor_constant182 : [num_users=1] = get_attr[target=_tensor_constant182]
    %_tensor_constant183 : [num_users=1] = get_attr[target=_tensor_constant183]
    %_native_batch_norm_legit_no_training_90 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_90, %_param_constant271, %_param_constant272, %_tensor_constant182, %_tensor_constant183, 0.1, 1e-05), kwargs = {})
    %getitem_272 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_90, 0), kwargs = {})
    %add_35 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_272, %relu_84), kwargs = {})
    %relu_87 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_35,), kwargs = {})
    %_param_constant273 : [num_users=1] = get_attr[target=_param_constant273]
    %convolution_91 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_87, %_param_constant273, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant274 : [num_users=1] = get_attr[target=_param_constant274]
    %_param_constant275 : [num_users=1] = get_attr[target=_param_constant275]
    %_tensor_constant184 : [num_users=1] = get_attr[target=_tensor_constant184]
    %_tensor_constant185 : [num_users=1] = get_attr[target=_tensor_constant185]
    %_native_batch_norm_legit_no_training_91 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_91, %_param_constant274, %_param_constant275, %_tensor_constant184, %_tensor_constant185, 0.1, 1e-05), kwargs = {})
    %getitem_275 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_91, 0), kwargs = {})
    %relu_88 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_275,), kwargs = {})
    %_param_constant276 : [num_users=1] = get_attr[target=_param_constant276]
    %convolution_92 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_88, %_param_constant276, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant277 : [num_users=1] = get_attr[target=_param_constant277]
    %_param_constant278 : [num_users=1] = get_attr[target=_param_constant278]
    %_tensor_constant186 : [num_users=1] = get_attr[target=_tensor_constant186]
    %_tensor_constant187 : [num_users=1] = get_attr[target=_tensor_constant187]
    %_native_batch_norm_legit_no_training_92 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_92, %_param_constant277, %_param_constant278, %_tensor_constant186, %_tensor_constant187, 0.1, 1e-05), kwargs = {})
    %getitem_278 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_92, 0), kwargs = {})
    %relu_89 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_278,), kwargs = {})
    %_param_constant279 : [num_users=1] = get_attr[target=_param_constant279]
    %convolution_93 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_89, %_param_constant279, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant280 : [num_users=1] = get_attr[target=_param_constant280]
    %_param_constant281 : [num_users=1] = get_attr[target=_param_constant281]
    %_tensor_constant188 : [num_users=1] = get_attr[target=_tensor_constant188]
    %_tensor_constant189 : [num_users=1] = get_attr[target=_tensor_constant189]
    %_native_batch_norm_legit_no_training_93 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_93, %_param_constant280, %_param_constant281, %_tensor_constant188, %_tensor_constant189, 0.1, 1e-05), kwargs = {})
    %getitem_281 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_93, 0), kwargs = {})
    %add_36 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_281, %relu_87), kwargs = {})
    %relu_90 : [num_users=3] = call_function[target=torch.ops.aten.relu.default](args = (%add_36,), kwargs = {})
    %_param_constant282 : [num_users=1] = get_attr[target=_param_constant282]
    %convolution_94 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_90, %_param_constant282, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant283 : [num_users=1] = get_attr[target=_param_constant283]
    %_param_constant284 : [num_users=1] = get_attr[target=_param_constant284]
    %_tensor_constant190 : [num_users=1] = get_attr[target=_tensor_constant190]
    %_tensor_constant191 : [num_users=1] = get_attr[target=_tensor_constant191]
    %_native_batch_norm_legit_no_training_94 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_94, %_param_constant283, %_param_constant284, %_tensor_constant190, %_tensor_constant191, 0.1, 1e-05), kwargs = {})
    %getitem_284 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_94, 0), kwargs = {})
    %relu_91 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_284,), kwargs = {})
    %_param_constant285 : [num_users=1] = get_attr[target=_param_constant285]
    %convolution_95 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_91, %_param_constant285, None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant286 : [num_users=1] = get_attr[target=_param_constant286]
    %_param_constant287 : [num_users=1] = get_attr[target=_param_constant287]
    %_tensor_constant192 : [num_users=1] = get_attr[target=_tensor_constant192]
    %_tensor_constant193 : [num_users=1] = get_attr[target=_tensor_constant193]
    %_native_batch_norm_legit_no_training_95 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_95, %_param_constant286, %_param_constant287, %_tensor_constant192, %_tensor_constant193, 0.1, 1e-05), kwargs = {})
    %getitem_287 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_95, 0), kwargs = {})
    %relu_92 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_287,), kwargs = {})
    %_param_constant288 : [num_users=1] = get_attr[target=_param_constant288]
    %convolution_96 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_92, %_param_constant288, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant289 : [num_users=1] = get_attr[target=_param_constant289]
    %_param_constant290 : [num_users=1] = get_attr[target=_param_constant290]
    %_tensor_constant194 : [num_users=1] = get_attr[target=_tensor_constant194]
    %_tensor_constant195 : [num_users=1] = get_attr[target=_tensor_constant195]
    %_native_batch_norm_legit_no_training_96 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_96, %_param_constant289, %_param_constant290, %_tensor_constant194, %_tensor_constant195, 0.1, 1e-05), kwargs = {})
    %getitem_290 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_96, 0), kwargs = {})
    %_param_constant291 : [num_users=1] = get_attr[target=_param_constant291]
    %convolution_97 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_90, %_param_constant291, None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant292 : [num_users=1] = get_attr[target=_param_constant292]
    %_param_constant293 : [num_users=1] = get_attr[target=_param_constant293]
    %_tensor_constant196 : [num_users=1] = get_attr[target=_tensor_constant196]
    %_tensor_constant197 : [num_users=1] = get_attr[target=_tensor_constant197]
    %_native_batch_norm_legit_no_training_97 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_97, %_param_constant292, %_param_constant293, %_tensor_constant196, %_tensor_constant197, 0.1, 1e-05), kwargs = {})
    %getitem_293 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_97, 0), kwargs = {})
    %add_37 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_290, %getitem_293), kwargs = {})
    %relu_93 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_37,), kwargs = {})
    %_param_constant294 : [num_users=1] = get_attr[target=_param_constant294]
    %convolution_98 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_93, %_param_constant294, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant295 : [num_users=1] = get_attr[target=_param_constant295]
    %_param_constant296 : [num_users=1] = get_attr[target=_param_constant296]
    %_tensor_constant198 : [num_users=1] = get_attr[target=_tensor_constant198]
    %_tensor_constant199 : [num_users=1] = get_attr[target=_tensor_constant199]
    %_native_batch_norm_legit_no_training_98 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_98, %_param_constant295, %_param_constant296, %_tensor_constant198, %_tensor_constant199, 0.1, 1e-05), kwargs = {})
    %getitem_296 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_98, 0), kwargs = {})
    %relu_94 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_296,), kwargs = {})
    %_param_constant297 : [num_users=1] = get_attr[target=_param_constant297]
    %convolution_99 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_94, %_param_constant297, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant298 : [num_users=1] = get_attr[target=_param_constant298]
    %_param_constant299 : [num_users=1] = get_attr[target=_param_constant299]
    %_tensor_constant200 : [num_users=1] = get_attr[target=_tensor_constant200]
    %_tensor_constant201 : [num_users=1] = get_attr[target=_tensor_constant201]
    %_native_batch_norm_legit_no_training_99 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_99, %_param_constant298, %_param_constant299, %_tensor_constant200, %_tensor_constant201, 0.1, 1e-05), kwargs = {})
    %getitem_299 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_99, 0), kwargs = {})
    %relu_95 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_299,), kwargs = {})
    %_param_constant300 : [num_users=1] = get_attr[target=_param_constant300]
    %convolution_100 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_95, %_param_constant300, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant301 : [num_users=1] = get_attr[target=_param_constant301]
    %_param_constant302 : [num_users=1] = get_attr[target=_param_constant302]
    %_tensor_constant202 : [num_users=1] = get_attr[target=_tensor_constant202]
    %_tensor_constant203 : [num_users=1] = get_attr[target=_tensor_constant203]
    %_native_batch_norm_legit_no_training_100 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_100, %_param_constant301, %_param_constant302, %_tensor_constant202, %_tensor_constant203, 0.1, 1e-05), kwargs = {})
    %getitem_302 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_100, 0), kwargs = {})
    %add_38 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_302, %relu_93), kwargs = {})
    %relu_96 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add_38,), kwargs = {})
    %_param_constant303 : [num_users=1] = get_attr[target=_param_constant303]
    %convolution_101 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_96, %_param_constant303, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant304 : [num_users=1] = get_attr[target=_param_constant304]
    %_param_constant305 : [num_users=1] = get_attr[target=_param_constant305]
    %_tensor_constant204 : [num_users=1] = get_attr[target=_tensor_constant204]
    %_tensor_constant205 : [num_users=1] = get_attr[target=_tensor_constant205]
    %_native_batch_norm_legit_no_training_101 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_101, %_param_constant304, %_param_constant305, %_tensor_constant204, %_tensor_constant205, 0.1, 1e-05), kwargs = {})
    %getitem_305 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_101, 0), kwargs = {})
    %relu_97 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_305,), kwargs = {})
    %_param_constant306 : [num_users=1] = get_attr[target=_param_constant306]
    %convolution_102 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_97, %_param_constant306, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant307 : [num_users=1] = get_attr[target=_param_constant307]
    %_param_constant308 : [num_users=1] = get_attr[target=_param_constant308]
    %_tensor_constant206 : [num_users=1] = get_attr[target=_tensor_constant206]
    %_tensor_constant207 : [num_users=1] = get_attr[target=_tensor_constant207]
    %_native_batch_norm_legit_no_training_102 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_102, %_param_constant307, %_param_constant308, %_tensor_constant206, %_tensor_constant207, 0.1, 1e-05), kwargs = {})
    %getitem_308 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_102, 0), kwargs = {})
    %relu_98 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%getitem_308,), kwargs = {})
    %_param_constant309 : [num_users=1] = get_attr[target=_param_constant309]
    %convolution_103 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_98, %_param_constant309, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant310 : [num_users=1] = get_attr[target=_param_constant310]
    %_param_constant311 : [num_users=1] = get_attr[target=_param_constant311]
    %_tensor_constant208 : [num_users=1] = get_attr[target=_tensor_constant208]
    %_tensor_constant209 : [num_users=1] = get_attr[target=_tensor_constant209]
    %_native_batch_norm_legit_no_training_103 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%convolution_103, %_param_constant310, %_param_constant311, %_tensor_constant208, %_tensor_constant209, 0.1, 1e-05), kwargs = {})
    %getitem_311 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_no_training_103, 0), kwargs = {})
    %add_39 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_311, %relu_96), kwargs = {})
    %relu_99 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%add_39,), kwargs = {})
    %_param_constant312 : [num_users=1] = get_attr[target=_param_constant312]
    %_param_constant313 : [num_users=1] = get_attr[target=_param_constant313]
    %convolution_104 : [num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_99, %_param_constant312, %_param_constant313, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant314 : [num_users=1] = get_attr[target=_param_constant314]
    %_param_constant315 : [num_users=1] = get_attr[target=_param_constant315]
    %convolution_105 : [num_users=2] = call_function[target=torch.ops.aten.convolution.default](args = (%convolution_104, %_param_constant314, %_param_constant315, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant316 : [num_users=1] = get_attr[target=_param_constant316]
    %_param_constant317 : [num_users=1] = get_attr[target=_param_constant317]
    %convolution_106 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_90, %_param_constant316, %_param_constant317, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_frozen_param10 : [num_users=1] = get_attr[target=_frozen_param10]
    %_frozen_param11 : [num_users=1] = get_attr[target=_frozen_param11]
    %index_4 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor](args = (%convolution_104, [None, None, %_frozen_param10, %_frozen_param11]), kwargs = {})
    %add_42 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%convolution_106, %index_4), kwargs = {})
    %_param_constant318 : [num_users=1] = get_attr[target=_param_constant318]
    %_param_constant319 : [num_users=1] = get_attr[target=_param_constant319]
    %convolution_107 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%add_42, %_param_constant318, %_param_constant319, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant320 : [num_users=1] = get_attr[target=_param_constant320]
    %_param_constant321 : [num_users=1] = get_attr[target=_param_constant321]
    %convolution_108 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_21, %_param_constant320, %_param_constant321, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_frozen_param12 : [num_users=1] = get_attr[target=_frozen_param12]
    %_frozen_param13 : [num_users=1] = get_attr[target=_frozen_param13]
    %index_5 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor](args = (%add_42, [None, None, %_frozen_param12, %_frozen_param13]), kwargs = {})
    %add_45 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%convolution_108, %index_5), kwargs = {})
    %_param_constant322 : [num_users=1] = get_attr[target=_param_constant322]
    %_param_constant323 : [num_users=1] = get_attr[target=_param_constant323]
    %convolution_109 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%add_45, %_param_constant322, %_param_constant323, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant324 : [num_users=1] = get_attr[target=_param_constant324]
    %_param_constant325 : [num_users=1] = get_attr[target=_param_constant325]
    %convolution_110 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_9, %_param_constant324, %_param_constant325, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_frozen_param14 : [num_users=1] = get_attr[target=_frozen_param14]
    %_frozen_param15 : [num_users=1] = get_attr[target=_frozen_param15]
    %index_6 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor](args = (%add_45, [None, None, %_frozen_param14, %_frozen_param15]), kwargs = {})
    %add_48 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%convolution_110, %index_6), kwargs = {})
    %_param_constant326 : [num_users=1] = get_attr[target=_param_constant326]
    %_param_constant327 : [num_users=1] = get_attr[target=_param_constant327]
    %convolution_111 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%add_48, %_param_constant326, %_param_constant327, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %max_pool2d_default_1 : [num_users=1] = call_function[target=torch.ops.aten.max_pool2d.default](args = (%convolution_105, [1, 1], [2, 2]), kwargs = {})
    return (convolution_111, convolution_109, convolution_107, convolution_105, max_pool2d_default_1)
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Adding input to in-progress INetwork: select_scatter [shape=[1, 3, 800, 800], dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution (kind: aten.convolution.default, args: ('select_scatter &lt;tensorrt.ITensor [shape=(1, 3, 800, 800), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64, 3, 7, 7), dtype=float32]&gt;', None, [2, 2], [3, 3], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution]_output &lt;tensorrt.ITensor [shape=(1, 64, 400, 400), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training]_output &lt;tensorrt.ITensor [shape=(1, 64, 400, 400), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training]_output &lt;tensorrt.ITensor [shape=(1, 64, 400, 400), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node max_pool2d_default (kind: aten.max_pool2d.default, args: ('[RELU]-[aten_ops.relu.default]-[relu]_output &lt;tensorrt.ITensor [shape=(1, 64, 400, 400), dtype=DataType.FLOAT]&gt;', [3, 3], [2, 2], [1, 1]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_1 (kind: aten.convolution.default, args: ('[MAX]-[aten_ops.max_pool2d.default]-[max_pool2d_default]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64, 64, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_1 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_1]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_5 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_1]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_5
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_1 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_1]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_2 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_1]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64, 64, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_2 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_2]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_8 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_2]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_8
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_2 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_2]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_3 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_2]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 64, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_3 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_3]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_11 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_3]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_11
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_4 (kind: aten.convolution.default, args: ('[MAX]-[aten_ops.max_pool2d.default]-[max_pool2d_default]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 64, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_4 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_4]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_14 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_4]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_14
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_7 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_3]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_4]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_3 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_7]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_5 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_3]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_5 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_5]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_17 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_5]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_17
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_4 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_5]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_6 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_4]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64, 64, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_6 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_6]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_20 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_6]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_20
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_5 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_6]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_7 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_5]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 64, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_7 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_7]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_23 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_7]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_23
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_8 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_7]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_3]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_6 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_8]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_8 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_6]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_8 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_8]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_26 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_8]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_26
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_7 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_8]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_9 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_7]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64, 64, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_9 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_9]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(64,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_29 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_9]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_29
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_8 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_9]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_10 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_8]_output &lt;tensorrt.ITensor [shape=(1, 64, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 64, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_10 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_10]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_32 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_10]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_32
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_9 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_10]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_6]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_9 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_9]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_11 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_9]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_11 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_11]_output &lt;tensorrt.ITensor [shape=(1, 128, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_35 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_11]_output &lt;tensorrt.ITensor [shape=(1, 128, 200, 200), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_35
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_10 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_11]_output &lt;tensorrt.ITensor [shape=(1, 128, 200, 200), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_12 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_10]_output &lt;tensorrt.ITensor [shape=(1, 128, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128, 128, 3, 3), dtype=float32]&gt;', None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_12 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_12]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_38 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_12]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_38
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_11 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_12]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_13 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_11]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512, 128, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_13 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_13]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_41 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_13]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_41
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_14 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_9]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512, 256, 1, 1), dtype=float32]&gt;', None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_14 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_14]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_44 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_14]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_44
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_10 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_13]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', '[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_14]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_12 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_10]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_15 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_12]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128, 512, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_15 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_15]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_47 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_15]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_47
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_13 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_15]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_16 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_13]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128, 128, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_16 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_16]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_50 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_16]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_50
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_14 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_16]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_17 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_14]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512, 128, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_17 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_17]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_53 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_17]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_53
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_11 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_17]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_12]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_15 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_11]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_18 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_15]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128, 512, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_18 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_18]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_56 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_18]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_56
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_16 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_18]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_19 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_16]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128, 128, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_19 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_19]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_59 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_19]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_59
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_17 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_19]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_20 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_17]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512, 128, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_20 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_20]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_62 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_20]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_62
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_12 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_20]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_15]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_18 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_12]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_21 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_18]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128, 512, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_21 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_21]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_65 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_21]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_65
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_19 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_21]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_22 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_19]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128, 128, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_22 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_22]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(128,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_68 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_22]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_68
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_20 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_22]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_23 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_20]_output &lt;tensorrt.ITensor [shape=(1, 128, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512, 128, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_23 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_23]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_71 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_23]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_71
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_13 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_23]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_18]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_21 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_13]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_24 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_21]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 512, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_24 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_24]_output &lt;tensorrt.ITensor [shape=(1, 256, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_74 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_24]_output &lt;tensorrt.ITensor [shape=(1, 256, 100, 100), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_74
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_22 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_24]_output &lt;tensorrt.ITensor [shape=(1, 256, 100, 100), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_25 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_22]_output &lt;tensorrt.ITensor [shape=(1, 256, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_25 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_25]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_77 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_25]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_77
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_23 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_25]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_26 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_23]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_26 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_26]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_80 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_26]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_80
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_27 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_21]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 512, 1, 1), dtype=float32]&gt;', None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_27 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_27]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_83 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_27]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_83
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_14 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_26]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_27]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_24 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_14]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_28 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_24]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_28 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_28]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_86 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_28]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_86
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_25 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_28]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_29 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_25]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_29 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_29]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_89 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_29]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_89
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_26 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_29]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_30 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_26]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_30 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_30]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_92 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_30]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_92
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_15 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_30]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_24]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_27 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_15]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_31 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_27]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_31 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_31]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_95 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_31]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_95
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_28 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_31]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_32 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_28]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_32 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_32]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_98 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_32]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_98
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_29 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_32]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_33 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_29]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_33 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_33]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_101 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_33]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_101
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_16 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_33]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_27]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_30 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_16]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_34 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_30]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_34 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_34]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_104 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_34]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_104
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_31 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_34]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_35 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_31]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_35 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_35]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_107 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_35]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_107
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_32 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_35]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_36 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_32]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_36 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_36]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_110 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_36]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_110
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_17 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_36]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_30]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_33 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_17]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_37 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_33]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_37 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_37]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_113 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_37]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_113
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_34 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_37]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_38 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_34]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_38 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_38]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_116 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_38]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_116
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_35 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_38]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_39 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_35]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_39 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_39]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_119 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_39]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_119
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_18 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_39]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_33]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_36 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_18]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_40 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_36]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_40 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_40]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_122 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_40]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_122
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_37 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_40]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_41 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_37]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_41 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_41]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_125 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_41]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_125
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_38 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_41]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_42 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_38]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_42 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_42]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_128 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_42]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_128
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_19 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_42]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_36]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_39 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_19]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_43 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_39]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_43 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_43]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_131 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_43]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_131
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_40 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_43]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_44 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_40]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_44 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_44]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_134 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_44]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_134
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_41 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_44]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_45 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_41]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_45 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_45]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_137 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_45]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_137
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_20 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_45]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_39]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_42 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_20]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_46 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_42]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_46 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_46]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_140 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_46]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_140
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_43 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_46]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_47 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_43]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_47 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_47]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_143 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_47]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_143
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_44 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_47]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_48 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_44]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_48 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_48]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_146 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_48]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_146
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_21 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_48]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_42]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_45 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_21]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_49 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_45]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_49 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_49]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_149 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_49]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_149
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_46 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_49]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_50 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_46]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_50 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_50]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_152 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_50]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_152
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_47 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_50]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_51 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_47]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_51 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_51]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_155 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_51]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_155
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_22 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_51]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_45]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_48 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_22]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_52 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_48]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_52 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_52]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_158 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_52]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_158
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_49 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_52]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_53 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_49]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_53 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_53]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_161 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_53]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_161
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_50 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_53]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_54 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_50]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_54 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_54]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_164 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_54]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_164
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_23 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_54]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_48]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_51 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_23]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_55 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_51]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_55 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_55]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_167 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_55]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_167
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_52 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_55]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_56 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_52]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_56 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_56]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_170 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_56]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_170
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_53 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_56]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_57 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_53]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_57 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_57]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_173 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_57]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_173
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_24 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_57]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_51]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_54 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_24]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_58 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_54]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_58 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_58]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_176 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_58]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_176
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_55 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_58]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_59 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_55]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_59 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_59]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_179 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_59]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_179
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_56 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_59]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_60 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_56]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_60 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_60]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_182 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_60]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_182
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_25 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_60]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_54]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_57 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_25]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_61 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_57]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_61 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_61]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_185 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_61]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_185
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_58 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_61]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_62 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_58]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_62 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_62]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_188 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_62]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_188
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_59 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_62]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_63 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_59]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_63 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_63]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_191 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_63]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_191
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_26 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_63]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_57]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_60 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_26]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_64 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_60]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_64 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_64]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_194 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_64]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_194
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_61 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_64]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_65 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_61]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_65 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_65]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_197 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_65]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_197
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_62 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_65]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_66 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_62]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_66 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_66]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_200 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_66]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_200
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_27 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_66]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_60]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_63 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_27]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_67 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_63]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_67 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_67]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_203 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_67]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_203
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_64 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_67]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_68 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_64]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_68 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_68]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_206 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_68]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_206
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_65 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_68]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_69 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_65]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_69 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_69]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_209 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_69]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_209
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_28 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_69]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_63]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_66 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_28]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_70 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_66]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_70 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_70]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_212 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_70]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_212
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_67 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_70]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_71 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_67]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_71 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_71]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_215 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_71]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_215
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_68 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_71]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_72 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_68]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_72 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_72]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_218 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_72]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_218
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_29 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_72]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_66]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_69 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_29]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_73 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_69]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_73 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_73]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_221 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_73]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_221
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_70 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_73]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_74 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_70]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_74 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_74]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_224 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_74]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_224
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_71 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_74]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_75 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_71]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_75 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_75]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_227 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_75]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_227
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_30 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_75]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_69]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_72 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_30]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_76 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_72]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_76 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_76]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_230 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_76]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_230
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_73 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_76]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_77 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_73]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_77 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_77]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_233 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_77]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_233
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_74 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_77]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_78 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_74]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_78 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_78]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_236 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_78]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_236
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_31 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_78]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_72]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_75 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_31]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_79 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_75]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_79 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_79]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_239 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_79]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_239
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_76 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_79]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_80 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_76]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_80 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_80]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_242 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_80]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_242
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_77 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_80]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_81 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_77]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_81 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_81]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_245 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_81]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_245
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_32 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_81]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_75]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_78 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_32]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_82 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_78]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_82 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_82]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_248 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_82]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_248
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_79 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_82]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_83 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_79]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_83 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_83]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_251 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_83]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_251
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_80 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_83]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_84 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_80]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_84 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_84]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_254 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_84]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_254
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_33 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_84]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_78]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_81 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_33]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_85 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_81]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_85 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_85]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_257 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_85]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_257
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_82 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_85]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_86 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_82]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_86 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_86]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_260 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_86]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_260
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_83 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_86]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_87 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_83]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_87 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_87]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_263 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_87]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_263
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_34 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_87]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_81]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_84 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_34]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_88 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_84]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_88 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_88]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_266 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_88]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_266
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_85 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_88]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_89 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_85]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_89 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_89]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_269 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_89]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_269
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_86 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_89]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_90 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_86]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_90 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_90]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_272 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_90]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_272
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_35 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_90]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_84]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_87 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_35]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_91 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_87]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_91 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_91]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_275 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_91]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_275
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_88 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_91]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_92 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_88]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_92 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_92]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_278 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_92]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_278
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_89 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_92]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_93 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_89]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 256, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_93 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_93]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_281 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_93]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_281
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_36 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_93]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_87]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_90 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_36]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_94 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_90]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512, 1024, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_94 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_94]_output &lt;tensorrt.ITensor [shape=(1, 512, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_284 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_94]_output &lt;tensorrt.ITensor [shape=(1, 512, 50, 50), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_284
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_91 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_94]_output &lt;tensorrt.ITensor [shape=(1, 512, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_95 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_91]_output &lt;tensorrt.ITensor [shape=(1, 512, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512, 512, 3, 3), dtype=float32]&gt;', None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_95 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_95]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_287 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_95]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_287
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_92 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_95]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_96 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_92]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048, 512, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_96 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_96]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_290 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_96]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_290
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_97 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_90]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048, 1024, 1, 1), dtype=float32]&gt;', None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_97 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_97]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_293 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_97]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_293
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_37 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_96]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;', '[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_97]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_93 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_37]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_98 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_93]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512, 2048, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_98 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_98]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_296 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_98]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_296
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_94 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_98]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_99 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_94]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512, 512, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_99 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_99]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_299 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_99]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_299
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_95 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_99]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_100 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_95]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048, 512, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_100 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_100]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_302 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_100]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_302
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_38 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_100]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_93]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_96 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_38]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_101 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_96]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512, 2048, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_101 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_101]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_305 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_101]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_305
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_97 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_101]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_102 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_97]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512, 512, 3, 3), dtype=float32]&gt;', None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_102 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_102]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(512,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_308 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_102]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_308
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_98 (kind: aten.relu.default, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_102]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_103 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_98]_output &lt;tensorrt.ITensor [shape=(1, 512, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048, 512, 1, 1), dtype=float32]&gt;', None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _native_batch_norm_legit_no_training_103 (kind: aten._native_batch_norm_legit_no_training.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_103]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(2048,), dtype=float32]&gt;', 0.1, 1e-05))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node getitem_311 (kind: &lt;built-in function getitem&gt;, args: (('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_103]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;', None, None), 0))
DEBUG:torch_tensorrt.dynamo.conversion.ops_evaluators:Evaluating _operator.getitem on object with name: getitem_311
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_39 (kind: aten.add.Tensor, args: ('[SCALE]-[aten_ops._native_batch_norm_legit_no_training.default]-[_native_batch_norm_legit_no_training_103]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_96]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_99 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_39]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_104 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_99]_output &lt;tensorrt.ITensor [shape=(1, 2048, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 2048, 1, 1), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_105 (kind: aten.convolution.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_104]_output &lt;tensorrt.ITensor [shape=(1, 256, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_106 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_90]_output &lt;tensorrt.ITensor [shape=(1, 1024, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 1024, 1, 1), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node index_4 (kind: aten.index.Tensor, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_104]_output &lt;tensorrt.ITensor [shape=(1, 256, 25, 25), dtype=DataType.FLOAT]&gt;', [None, None, '&lt;torch.Tensor as np.ndarray [shape=(50, 1), dtype=int64]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(50,), dtype=int64]&gt;']))
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:Determining whether aten.index constant-index optimization can be invoked
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:Shape of 2 index is (50, 1)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:Shape of 3 index is (50,)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The input shape is (1, 256, 25, 25)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The new transpose order is [2, 3, 0, 1]
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The shape of transpose tensor is (25, 25, 1, 256)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The flatten tensor shape is (625, 256)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The shape after cumultative gather is (50, 50, 256)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The shape for cumulative adv index is &lt;tensorrt_bindings.tensorrt.ITensor object at 0x7bb08db61870&gt;
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The shape for cumulative adv index is (50, 50)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The indices are continuous in this case
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The tensor is unfolded now
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The unfolded tensor shape is (2500, 1, 256)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:Transposing the indices to correct position [1, 2, 0]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_42 (kind: aten.add.Tensor, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_106]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.index.Tensor]-[index_4_unfold_advanced_index]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_107 (kind: aten.convolution.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_42]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_108 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_21]_output &lt;tensorrt.ITensor [shape=(1, 512, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 512, 1, 1), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node index_5 (kind: aten.index.Tensor, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_42]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', [None, None, '&lt;torch.Tensor as np.ndarray [shape=(100, 1), dtype=int64]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(100,), dtype=int64]&gt;']))
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:Determining whether aten.index constant-index optimization can be invoked
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:Shape of 2 index is (100, 1)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:Shape of 3 index is (100,)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The input shape is (1, 256, 50, 50)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The new transpose order is [2, 3, 0, 1]
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The shape of transpose tensor is (50, 50, 1, 256)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The flatten tensor shape is (2500, 256)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The shape after cumultative gather is (100, 100, 256)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The shape for cumulative adv index is &lt;tensorrt_bindings.tensorrt.ITensor object at 0x7bb08df3ca70&gt;
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The shape for cumulative adv index is (100, 100)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The indices are continuous in this case
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The tensor is unfolded now
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The unfolded tensor shape is (10000, 1, 256)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:Transposing the indices to correct position [1, 2, 0]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_45 (kind: aten.add.Tensor, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_108]_output &lt;tensorrt.ITensor [shape=(1, 256, 100, 100), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.index.Tensor]-[index_5_unfold_advanced_index]_output &lt;tensorrt.ITensor [shape=(1, 256, 100, 100), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_109 (kind: aten.convolution.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_45]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 256, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_110 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_9]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 1, 1), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node index_6 (kind: aten.index.Tensor, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_45]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 256, 100, 100), dtype=DataType.FLOAT]&gt;', [None, None, '&lt;torch.Tensor as np.ndarray [shape=(200, 1), dtype=int64]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(200,), dtype=int64]&gt;']))
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:Determining whether aten.index constant-index optimization can be invoked
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:Shape of 2 index is (200, 1)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:Shape of 3 index is (200,)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The input shape is (1, 256, 100, 100)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The new transpose order is [2, 3, 0, 1]
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The shape of transpose tensor is (100, 100, 1, 256)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The flatten tensor shape is (10000, 256)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The shape after cumultative gather is (200, 200, 256)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The shape for cumulative adv index is &lt;tensorrt_bindings.tensorrt.ITensor object at 0x7bb08df3f230&gt;
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The shape for cumulative adv index is (200, 200)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The indices are continuous in this case
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The tensor is unfolded now
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:The unfolded tensor shape is (40000, 1, 256)
DEBUG:torch_tensorrt.dynamo.conversion.impl.select:Transposing the indices to correct position [1, 2, 0]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_48 (kind: aten.add.Tensor, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_110]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.index.Tensor]-[index_6_unfold_advanced_index]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_111 (kind: aten.convolution.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_48]_output_add.Tensor &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node max_pool2d_default_1 (kind: aten.max_pool2d.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_105]_output &lt;tensorrt.ITensor [shape=(1, 256, 25, 25), dtype=DataType.FLOAT]&gt;', [1, 1], [2, 2]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Marking output output0 [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Marking output output1 [shape=(1, 256, 100, 100), dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Marking output output2 [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Marking output output3 [shape=(1, 256, 25, 25), dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Marking output output4 [shape=(1, 256, 13, 13), dtype=DataType.FLOAT]
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT INetwork construction elapsed time: 0:00:07.288187
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Build TRT engine elapsed time: 0:00:21.682833
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT Engine uses: 189290524 bytes of Memory
DEBUG:torch_tensorrt.dynamo._DryRunTracker:
++++++++++++++++++++++++++++++++++++++++++++++++++ Dry-Run Results for Graph ++++++++++++++++++++++++++++++++++++++++++++++++++

The graph consists of 480 Total Operators, of which 479 operators are supported, 99.79% coverage

The following ops are currently unsupported or excluded from conversion, and are listed with their op-count in the graph:
 torch.ops.aten.select_scatter.default: 1

The following nodes are currently set to run in Torch:
Node: torch.ops.aten.select_scatter.default, with layer location: select_scatter
Note: Some of the above nodes may be supported, but were not included in a TRT graph by the partitioner

Compiled with: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

  Graph Structure:

   Inputs: List[Tensor: (3, 200, 200)@float32]
    ...
    TRT Engine #1 - Submodule name: _run_on_acc_0
     Engine Inputs: List[Tensor: (3, 200, 200)@float32]
     Number of Operators in Engine: 18
     Engine Outputs: Tensor: (3, 800, 800)@float32
    ...
    TRT Engine #2 - Submodule name: _run_on_acc_2
     Engine Inputs: List[Tensor: (1, 3, 800, 800)@float32]
     Number of Operators in Engine: 461
     Engine Outputs: Tuple(Tensor: (1, 256, 200, 200)@float32, Tensor: (1, 256, 100, 100)@float32, Tensor: (1, 256, 50, 50)@float32, Tensor: (1, 256, 25, 25)@float32, Tensor: (1, 256, 13, 13)@float32)
    ...
   Outputs: Tuple(Tensor: (1, 256, 200, 200)@float32, Tensor: (1, 256, 100, 100)@float32, Tensor: (1, 256, 50, 50)@float32, Tensor: (1, 256, 25, 25)@float32, Tensor: (1, 256, 13, 13)@float32, Tensor: (1, 3, 800, 800)@float32)

  ------------------------- Aggregate Stats -------------------------

   Average Number of Operators per TRT Engine: 239.5
   Most Operators in a TRT Engine: 461

  ********** Recommendations **********

   - For minimal graph segmentation, select min_block_size=461 which would generate 1 TRT engine(s)
   - For moderate graph segmentation, select min_block_size=240 which would generate 1 TRT engine(s)
   - The current level of graph segmentation is equivalent to selecting min_block_size=18 which generates 2 TRT engine(s)
INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)
INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.
INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

DEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:
graph():
    %feature_map : torch.Tensor [num_users=1] = placeholder[target=L_features_0_]
    %feature_map_1 : torch.Tensor [num_users=1] = placeholder[target=L_features_1_]
    %feature_map_2 : torch.Tensor [num_users=1] = placeholder[target=L_features_2_]
    %feature_map_3 : torch.Tensor [num_users=1] = placeholder[target=L_features_3_]
    %feature_map_4 : torch.Tensor [num_users=1] = placeholder[target=L_features_pool_]
    %l__self___head_conv_0_0 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%feature_map,), kwargs = {})
    %t : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_0,), kwargs = {})
    %box_cls_per_level : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t,), kwargs = {})
    %box_regression_per_level : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t,), kwargs = {})
    %l__self___head_conv_0_2 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%feature_map_1,), kwargs = {})
    %t_1 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_2,), kwargs = {})
    %box_cls_per_level_2 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_1,), kwargs = {})
    %box_regression_per_level_2 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_1,), kwargs = {})
    %l__self___head_conv_0_4 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%feature_map_2,), kwargs = {})
    %t_2 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_4,), kwargs = {})
    %box_cls_per_level_4 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_2,), kwargs = {})
    %box_regression_per_level_4 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_2,), kwargs = {})
    %l__self___head_conv_0_6 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%feature_map_3,), kwargs = {})
    %t_3 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_6,), kwargs = {})
    %box_cls_per_level_6 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_3,), kwargs = {})
    %box_regression_per_level_6 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_3,), kwargs = {})
    %l__self___head_conv_0_8 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%feature_map_4,), kwargs = {})
    %t_4 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_8,), kwargs = {})
    %box_cls_per_level_8 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_4,), kwargs = {})
    %box_regression_per_level_8 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_4,), kwargs = {})
    %empty : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height : [num_users=1] = call_method[target=fill_](args = (%empty, 4), kwargs = {})
    %empty_1 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width : [num_users=1] = call_method[target=fill_](args = (%empty_1, 4), kwargs = {})
    %empty_2 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_1 : [num_users=1] = call_method[target=fill_](args = (%empty_2, 8), kwargs = {})
    %empty_3 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_1 : [num_users=1] = call_method[target=fill_](args = (%empty_3, 8), kwargs = {})
    %empty_4 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_2 : [num_users=1] = call_method[target=fill_](args = (%empty_4, 16), kwargs = {})
    %empty_5 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_2 : [num_users=1] = call_method[target=fill_](args = (%empty_5, 16), kwargs = {})
    %empty_6 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_3 : [num_users=1] = call_method[target=fill_](args = (%empty_6, 32), kwargs = {})
    %empty_7 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_3 : [num_users=1] = call_method[target=fill_](args = (%empty_7, 32), kwargs = {})
    %empty_8 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_4 : [num_users=1] = call_method[target=fill_](args = (%empty_8, 61), kwargs = {})
    %empty_9 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_4 : [num_users=1] = call_method[target=fill_](args = (%empty_9, 61), kwargs = {})
    %cell_anchor : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_0]
    %base_anchors : [num_users=2] = call_method[target=to](args = (%cell_anchor,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_1 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_1]
    %base_anchors_1 : [num_users=2] = call_method[target=to](args = (%cell_anchor_1,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_2 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_2]
    %base_anchors_2 : [num_users=2] = call_method[target=to](args = (%cell_anchor_2,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_3 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_3]
    %base_anchors_3 : [num_users=2] = call_method[target=to](args = (%cell_anchor_3,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_4 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_4]
    %base_anchors_4 : [num_users=2] = call_method[target=to](args = (%cell_anchor_4,), kwargs = {dtype: torch.float32, device: cuda:0})
    %arange : [num_users=1] = call_function[target=torch.arange](args = (0, 200), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x : [num_users=1] = call_function[target=operator.mul](args = (%arange, %stride_width), kwargs = {})
    %arange_1 : [num_users=1] = call_function[target=torch.arange](args = (0, 200), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y : [num_users=1] = call_function[target=operator.mul](args = (%arange_1, %stride_height), kwargs = {})
    %meshgrid : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y, %shifts_x), kwargs = {indexing: ij})
    %shift_y : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid, 0), kwargs = {})
    %shift_x : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid, 1), kwargs = {})
    %shift_x_1 : [num_users=1] = call_method[target=reshape](args = (%shift_x, -1), kwargs = {})
    %shift_y_1 : [num_users=1] = call_method[target=reshape](args = (%shift_y, -1), kwargs = {})
    %shifts : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_1, %shift_y_1, %shift_x_1, %shift_y_1),), kwargs = {dim: 1})
    %view : [num_users=1] = call_method[target=view](args = (%shifts, -1, 1, 4), kwargs = {})
    %view_1 : [num_users=1] = call_method[target=view](args = (%base_anchors, 1, -1, 4), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%view, %view_1), kwargs = {})
    %anchors_per_feature_map : [num_users=1] = call_method[target=reshape](args = (%add, -1, 4), kwargs = {})
    %arange_2 : [num_users=1] = call_function[target=torch.arange](args = (0, 100), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_1 : [num_users=1] = call_function[target=operator.mul](args = (%arange_2, %stride_width_1), kwargs = {})
    %arange_3 : [num_users=1] = call_function[target=torch.arange](args = (0, 100), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_1 : [num_users=1] = call_function[target=operator.mul](args = (%arange_3, %stride_height_1), kwargs = {})
    %meshgrid_1 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_1, %shifts_x_1), kwargs = {indexing: ij})
    %shift_y_2 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_1, 0), kwargs = {})
    %shift_x_2 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_1, 1), kwargs = {})
    %shift_x_3 : [num_users=1] = call_method[target=reshape](args = (%shift_x_2, -1), kwargs = {})
    %shift_y_3 : [num_users=1] = call_method[target=reshape](args = (%shift_y_2, -1), kwargs = {})
    %shifts_1 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_3, %shift_y_3, %shift_x_3, %shift_y_3),), kwargs = {dim: 1})
    %view_2 : [num_users=1] = call_method[target=view](args = (%shifts_1, -1, 1, 4), kwargs = {})
    %view_3 : [num_users=1] = call_method[target=view](args = (%base_anchors_1, 1, -1, 4), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%view_2, %view_3), kwargs = {})
    %anchors_per_feature_map_1 : [num_users=1] = call_method[target=reshape](args = (%add_1, -1, 4), kwargs = {})
    %arange_4 : [num_users=1] = call_function[target=torch.arange](args = (0, 50), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_2 : [num_users=1] = call_function[target=operator.mul](args = (%arange_4, %stride_width_2), kwargs = {})
    %arange_5 : [num_users=1] = call_function[target=torch.arange](args = (0, 50), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_2 : [num_users=1] = call_function[target=operator.mul](args = (%arange_5, %stride_height_2), kwargs = {})
    %meshgrid_2 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_2, %shifts_x_2), kwargs = {indexing: ij})
    %shift_y_4 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_2, 0), kwargs = {})
    %shift_x_4 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_2, 1), kwargs = {})
    %shift_x_5 : [num_users=1] = call_method[target=reshape](args = (%shift_x_4, -1), kwargs = {})
    %shift_y_5 : [num_users=1] = call_method[target=reshape](args = (%shift_y_4, -1), kwargs = {})
    %shifts_2 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_5, %shift_y_5, %shift_x_5, %shift_y_5),), kwargs = {dim: 1})
    %view_4 : [num_users=1] = call_method[target=view](args = (%shifts_2, -1, 1, 4), kwargs = {})
    %view_5 : [num_users=1] = call_method[target=view](args = (%base_anchors_2, 1, -1, 4), kwargs = {})
    %add_2 : [num_users=1] = call_function[target=operator.add](args = (%view_4, %view_5), kwargs = {})
    %anchors_per_feature_map_2 : [num_users=1] = call_method[target=reshape](args = (%add_2, -1, 4), kwargs = {})
    %arange_6 : [num_users=1] = call_function[target=torch.arange](args = (0, 25), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_3 : [num_users=1] = call_function[target=operator.mul](args = (%arange_6, %stride_width_3), kwargs = {})
    %arange_7 : [num_users=1] = call_function[target=torch.arange](args = (0, 25), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_3 : [num_users=1] = call_function[target=operator.mul](args = (%arange_7, %stride_height_3), kwargs = {})
    %meshgrid_3 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_3, %shifts_x_3), kwargs = {indexing: ij})
    %shift_y_6 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_3, 0), kwargs = {})
    %shift_x_6 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_3, 1), kwargs = {})
    %shift_x_7 : [num_users=1] = call_method[target=reshape](args = (%shift_x_6, -1), kwargs = {})
    %shift_y_7 : [num_users=1] = call_method[target=reshape](args = (%shift_y_6, -1), kwargs = {})
    %shifts_3 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_7, %shift_y_7, %shift_x_7, %shift_y_7),), kwargs = {dim: 1})
    %view_6 : [num_users=1] = call_method[target=view](args = (%shifts_3, -1, 1, 4), kwargs = {})
    %view_7 : [num_users=1] = call_method[target=view](args = (%base_anchors_3, 1, -1, 4), kwargs = {})
    %add_3 : [num_users=1] = call_function[target=operator.add](args = (%view_6, %view_7), kwargs = {})
    %anchors_per_feature_map_3 : [num_users=1] = call_method[target=reshape](args = (%add_3, -1, 4), kwargs = {})
    %arange_8 : [num_users=1] = call_function[target=torch.arange](args = (0, 13), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_4 : [num_users=1] = call_function[target=operator.mul](args = (%arange_8, %stride_width_4), kwargs = {})
    %arange_9 : [num_users=1] = call_function[target=torch.arange](args = (0, 13), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_4 : [num_users=1] = call_function[target=operator.mul](args = (%arange_9, %stride_height_4), kwargs = {})
    %meshgrid_4 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_4, %shifts_x_4), kwargs = {indexing: ij})
    %shift_y_8 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_4, 0), kwargs = {})
    %shift_x_8 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_4, 1), kwargs = {})
    %shift_x_9 : [num_users=1] = call_method[target=reshape](args = (%shift_x_8, -1), kwargs = {})
    %shift_y_9 : [num_users=1] = call_method[target=reshape](args = (%shift_y_8, -1), kwargs = {})
    %shifts_4 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_9, %shift_y_9, %shift_x_9, %shift_y_9),), kwargs = {dim: 1})
    %view_8 : [num_users=1] = call_method[target=view](args = (%shifts_4, -1, 1, 4), kwargs = {})
    %view_9 : [num_users=1] = call_method[target=view](args = (%base_anchors_4, 1, -1, 4), kwargs = {})
    %add_4 : [num_users=1] = call_function[target=operator.add](args = (%view_8, %view_9), kwargs = {})
    %anchors_per_feature_map_4 : [num_users=1] = call_method[target=reshape](args = (%add_4, -1, 4), kwargs = {})
    %b : [num_users=2] = call_function[target=torch.cat](args = ([%anchors_per_feature_map, %anchors_per_feature_map_1, %anchors_per_feature_map_2, %anchors_per_feature_map_3, %anchors_per_feature_map_4],), kwargs = {})
    %layer : [num_users=1] = call_method[target=view](args = (%box_cls_per_level, 1, -1, 1, 200, 200), kwargs = {})
    %layer_1 : [num_users=1] = call_method[target=permute](args = (%layer, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_1 : [num_users=1] = call_method[target=reshape](args = (%layer_1, 1, -1, 1), kwargs = {})
    %layer_3 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level, 1, -1, 4, 200, 200), kwargs = {})
    %layer_4 : [num_users=1] = call_method[target=permute](args = (%layer_3, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_1 : [num_users=1] = call_method[target=reshape](args = (%layer_4, 1, -1, 4), kwargs = {})
    %layer_6 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_2, 1, -1, 1, 100, 100), kwargs = {})
    %layer_7 : [num_users=1] = call_method[target=permute](args = (%layer_6, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_3 : [num_users=1] = call_method[target=reshape](args = (%layer_7, 1, -1, 1), kwargs = {})
    %layer_9 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_2, 1, -1, 4, 100, 100), kwargs = {})
    %layer_10 : [num_users=1] = call_method[target=permute](args = (%layer_9, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_3 : [num_users=1] = call_method[target=reshape](args = (%layer_10, 1, -1, 4), kwargs = {})
    %layer_12 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_4, 1, -1, 1, 50, 50), kwargs = {})
    %layer_13 : [num_users=1] = call_method[target=permute](args = (%layer_12, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_5 : [num_users=1] = call_method[target=reshape](args = (%layer_13, 1, -1, 1), kwargs = {})
    %layer_15 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_4, 1, -1, 4, 50, 50), kwargs = {})
    %layer_16 : [num_users=1] = call_method[target=permute](args = (%layer_15, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_5 : [num_users=1] = call_method[target=reshape](args = (%layer_16, 1, -1, 4), kwargs = {})
    %layer_18 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_6, 1, -1, 1, 25, 25), kwargs = {})
    %layer_19 : [num_users=1] = call_method[target=permute](args = (%layer_18, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_7 : [num_users=1] = call_method[target=reshape](args = (%layer_19, 1, -1, 1), kwargs = {})
    %layer_21 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_6, 1, -1, 4, 25, 25), kwargs = {})
    %layer_22 : [num_users=1] = call_method[target=permute](args = (%layer_21, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_7 : [num_users=1] = call_method[target=reshape](args = (%layer_22, 1, -1, 4), kwargs = {})
    %layer_24 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_8, 1, -1, 1, 13, 13), kwargs = {})
    %layer_25 : [num_users=1] = call_method[target=permute](args = (%layer_24, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_9 : [num_users=1] = call_method[target=reshape](args = (%layer_25, 1, -1, 1), kwargs = {})
    %layer_27 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_8, 1, -1, 4, 13, 13), kwargs = {})
    %layer_28 : [num_users=1] = call_method[target=permute](args = (%layer_27, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_9 : [num_users=1] = call_method[target=reshape](args = (%layer_28, 1, -1, 4), kwargs = {})
    %cat_1 : [num_users=1] = call_function[target=torch.cat](args = ([%box_cls_per_level_1, %box_cls_per_level_3, %box_cls_per_level_5, %box_cls_per_level_7, %box_cls_per_level_9],), kwargs = {dim: 1})
    %objectness : [num_users=1] = call_method[target=flatten](args = (%cat_1, 0, -2), kwargs = {})
    %cat_2 : [num_users=1] = call_function[target=torch.cat](args = ([%box_regression_per_level_1, %box_regression_per_level_3, %box_regression_per_level_5, %box_regression_per_level_7, %box_regression_per_level_9],), kwargs = {dim: 1})
    %pred_bbox_deltas : [num_users=2] = call_method[target=reshape](args = (%cat_2, -1, 4), kwargs = {})
    %detach : [num_users=1] = call_method[target=detach](args = (%pred_bbox_deltas,), kwargs = {})
    %concat_boxes : [num_users=1] = call_function[target=torch.cat](args = ([%b],), kwargs = {dim: 0})
    %rel_codes : [num_users=4] = call_method[target=reshape](args = (%detach, 159882, -1), kwargs = {})
    %boxes : [num_users=6] = call_method[target=to](args = (%concat_boxes, torch.float32), kwargs = {})
    %getitem_15 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 2)), kwargs = {})
    %getitem_16 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 0)), kwargs = {})
    %widths : [num_users=3] = call_function[target=operator.sub](args = (%getitem_15, %getitem_16), kwargs = {})
    %getitem_17 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 3)), kwargs = {})
    %getitem_18 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 1)), kwargs = {})
    %heights : [num_users=3] = call_function[target=operator.sub](args = (%getitem_17, %getitem_18), kwargs = {})
    %getitem_19 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 0)), kwargs = {})
    %mul_10 : [num_users=1] = call_function[target=operator.mul](args = (0.5, %widths), kwargs = {})
    %ctr_x : [num_users=1] = call_function[target=operator.add](args = (%getitem_19, %mul_10), kwargs = {})
    %getitem_20 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 1)), kwargs = {})
    %mul_11 : [num_users=1] = call_function[target=operator.mul](args = (0.5, %heights), kwargs = {})
    %ctr_y : [num_users=1] = call_function[target=operator.add](args = (%getitem_20, %mul_11), kwargs = {})
    %getitem_21 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(0, None, 4))), kwargs = {})
    %dx : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_21, 1.0), kwargs = {})
    %getitem_22 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(1, None, 4))), kwargs = {})
    %dy : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_22, 1.0), kwargs = {})
    %getitem_23 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(2, None, 4))), kwargs = {})
    %dw : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_23, 1.0), kwargs = {})
    %getitem_24 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(3, None, 4))), kwargs = {})
    %dh : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_24, 1.0), kwargs = {})
    %dw_1 : [num_users=1] = call_function[target=torch.clamp](args = (%dw,), kwargs = {max: 4.135166556742356})
    %dh_1 : [num_users=1] = call_function[target=torch.clamp](args = (%dh,), kwargs = {max: 4.135166556742356})
    %getitem_25 : [num_users=1] = call_function[target=operator.getitem](args = (%widths, (slice(None, None, None), None)), kwargs = {})
    %mul_12 : [num_users=1] = call_function[target=operator.mul](args = (%dx, %getitem_25), kwargs = {})
    %getitem_26 : [num_users=1] = call_function[target=operator.getitem](args = (%ctr_x, (slice(None, None, None), None)), kwargs = {})
    %pred_ctr_x : [num_users=2] = call_function[target=operator.add](args = (%mul_12, %getitem_26), kwargs = {})
    %getitem_27 : [num_users=1] = call_function[target=operator.getitem](args = (%heights, (slice(None, None, None), None)), kwargs = {})
    %mul_13 : [num_users=1] = call_function[target=operator.mul](args = (%dy, %getitem_27), kwargs = {})
    %getitem_28 : [num_users=1] = call_function[target=operator.getitem](args = (%ctr_y, (slice(None, None, None), None)), kwargs = {})
    %pred_ctr_y : [num_users=2] = call_function[target=operator.add](args = (%mul_13, %getitem_28), kwargs = {})
    %exp : [num_users=1] = call_function[target=torch.exp](args = (%dw_1,), kwargs = {})
    %getitem_29 : [num_users=1] = call_function[target=operator.getitem](args = (%widths, (slice(None, None, None), None)), kwargs = {})
    %pred_w : [num_users=1] = call_function[target=operator.mul](args = (%exp, %getitem_29), kwargs = {})
    %exp_1 : [num_users=1] = call_function[target=torch.exp](args = (%dh_1,), kwargs = {})
    %getitem_30 : [num_users=1] = call_function[target=operator.getitem](args = (%heights, (slice(None, None, None), None)), kwargs = {})
    %pred_h : [num_users=1] = call_function[target=operator.mul](args = (%exp_1, %getitem_30), kwargs = {})
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (0.5,), kwargs = {dtype: torch.float32, device: cuda:0})
    %c_to_c_h : [num_users=2] = call_function[target=operator.mul](args = (%tensor, %pred_h), kwargs = {})
    %tensor_1 : [num_users=1] = call_function[target=torch.tensor](args = (0.5,), kwargs = {dtype: torch.float32, device: cuda:0})
    %c_to_c_w : [num_users=2] = call_function[target=operator.mul](args = (%tensor_1, %pred_w), kwargs = {})
    %pred_boxes1 : [num_users=1] = call_function[target=operator.sub](args = (%pred_ctr_x, %c_to_c_w), kwargs = {})
    %pred_boxes2 : [num_users=1] = call_function[target=operator.sub](args = (%pred_ctr_y, %c_to_c_h), kwargs = {})
    %pred_boxes3 : [num_users=1] = call_function[target=operator.add](args = (%pred_ctr_x, %c_to_c_w), kwargs = {})
    %pred_boxes4 : [num_users=1] = call_function[target=operator.add](args = (%pred_ctr_y, %c_to_c_h), kwargs = {})
    %stack_5 : [num_users=1] = call_function[target=torch.stack](args = ((%pred_boxes1, %pred_boxes2, %pred_boxes3, %pred_boxes4),), kwargs = {dim: 2})
    %pred_boxes : [num_users=1] = call_method[target=flatten](args = (%stack_5, 1), kwargs = {})
    %proposals : [num_users=1] = call_method[target=reshape](args = (%pred_boxes, 159882, -1, 4), kwargs = {})
    %proposals_1 : [num_users=1] = call_method[target=view](args = (%proposals, 1, -1, 4), kwargs = {})
    return (proposals_1, objectness, pred_bbox_deltas, b, base_anchors, base_anchors_1, base_anchors_2, base_anchors_3, base_anchors_4)
DEBUG:torch_tensorrt.dynamo.lowering._repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:
graph():
    %feature_map : torch.Tensor [num_users=1] = placeholder[target=L_features_0_]
    %feature_map_1 : torch.Tensor [num_users=1] = placeholder[target=L_features_1_]
    %feature_map_2 : torch.Tensor [num_users=1] = placeholder[target=L_features_2_]
    %feature_map_3 : torch.Tensor [num_users=1] = placeholder[target=L_features_3_]
    %feature_map_4 : torch.Tensor [num_users=1] = placeholder[target=L_features_pool_]
    %clone_default_4 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%feature_map_4,), kwargs = {})
    %clone_default_3 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%feature_map_3,), kwargs = {})
    %clone_default_2 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%feature_map_2,), kwargs = {})
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%feature_map_1,), kwargs = {})
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%feature_map,), kwargs = {})
    %l__self___head_conv_0_0 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%clone_default,), kwargs = {})
    %t : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_0,), kwargs = {})
    %box_cls_per_level : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t,), kwargs = {})
    %box_regression_per_level : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t,), kwargs = {})
    %l__self___head_conv_0_2 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%clone_default_1,), kwargs = {})
    %t_1 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_2,), kwargs = {})
    %box_cls_per_level_2 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_1,), kwargs = {})
    %box_regression_per_level_2 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_1,), kwargs = {})
    %l__self___head_conv_0_4 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%clone_default_2,), kwargs = {})
    %t_2 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_4,), kwargs = {})
    %box_cls_per_level_4 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_2,), kwargs = {})
    %box_regression_per_level_4 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_2,), kwargs = {})
    %l__self___head_conv_0_6 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%clone_default_3,), kwargs = {})
    %t_3 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_6,), kwargs = {})
    %box_cls_per_level_6 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_3,), kwargs = {})
    %box_regression_per_level_6 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_3,), kwargs = {})
    %l__self___head_conv_0_8 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%clone_default_4,), kwargs = {})
    %t_4 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_8,), kwargs = {})
    %box_cls_per_level_8 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_4,), kwargs = {})
    %box_regression_per_level_8 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_4,), kwargs = {})
    %empty : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height : [num_users=1] = call_method[target=fill_](args = (%empty, 4), kwargs = {})
    %empty_1 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width : [num_users=1] = call_method[target=fill_](args = (%empty_1, 4), kwargs = {})
    %empty_2 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_1 : [num_users=1] = call_method[target=fill_](args = (%empty_2, 8), kwargs = {})
    %empty_3 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_1 : [num_users=1] = call_method[target=fill_](args = (%empty_3, 8), kwargs = {})
    %empty_4 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_2 : [num_users=1] = call_method[target=fill_](args = (%empty_4, 16), kwargs = {})
    %empty_5 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_2 : [num_users=1] = call_method[target=fill_](args = (%empty_5, 16), kwargs = {})
    %empty_6 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_3 : [num_users=1] = call_method[target=fill_](args = (%empty_6, 32), kwargs = {})
    %empty_7 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_3 : [num_users=1] = call_method[target=fill_](args = (%empty_7, 32), kwargs = {})
    %empty_8 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_4 : [num_users=1] = call_method[target=fill_](args = (%empty_8, 61), kwargs = {})
    %empty_9 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_4 : [num_users=1] = call_method[target=fill_](args = (%empty_9, 61), kwargs = {})
    %cell_anchor : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_0]
    %base_anchors : [num_users=2] = call_method[target=to](args = (%cell_anchor,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_1 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_1]
    %base_anchors_1 : [num_users=2] = call_method[target=to](args = (%cell_anchor_1,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_2 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_2]
    %base_anchors_2 : [num_users=2] = call_method[target=to](args = (%cell_anchor_2,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_3 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_3]
    %base_anchors_3 : [num_users=2] = call_method[target=to](args = (%cell_anchor_3,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_4 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_4]
    %base_anchors_4 : [num_users=2] = call_method[target=to](args = (%cell_anchor_4,), kwargs = {dtype: torch.float32, device: cuda:0})
    %arange : [num_users=1] = call_function[target=torch.arange](args = (0, 200), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x : [num_users=1] = call_function[target=operator.mul](args = (%arange, %stride_width), kwargs = {})
    %arange_1 : [num_users=1] = call_function[target=torch.arange](args = (0, 200), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y : [num_users=1] = call_function[target=operator.mul](args = (%arange_1, %stride_height), kwargs = {})
    %meshgrid : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y, %shifts_x), kwargs = {indexing: ij})
    %shift_y : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid, 0), kwargs = {})
    %shift_x : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid, 1), kwargs = {})
    %shift_x_1 : [num_users=1] = call_method[target=reshape](args = (%shift_x, -1), kwargs = {})
    %shift_y_1 : [num_users=1] = call_method[target=reshape](args = (%shift_y, -1), kwargs = {})
    %shifts : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_1, %shift_y_1, %shift_x_1, %shift_y_1),), kwargs = {dim: 1})
    %view : [num_users=1] = call_method[target=view](args = (%shifts, -1, 1, 4), kwargs = {})
    %view_1 : [num_users=1] = call_method[target=view](args = (%base_anchors, 1, -1, 4), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%view, %view_1), kwargs = {})
    %anchors_per_feature_map : [num_users=1] = call_method[target=reshape](args = (%add, -1, 4), kwargs = {})
    %arange_2 : [num_users=1] = call_function[target=torch.arange](args = (0, 100), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_1 : [num_users=1] = call_function[target=operator.mul](args = (%arange_2, %stride_width_1), kwargs = {})
    %arange_3 : [num_users=1] = call_function[target=torch.arange](args = (0, 100), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_1 : [num_users=1] = call_function[target=operator.mul](args = (%arange_3, %stride_height_1), kwargs = {})
    %meshgrid_1 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_1, %shifts_x_1), kwargs = {indexing: ij})
    %shift_y_2 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_1, 0), kwargs = {})
    %shift_x_2 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_1, 1), kwargs = {})
    %shift_x_3 : [num_users=1] = call_method[target=reshape](args = (%shift_x_2, -1), kwargs = {})
    %shift_y_3 : [num_users=1] = call_method[target=reshape](args = (%shift_y_2, -1), kwargs = {})
    %shifts_1 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_3, %shift_y_3, %shift_x_3, %shift_y_3),), kwargs = {dim: 1})
    %view_2 : [num_users=1] = call_method[target=view](args = (%shifts_1, -1, 1, 4), kwargs = {})
    %view_3 : [num_users=1] = call_method[target=view](args = (%base_anchors_1, 1, -1, 4), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%view_2, %view_3), kwargs = {})
    %anchors_per_feature_map_1 : [num_users=1] = call_method[target=reshape](args = (%add_1, -1, 4), kwargs = {})
    %arange_4 : [num_users=1] = call_function[target=torch.arange](args = (0, 50), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_2 : [num_users=1] = call_function[target=operator.mul](args = (%arange_4, %stride_width_2), kwargs = {})
    %arange_5 : [num_users=1] = call_function[target=torch.arange](args = (0, 50), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_2 : [num_users=1] = call_function[target=operator.mul](args = (%arange_5, %stride_height_2), kwargs = {})
    %meshgrid_2 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_2, %shifts_x_2), kwargs = {indexing: ij})
    %shift_y_4 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_2, 0), kwargs = {})
    %shift_x_4 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_2, 1), kwargs = {})
    %shift_x_5 : [num_users=1] = call_method[target=reshape](args = (%shift_x_4, -1), kwargs = {})
    %shift_y_5 : [num_users=1] = call_method[target=reshape](args = (%shift_y_4, -1), kwargs = {})
    %shifts_2 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_5, %shift_y_5, %shift_x_5, %shift_y_5),), kwargs = {dim: 1})
    %view_4 : [num_users=1] = call_method[target=view](args = (%shifts_2, -1, 1, 4), kwargs = {})
    %view_5 : [num_users=1] = call_method[target=view](args = (%base_anchors_2, 1, -1, 4), kwargs = {})
    %add_2 : [num_users=1] = call_function[target=operator.add](args = (%view_4, %view_5), kwargs = {})
    %anchors_per_feature_map_2 : [num_users=1] = call_method[target=reshape](args = (%add_2, -1, 4), kwargs = {})
    %arange_6 : [num_users=1] = call_function[target=torch.arange](args = (0, 25), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_3 : [num_users=1] = call_function[target=operator.mul](args = (%arange_6, %stride_width_3), kwargs = {})
    %arange_7 : [num_users=1] = call_function[target=torch.arange](args = (0, 25), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_3 : [num_users=1] = call_function[target=operator.mul](args = (%arange_7, %stride_height_3), kwargs = {})
    %meshgrid_3 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_3, %shifts_x_3), kwargs = {indexing: ij})
    %shift_y_6 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_3, 0), kwargs = {})
    %shift_x_6 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_3, 1), kwargs = {})
    %shift_x_7 : [num_users=1] = call_method[target=reshape](args = (%shift_x_6, -1), kwargs = {})
    %shift_y_7 : [num_users=1] = call_method[target=reshape](args = (%shift_y_6, -1), kwargs = {})
    %shifts_3 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_7, %shift_y_7, %shift_x_7, %shift_y_7),), kwargs = {dim: 1})
    %view_6 : [num_users=1] = call_method[target=view](args = (%shifts_3, -1, 1, 4), kwargs = {})
    %view_7 : [num_users=1] = call_method[target=view](args = (%base_anchors_3, 1, -1, 4), kwargs = {})
    %add_3 : [num_users=1] = call_function[target=operator.add](args = (%view_6, %view_7), kwargs = {})
    %anchors_per_feature_map_3 : [num_users=1] = call_method[target=reshape](args = (%add_3, -1, 4), kwargs = {})
    %arange_8 : [num_users=1] = call_function[target=torch.arange](args = (0, 13), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_4 : [num_users=1] = call_function[target=operator.mul](args = (%arange_8, %stride_width_4), kwargs = {})
    %arange_9 : [num_users=1] = call_function[target=torch.arange](args = (0, 13), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_4 : [num_users=1] = call_function[target=operator.mul](args = (%arange_9, %stride_height_4), kwargs = {})
    %meshgrid_4 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_4, %shifts_x_4), kwargs = {indexing: ij})
    %shift_y_8 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_4, 0), kwargs = {})
    %shift_x_8 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_4, 1), kwargs = {})
    %shift_x_9 : [num_users=1] = call_method[target=reshape](args = (%shift_x_8, -1), kwargs = {})
    %shift_y_9 : [num_users=1] = call_method[target=reshape](args = (%shift_y_8, -1), kwargs = {})
    %shifts_4 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_9, %shift_y_9, %shift_x_9, %shift_y_9),), kwargs = {dim: 1})
    %view_8 : [num_users=1] = call_method[target=view](args = (%shifts_4, -1, 1, 4), kwargs = {})
    %view_9 : [num_users=1] = call_method[target=view](args = (%base_anchors_4, 1, -1, 4), kwargs = {})
    %add_4 : [num_users=1] = call_function[target=operator.add](args = (%view_8, %view_9), kwargs = {})
    %anchors_per_feature_map_4 : [num_users=1] = call_method[target=reshape](args = (%add_4, -1, 4), kwargs = {})
    %b : [num_users=2] = call_function[target=torch.cat](args = ([%anchors_per_feature_map, %anchors_per_feature_map_1, %anchors_per_feature_map_2, %anchors_per_feature_map_3, %anchors_per_feature_map_4],), kwargs = {})
    %layer : [num_users=1] = call_method[target=view](args = (%box_cls_per_level, 1, -1, 1, 200, 200), kwargs = {})
    %layer_1 : [num_users=1] = call_method[target=permute](args = (%layer, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_1 : [num_users=1] = call_method[target=reshape](args = (%layer_1, 1, -1, 1), kwargs = {})
    %layer_3 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level, 1, -1, 4, 200, 200), kwargs = {})
    %layer_4 : [num_users=1] = call_method[target=permute](args = (%layer_3, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_1 : [num_users=1] = call_method[target=reshape](args = (%layer_4, 1, -1, 4), kwargs = {})
    %layer_6 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_2, 1, -1, 1, 100, 100), kwargs = {})
    %layer_7 : [num_users=1] = call_method[target=permute](args = (%layer_6, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_3 : [num_users=1] = call_method[target=reshape](args = (%layer_7, 1, -1, 1), kwargs = {})
    %layer_9 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_2, 1, -1, 4, 100, 100), kwargs = {})
    %layer_10 : [num_users=1] = call_method[target=permute](args = (%layer_9, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_3 : [num_users=1] = call_method[target=reshape](args = (%layer_10, 1, -1, 4), kwargs = {})
    %layer_12 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_4, 1, -1, 1, 50, 50), kwargs = {})
    %layer_13 : [num_users=1] = call_method[target=permute](args = (%layer_12, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_5 : [num_users=1] = call_method[target=reshape](args = (%layer_13, 1, -1, 1), kwargs = {})
    %layer_15 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_4, 1, -1, 4, 50, 50), kwargs = {})
    %layer_16 : [num_users=1] = call_method[target=permute](args = (%layer_15, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_5 : [num_users=1] = call_method[target=reshape](args = (%layer_16, 1, -1, 4), kwargs = {})
    %layer_18 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_6, 1, -1, 1, 25, 25), kwargs = {})
    %layer_19 : [num_users=1] = call_method[target=permute](args = (%layer_18, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_7 : [num_users=1] = call_method[target=reshape](args = (%layer_19, 1, -1, 1), kwargs = {})
    %layer_21 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_6, 1, -1, 4, 25, 25), kwargs = {})
    %layer_22 : [num_users=1] = call_method[target=permute](args = (%layer_21, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_7 : [num_users=1] = call_method[target=reshape](args = (%layer_22, 1, -1, 4), kwargs = {})
    %layer_24 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_8, 1, -1, 1, 13, 13), kwargs = {})
    %layer_25 : [num_users=1] = call_method[target=permute](args = (%layer_24, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_9 : [num_users=1] = call_method[target=reshape](args = (%layer_25, 1, -1, 1), kwargs = {})
    %layer_27 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_8, 1, -1, 4, 13, 13), kwargs = {})
    %layer_28 : [num_users=1] = call_method[target=permute](args = (%layer_27, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_9 : [num_users=1] = call_method[target=reshape](args = (%layer_28, 1, -1, 4), kwargs = {})
    %cat_1 : [num_users=1] = call_function[target=torch.cat](args = ([%box_cls_per_level_1, %box_cls_per_level_3, %box_cls_per_level_5, %box_cls_per_level_7, %box_cls_per_level_9],), kwargs = {dim: 1})
    %objectness : [num_users=1] = call_method[target=flatten](args = (%cat_1, 0, -2), kwargs = {})
    %cat_2 : [num_users=1] = call_function[target=torch.cat](args = ([%box_regression_per_level_1, %box_regression_per_level_3, %box_regression_per_level_5, %box_regression_per_level_7, %box_regression_per_level_9],), kwargs = {dim: 1})
    %pred_bbox_deltas : [num_users=2] = call_method[target=reshape](args = (%cat_2, -1, 4), kwargs = {})
    %detach : [num_users=1] = call_method[target=detach](args = (%pred_bbox_deltas,), kwargs = {})
    %concat_boxes : [num_users=1] = call_function[target=torch.cat](args = ([%b],), kwargs = {dim: 0})
    %rel_codes : [num_users=4] = call_method[target=reshape](args = (%detach, 159882, -1), kwargs = {})
    %boxes : [num_users=6] = call_method[target=to](args = (%concat_boxes, torch.float32), kwargs = {})
    %getitem_15 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 2)), kwargs = {})
    %getitem_16 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 0)), kwargs = {})
    %widths : [num_users=3] = call_function[target=operator.sub](args = (%getitem_15, %getitem_16), kwargs = {})
    %getitem_17 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 3)), kwargs = {})
    %getitem_18 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 1)), kwargs = {})
    %heights : [num_users=3] = call_function[target=operator.sub](args = (%getitem_17, %getitem_18), kwargs = {})
    %getitem_19 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 0)), kwargs = {})
    %mul_10 : [num_users=1] = call_function[target=operator.mul](args = (0.5, %widths), kwargs = {})
    %ctr_x : [num_users=1] = call_function[target=operator.add](args = (%getitem_19, %mul_10), kwargs = {})
    %getitem_20 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 1)), kwargs = {})
    %mul_11 : [num_users=1] = call_function[target=operator.mul](args = (0.5, %heights), kwargs = {})
    %ctr_y : [num_users=1] = call_function[target=operator.add](args = (%getitem_20, %mul_11), kwargs = {})
    %getitem_21 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(0, None, 4))), kwargs = {})
    %dx : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_21, 1.0), kwargs = {})
    %getitem_22 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(1, None, 4))), kwargs = {})
    %dy : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_22, 1.0), kwargs = {})
    %getitem_23 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(2, None, 4))), kwargs = {})
    %dw : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_23, 1.0), kwargs = {})
    %getitem_24 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(3, None, 4))), kwargs = {})
    %dh : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_24, 1.0), kwargs = {})
    %dw_1 : [num_users=1] = call_function[target=torch.clamp](args = (%dw,), kwargs = {max: 4.135166556742356})
    %dh_1 : [num_users=1] = call_function[target=torch.clamp](args = (%dh,), kwargs = {max: 4.135166556742356})
    %getitem_25 : [num_users=1] = call_function[target=operator.getitem](args = (%widths, (slice(None, None, None), None)), kwargs = {})
    %mul_12 : [num_users=1] = call_function[target=operator.mul](args = (%dx, %getitem_25), kwargs = {})
    %getitem_26 : [num_users=1] = call_function[target=operator.getitem](args = (%ctr_x, (slice(None, None, None), None)), kwargs = {})
    %pred_ctr_x : [num_users=2] = call_function[target=operator.add](args = (%mul_12, %getitem_26), kwargs = {})
    %getitem_27 : [num_users=1] = call_function[target=operator.getitem](args = (%heights, (slice(None, None, None), None)), kwargs = {})
    %mul_13 : [num_users=1] = call_function[target=operator.mul](args = (%dy, %getitem_27), kwargs = {})
    %getitem_28 : [num_users=1] = call_function[target=operator.getitem](args = (%ctr_y, (slice(None, None, None), None)), kwargs = {})
    %pred_ctr_y : [num_users=2] = call_function[target=operator.add](args = (%mul_13, %getitem_28), kwargs = {})
    %exp : [num_users=1] = call_function[target=torch.exp](args = (%dw_1,), kwargs = {})
    %getitem_29 : [num_users=1] = call_function[target=operator.getitem](args = (%widths, (slice(None, None, None), None)), kwargs = {})
    %pred_w : [num_users=1] = call_function[target=operator.mul](args = (%exp, %getitem_29), kwargs = {})
    %exp_1 : [num_users=1] = call_function[target=torch.exp](args = (%dh_1,), kwargs = {})
    %getitem_30 : [num_users=1] = call_function[target=operator.getitem](args = (%heights, (slice(None, None, None), None)), kwargs = {})
    %pred_h : [num_users=1] = call_function[target=operator.mul](args = (%exp_1, %getitem_30), kwargs = {})
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (0.5,), kwargs = {dtype: torch.float32, device: cuda:0})
    %c_to_c_h : [num_users=2] = call_function[target=operator.mul](args = (%tensor, %pred_h), kwargs = {})
    %tensor_1 : [num_users=1] = call_function[target=torch.tensor](args = (0.5,), kwargs = {dtype: torch.float32, device: cuda:0})
    %c_to_c_w : [num_users=2] = call_function[target=operator.mul](args = (%tensor_1, %pred_w), kwargs = {})
    %pred_boxes1 : [num_users=1] = call_function[target=operator.sub](args = (%pred_ctr_x, %c_to_c_w), kwargs = {})
    %pred_boxes2 : [num_users=1] = call_function[target=operator.sub](args = (%pred_ctr_y, %c_to_c_h), kwargs = {})
    %pred_boxes3 : [num_users=1] = call_function[target=operator.add](args = (%pred_ctr_x, %c_to_c_w), kwargs = {})
    %pred_boxes4 : [num_users=1] = call_function[target=operator.add](args = (%pred_ctr_y, %c_to_c_h), kwargs = {})
    %stack_5 : [num_users=1] = call_function[target=torch.stack](args = ((%pred_boxes1, %pred_boxes2, %pred_boxes3, %pred_boxes4),), kwargs = {dim: 2})
    %pred_boxes : [num_users=1] = call_method[target=flatten](args = (%stack_5, 1), kwargs = {})
    %proposals : [num_users=1] = call_method[target=reshape](args = (%pred_boxes, 159882, -1, 4), kwargs = {})
    %proposals_1 : [num_users=1] = call_method[target=view](args = (%proposals, 1, -1, 4), kwargs = {})
    return (proposals_1, objectness, pred_bbox_deltas, b, base_anchors, base_anchors_1, base_anchors_2, base_anchors_3, base_anchors_4)
DEBUG:torch_tensorrt.dynamo.lowering._remove_sym_nodes:Removed SymInt placeholders:
graph():
    %feature_map : torch.Tensor [num_users=1] = placeholder[target=L_features_0_]
    %feature_map_1 : torch.Tensor [num_users=1] = placeholder[target=L_features_1_]
    %feature_map_2 : torch.Tensor [num_users=1] = placeholder[target=L_features_2_]
    %feature_map_3 : torch.Tensor [num_users=1] = placeholder[target=L_features_3_]
    %feature_map_4 : torch.Tensor [num_users=1] = placeholder[target=L_features_pool_]
    %clone_default_4 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%feature_map_4,), kwargs = {})
    %clone_default_3 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%feature_map_3,), kwargs = {})
    %clone_default_2 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%feature_map_2,), kwargs = {})
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%feature_map_1,), kwargs = {})
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%feature_map,), kwargs = {})
    %l__self___head_conv_0_0 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%clone_default,), kwargs = {})
    %t : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_0,), kwargs = {})
    %box_cls_per_level : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t,), kwargs = {})
    %box_regression_per_level : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t,), kwargs = {})
    %l__self___head_conv_0_2 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%clone_default_1,), kwargs = {})
    %t_1 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_2,), kwargs = {})
    %box_cls_per_level_2 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_1,), kwargs = {})
    %box_regression_per_level_2 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_1,), kwargs = {})
    %l__self___head_conv_0_4 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%clone_default_2,), kwargs = {})
    %t_2 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_4,), kwargs = {})
    %box_cls_per_level_4 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_2,), kwargs = {})
    %box_regression_per_level_4 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_2,), kwargs = {})
    %l__self___head_conv_0_6 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%clone_default_3,), kwargs = {})
    %t_3 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_6,), kwargs = {})
    %box_cls_per_level_6 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_3,), kwargs = {})
    %box_regression_per_level_6 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_3,), kwargs = {})
    %l__self___head_conv_0_8 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%clone_default_4,), kwargs = {})
    %t_4 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_8,), kwargs = {})
    %box_cls_per_level_8 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_4,), kwargs = {})
    %box_regression_per_level_8 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_4,), kwargs = {})
    %empty : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height : [num_users=1] = call_method[target=fill_](args = (%empty, 4), kwargs = {})
    %empty_1 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width : [num_users=1] = call_method[target=fill_](args = (%empty_1, 4), kwargs = {})
    %empty_2 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_1 : [num_users=1] = call_method[target=fill_](args = (%empty_2, 8), kwargs = {})
    %empty_3 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_1 : [num_users=1] = call_method[target=fill_](args = (%empty_3, 8), kwargs = {})
    %empty_4 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_2 : [num_users=1] = call_method[target=fill_](args = (%empty_4, 16), kwargs = {})
    %empty_5 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_2 : [num_users=1] = call_method[target=fill_](args = (%empty_5, 16), kwargs = {})
    %empty_6 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_3 : [num_users=1] = call_method[target=fill_](args = (%empty_6, 32), kwargs = {})
    %empty_7 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_3 : [num_users=1] = call_method[target=fill_](args = (%empty_7, 32), kwargs = {})
    %empty_8 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_4 : [num_users=1] = call_method[target=fill_](args = (%empty_8, 61), kwargs = {})
    %empty_9 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_4 : [num_users=1] = call_method[target=fill_](args = (%empty_9, 61), kwargs = {})
    %cell_anchor : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_0]
    %base_anchors : [num_users=2] = call_method[target=to](args = (%cell_anchor,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_1 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_1]
    %base_anchors_1 : [num_users=2] = call_method[target=to](args = (%cell_anchor_1,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_2 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_2]
    %base_anchors_2 : [num_users=2] = call_method[target=to](args = (%cell_anchor_2,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_3 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_3]
    %base_anchors_3 : [num_users=2] = call_method[target=to](args = (%cell_anchor_3,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_4 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_4]
    %base_anchors_4 : [num_users=2] = call_method[target=to](args = (%cell_anchor_4,), kwargs = {dtype: torch.float32, device: cuda:0})
    %arange : [num_users=1] = call_function[target=torch.arange](args = (0, 200), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x : [num_users=1] = call_function[target=operator.mul](args = (%arange, %stride_width), kwargs = {})
    %arange_1 : [num_users=1] = call_function[target=torch.arange](args = (0, 200), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y : [num_users=1] = call_function[target=operator.mul](args = (%arange_1, %stride_height), kwargs = {})
    %meshgrid : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y, %shifts_x), kwargs = {indexing: ij})
    %shift_y : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid, 0), kwargs = {})
    %shift_x : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid, 1), kwargs = {})
    %shift_x_1 : [num_users=1] = call_method[target=reshape](args = (%shift_x, -1), kwargs = {})
    %shift_y_1 : [num_users=1] = call_method[target=reshape](args = (%shift_y, -1), kwargs = {})
    %shifts : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_1, %shift_y_1, %shift_x_1, %shift_y_1),), kwargs = {dim: 1})
    %view : [num_users=1] = call_method[target=view](args = (%shifts, -1, 1, 4), kwargs = {})
    %view_1 : [num_users=1] = call_method[target=view](args = (%base_anchors, 1, -1, 4), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%view, %view_1), kwargs = {})
    %anchors_per_feature_map : [num_users=1] = call_method[target=reshape](args = (%add, -1, 4), kwargs = {})
    %arange_2 : [num_users=1] = call_function[target=torch.arange](args = (0, 100), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_1 : [num_users=1] = call_function[target=operator.mul](args = (%arange_2, %stride_width_1), kwargs = {})
    %arange_3 : [num_users=1] = call_function[target=torch.arange](args = (0, 100), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_1 : [num_users=1] = call_function[target=operator.mul](args = (%arange_3, %stride_height_1), kwargs = {})
    %meshgrid_1 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_1, %shifts_x_1), kwargs = {indexing: ij})
    %shift_y_2 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_1, 0), kwargs = {})
    %shift_x_2 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_1, 1), kwargs = {})
    %shift_x_3 : [num_users=1] = call_method[target=reshape](args = (%shift_x_2, -1), kwargs = {})
    %shift_y_3 : [num_users=1] = call_method[target=reshape](args = (%shift_y_2, -1), kwargs = {})
    %shifts_1 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_3, %shift_y_3, %shift_x_3, %shift_y_3),), kwargs = {dim: 1})
    %view_2 : [num_users=1] = call_method[target=view](args = (%shifts_1, -1, 1, 4), kwargs = {})
    %view_3 : [num_users=1] = call_method[target=view](args = (%base_anchors_1, 1, -1, 4), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%view_2, %view_3), kwargs = {})
    %anchors_per_feature_map_1 : [num_users=1] = call_method[target=reshape](args = (%add_1, -1, 4), kwargs = {})
    %arange_4 : [num_users=1] = call_function[target=torch.arange](args = (0, 50), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_2 : [num_users=1] = call_function[target=operator.mul](args = (%arange_4, %stride_width_2), kwargs = {})
    %arange_5 : [num_users=1] = call_function[target=torch.arange](args = (0, 50), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_2 : [num_users=1] = call_function[target=operator.mul](args = (%arange_5, %stride_height_2), kwargs = {})
    %meshgrid_2 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_2, %shifts_x_2), kwargs = {indexing: ij})
    %shift_y_4 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_2, 0), kwargs = {})
    %shift_x_4 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_2, 1), kwargs = {})
    %shift_x_5 : [num_users=1] = call_method[target=reshape](args = (%shift_x_4, -1), kwargs = {})
    %shift_y_5 : [num_users=1] = call_method[target=reshape](args = (%shift_y_4, -1), kwargs = {})
    %shifts_2 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_5, %shift_y_5, %shift_x_5, %shift_y_5),), kwargs = {dim: 1})
    %view_4 : [num_users=1] = call_method[target=view](args = (%shifts_2, -1, 1, 4), kwargs = {})
    %view_5 : [num_users=1] = call_method[target=view](args = (%base_anchors_2, 1, -1, 4), kwargs = {})
    %add_2 : [num_users=1] = call_function[target=operator.add](args = (%view_4, %view_5), kwargs = {})
    %anchors_per_feature_map_2 : [num_users=1] = call_method[target=reshape](args = (%add_2, -1, 4), kwargs = {})
    %arange_6 : [num_users=1] = call_function[target=torch.arange](args = (0, 25), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_3 : [num_users=1] = call_function[target=operator.mul](args = (%arange_6, %stride_width_3), kwargs = {})
    %arange_7 : [num_users=1] = call_function[target=torch.arange](args = (0, 25), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_3 : [num_users=1] = call_function[target=operator.mul](args = (%arange_7, %stride_height_3), kwargs = {})
    %meshgrid_3 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_3, %shifts_x_3), kwargs = {indexing: ij})
    %shift_y_6 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_3, 0), kwargs = {})
    %shift_x_6 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_3, 1), kwargs = {})
    %shift_x_7 : [num_users=1] = call_method[target=reshape](args = (%shift_x_6, -1), kwargs = {})
    %shift_y_7 : [num_users=1] = call_method[target=reshape](args = (%shift_y_6, -1), kwargs = {})
    %shifts_3 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_7, %shift_y_7, %shift_x_7, %shift_y_7),), kwargs = {dim: 1})
    %view_6 : [num_users=1] = call_method[target=view](args = (%shifts_3, -1, 1, 4), kwargs = {})
    %view_7 : [num_users=1] = call_method[target=view](args = (%base_anchors_3, 1, -1, 4), kwargs = {})
    %add_3 : [num_users=1] = call_function[target=operator.add](args = (%view_6, %view_7), kwargs = {})
    %anchors_per_feature_map_3 : [num_users=1] = call_method[target=reshape](args = (%add_3, -1, 4), kwargs = {})
    %arange_8 : [num_users=1] = call_function[target=torch.arange](args = (0, 13), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_4 : [num_users=1] = call_function[target=operator.mul](args = (%arange_8, %stride_width_4), kwargs = {})
    %arange_9 : [num_users=1] = call_function[target=torch.arange](args = (0, 13), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_4 : [num_users=1] = call_function[target=operator.mul](args = (%arange_9, %stride_height_4), kwargs = {})
    %meshgrid_4 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_4, %shifts_x_4), kwargs = {indexing: ij})
    %shift_y_8 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_4, 0), kwargs = {})
    %shift_x_8 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_4, 1), kwargs = {})
    %shift_x_9 : [num_users=1] = call_method[target=reshape](args = (%shift_x_8, -1), kwargs = {})
    %shift_y_9 : [num_users=1] = call_method[target=reshape](args = (%shift_y_8, -1), kwargs = {})
    %shifts_4 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_9, %shift_y_9, %shift_x_9, %shift_y_9),), kwargs = {dim: 1})
    %view_8 : [num_users=1] = call_method[target=view](args = (%shifts_4, -1, 1, 4), kwargs = {})
    %view_9 : [num_users=1] = call_method[target=view](args = (%base_anchors_4, 1, -1, 4), kwargs = {})
    %add_4 : [num_users=1] = call_function[target=operator.add](args = (%view_8, %view_9), kwargs = {})
    %anchors_per_feature_map_4 : [num_users=1] = call_method[target=reshape](args = (%add_4, -1, 4), kwargs = {})
    %b : [num_users=2] = call_function[target=torch.cat](args = ([%anchors_per_feature_map, %anchors_per_feature_map_1, %anchors_per_feature_map_2, %anchors_per_feature_map_3, %anchors_per_feature_map_4],), kwargs = {})
    %layer : [num_users=1] = call_method[target=view](args = (%box_cls_per_level, 1, -1, 1, 200, 200), kwargs = {})
    %layer_1 : [num_users=1] = call_method[target=permute](args = (%layer, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_1 : [num_users=1] = call_method[target=reshape](args = (%layer_1, 1, -1, 1), kwargs = {})
    %layer_3 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level, 1, -1, 4, 200, 200), kwargs = {})
    %layer_4 : [num_users=1] = call_method[target=permute](args = (%layer_3, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_1 : [num_users=1] = call_method[target=reshape](args = (%layer_4, 1, -1, 4), kwargs = {})
    %layer_6 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_2, 1, -1, 1, 100, 100), kwargs = {})
    %layer_7 : [num_users=1] = call_method[target=permute](args = (%layer_6, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_3 : [num_users=1] = call_method[target=reshape](args = (%layer_7, 1, -1, 1), kwargs = {})
    %layer_9 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_2, 1, -1, 4, 100, 100), kwargs = {})
    %layer_10 : [num_users=1] = call_method[target=permute](args = (%layer_9, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_3 : [num_users=1] = call_method[target=reshape](args = (%layer_10, 1, -1, 4), kwargs = {})
    %layer_12 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_4, 1, -1, 1, 50, 50), kwargs = {})
    %layer_13 : [num_users=1] = call_method[target=permute](args = (%layer_12, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_5 : [num_users=1] = call_method[target=reshape](args = (%layer_13, 1, -1, 1), kwargs = {})
    %layer_15 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_4, 1, -1, 4, 50, 50), kwargs = {})
    %layer_16 : [num_users=1] = call_method[target=permute](args = (%layer_15, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_5 : [num_users=1] = call_method[target=reshape](args = (%layer_16, 1, -1, 4), kwargs = {})
    %layer_18 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_6, 1, -1, 1, 25, 25), kwargs = {})
    %layer_19 : [num_users=1] = call_method[target=permute](args = (%layer_18, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_7 : [num_users=1] = call_method[target=reshape](args = (%layer_19, 1, -1, 1), kwargs = {})
    %layer_21 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_6, 1, -1, 4, 25, 25), kwargs = {})
    %layer_22 : [num_users=1] = call_method[target=permute](args = (%layer_21, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_7 : [num_users=1] = call_method[target=reshape](args = (%layer_22, 1, -1, 4), kwargs = {})
    %layer_24 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_8, 1, -1, 1, 13, 13), kwargs = {})
    %layer_25 : [num_users=1] = call_method[target=permute](args = (%layer_24, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_9 : [num_users=1] = call_method[target=reshape](args = (%layer_25, 1, -1, 1), kwargs = {})
    %layer_27 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_8, 1, -1, 4, 13, 13), kwargs = {})
    %layer_28 : [num_users=1] = call_method[target=permute](args = (%layer_27, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_9 : [num_users=1] = call_method[target=reshape](args = (%layer_28, 1, -1, 4), kwargs = {})
    %cat_1 : [num_users=1] = call_function[target=torch.cat](args = ([%box_cls_per_level_1, %box_cls_per_level_3, %box_cls_per_level_5, %box_cls_per_level_7, %box_cls_per_level_9],), kwargs = {dim: 1})
    %objectness : [num_users=1] = call_method[target=flatten](args = (%cat_1, 0, -2), kwargs = {})
    %cat_2 : [num_users=1] = call_function[target=torch.cat](args = ([%box_regression_per_level_1, %box_regression_per_level_3, %box_regression_per_level_5, %box_regression_per_level_7, %box_regression_per_level_9],), kwargs = {dim: 1})
    %pred_bbox_deltas : [num_users=2] = call_method[target=reshape](args = (%cat_2, -1, 4), kwargs = {})
    %detach : [num_users=1] = call_method[target=detach](args = (%pred_bbox_deltas,), kwargs = {})
    %concat_boxes : [num_users=1] = call_function[target=torch.cat](args = ([%b],), kwargs = {dim: 0})
    %rel_codes : [num_users=4] = call_method[target=reshape](args = (%detach, 159882, -1), kwargs = {})
    %boxes : [num_users=6] = call_method[target=to](args = (%concat_boxes, torch.float32), kwargs = {})
    %getitem_15 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 2)), kwargs = {})
    %getitem_16 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 0)), kwargs = {})
    %widths : [num_users=3] = call_function[target=operator.sub](args = (%getitem_15, %getitem_16), kwargs = {})
    %getitem_17 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 3)), kwargs = {})
    %getitem_18 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 1)), kwargs = {})
    %heights : [num_users=3] = call_function[target=operator.sub](args = (%getitem_17, %getitem_18), kwargs = {})
    %getitem_19 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 0)), kwargs = {})
    %mul_10 : [num_users=1] = call_function[target=operator.mul](args = (0.5, %widths), kwargs = {})
    %ctr_x : [num_users=1] = call_function[target=operator.add](args = (%getitem_19, %mul_10), kwargs = {})
    %getitem_20 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 1)), kwargs = {})
    %mul_11 : [num_users=1] = call_function[target=operator.mul](args = (0.5, %heights), kwargs = {})
    %ctr_y : [num_users=1] = call_function[target=operator.add](args = (%getitem_20, %mul_11), kwargs = {})
    %getitem_21 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(0, None, 4))), kwargs = {})
    %dx : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_21, 1.0), kwargs = {})
    %getitem_22 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(1, None, 4))), kwargs = {})
    %dy : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_22, 1.0), kwargs = {})
    %getitem_23 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(2, None, 4))), kwargs = {})
    %dw : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_23, 1.0), kwargs = {})
    %getitem_24 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(3, None, 4))), kwargs = {})
    %dh : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_24, 1.0), kwargs = {})
    %dw_1 : [num_users=1] = call_function[target=torch.clamp](args = (%dw,), kwargs = {max: 4.135166556742356})
    %dh_1 : [num_users=1] = call_function[target=torch.clamp](args = (%dh,), kwargs = {max: 4.135166556742356})
    %getitem_25 : [num_users=1] = call_function[target=operator.getitem](args = (%widths, (slice(None, None, None), None)), kwargs = {})
    %mul_12 : [num_users=1] = call_function[target=operator.mul](args = (%dx, %getitem_25), kwargs = {})
    %getitem_26 : [num_users=1] = call_function[target=operator.getitem](args = (%ctr_x, (slice(None, None, None), None)), kwargs = {})
    %pred_ctr_x : [num_users=2] = call_function[target=operator.add](args = (%mul_12, %getitem_26), kwargs = {})
    %getitem_27 : [num_users=1] = call_function[target=operator.getitem](args = (%heights, (slice(None, None, None), None)), kwargs = {})
    %mul_13 : [num_users=1] = call_function[target=operator.mul](args = (%dy, %getitem_27), kwargs = {})
    %getitem_28 : [num_users=1] = call_function[target=operator.getitem](args = (%ctr_y, (slice(None, None, None), None)), kwargs = {})
    %pred_ctr_y : [num_users=2] = call_function[target=operator.add](args = (%mul_13, %getitem_28), kwargs = {})
    %exp : [num_users=1] = call_function[target=torch.exp](args = (%dw_1,), kwargs = {})
    %getitem_29 : [num_users=1] = call_function[target=operator.getitem](args = (%widths, (slice(None, None, None), None)), kwargs = {})
    %pred_w : [num_users=1] = call_function[target=operator.mul](args = (%exp, %getitem_29), kwargs = {})
    %exp_1 : [num_users=1] = call_function[target=torch.exp](args = (%dh_1,), kwargs = {})
    %getitem_30 : [num_users=1] = call_function[target=operator.getitem](args = (%heights, (slice(None, None, None), None)), kwargs = {})
    %pred_h : [num_users=1] = call_function[target=operator.mul](args = (%exp_1, %getitem_30), kwargs = {})
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (0.5,), kwargs = {dtype: torch.float32, device: cuda:0})
    %c_to_c_h : [num_users=2] = call_function[target=operator.mul](args = (%tensor, %pred_h), kwargs = {})
    %tensor_1 : [num_users=1] = call_function[target=torch.tensor](args = (0.5,), kwargs = {dtype: torch.float32, device: cuda:0})
    %c_to_c_w : [num_users=2] = call_function[target=operator.mul](args = (%tensor_1, %pred_w), kwargs = {})
    %pred_boxes1 : [num_users=1] = call_function[target=operator.sub](args = (%pred_ctr_x, %c_to_c_w), kwargs = {})
    %pred_boxes2 : [num_users=1] = call_function[target=operator.sub](args = (%pred_ctr_y, %c_to_c_h), kwargs = {})
    %pred_boxes3 : [num_users=1] = call_function[target=operator.add](args = (%pred_ctr_x, %c_to_c_w), kwargs = {})
    %pred_boxes4 : [num_users=1] = call_function[target=operator.add](args = (%pred_ctr_y, %c_to_c_h), kwargs = {})
    %stack_5 : [num_users=1] = call_function[target=torch.stack](args = ((%pred_boxes1, %pred_boxes2, %pred_boxes3, %pred_boxes4),), kwargs = {dim: 2})
    %pred_boxes : [num_users=1] = call_method[target=flatten](args = (%stack_5, 1), kwargs = {})
    %proposals : [num_users=1] = call_method[target=reshape](args = (%pred_boxes, 159882, -1, 4), kwargs = {})
    %proposals_1 : [num_users=1] = call_method[target=view](args = (%proposals, 1, -1, 4), kwargs = {})
    return (proposals_1, objectness, pred_bbox_deltas, b, base_anchors, base_anchors_1, base_anchors_2, base_anchors_3, base_anchors_4)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 1 detach nodes:
graph():
    %feature_map : torch.Tensor [num_users=1] = placeholder[target=L_features_0_]
    %feature_map_1 : torch.Tensor [num_users=1] = placeholder[target=L_features_1_]
    %feature_map_2 : torch.Tensor [num_users=1] = placeholder[target=L_features_2_]
    %feature_map_3 : torch.Tensor [num_users=1] = placeholder[target=L_features_3_]
    %feature_map_4 : torch.Tensor [num_users=1] = placeholder[target=L_features_pool_]
    %clone_default_4 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%feature_map_4,), kwargs = {})
    %clone_default_3 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%feature_map_3,), kwargs = {})
    %clone_default_2 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%feature_map_2,), kwargs = {})
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%feature_map_1,), kwargs = {})
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%feature_map,), kwargs = {})
    %l__self___head_conv_0_0 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%clone_default,), kwargs = {})
    %t : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_0,), kwargs = {})
    %box_cls_per_level : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t,), kwargs = {})
    %box_regression_per_level : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t,), kwargs = {})
    %l__self___head_conv_0_2 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%clone_default_1,), kwargs = {})
    %t_1 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_2,), kwargs = {})
    %box_cls_per_level_2 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_1,), kwargs = {})
    %box_regression_per_level_2 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_1,), kwargs = {})
    %l__self___head_conv_0_4 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%clone_default_2,), kwargs = {})
    %t_2 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_4,), kwargs = {})
    %box_cls_per_level_4 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_2,), kwargs = {})
    %box_regression_per_level_4 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_2,), kwargs = {})
    %l__self___head_conv_0_6 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%clone_default_3,), kwargs = {})
    %t_3 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_6,), kwargs = {})
    %box_cls_per_level_6 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_3,), kwargs = {})
    %box_regression_per_level_6 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_3,), kwargs = {})
    %l__self___head_conv_0_8 : [num_users=1] = call_module[target=L__self___head_conv_0_0](args = (%clone_default_4,), kwargs = {})
    %t_4 : [num_users=2] = call_module[target=L__self___head_conv_0_1](args = (%l__self___head_conv_0_8,), kwargs = {})
    %box_cls_per_level_8 : [num_users=1] = call_module[target=L__self___head_cls_logits](args = (%t_4,), kwargs = {})
    %box_regression_per_level_8 : [num_users=1] = call_module[target=L__self___head_bbox_pred](args = (%t_4,), kwargs = {})
    %empty : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height : [num_users=1] = call_method[target=fill_](args = (%empty, 4), kwargs = {})
    %empty_1 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width : [num_users=1] = call_method[target=fill_](args = (%empty_1, 4), kwargs = {})
    %empty_2 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_1 : [num_users=1] = call_method[target=fill_](args = (%empty_2, 8), kwargs = {})
    %empty_3 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_1 : [num_users=1] = call_method[target=fill_](args = (%empty_3, 8), kwargs = {})
    %empty_4 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_2 : [num_users=1] = call_method[target=fill_](args = (%empty_4, 16), kwargs = {})
    %empty_5 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_2 : [num_users=1] = call_method[target=fill_](args = (%empty_5, 16), kwargs = {})
    %empty_6 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_3 : [num_users=1] = call_method[target=fill_](args = (%empty_6, 32), kwargs = {})
    %empty_7 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_3 : [num_users=1] = call_method[target=fill_](args = (%empty_7, 32), kwargs = {})
    %empty_8 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_height_4 : [num_users=1] = call_method[target=fill_](args = (%empty_8, 61), kwargs = {})
    %empty_9 : [num_users=1] = call_function[target=torch.empty](args = ((),), kwargs = {dtype: torch.int64, device: cuda:0})
    %stride_width_4 : [num_users=1] = call_method[target=fill_](args = (%empty_9, 61), kwargs = {})
    %cell_anchor : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_0]
    %base_anchors : [num_users=2] = call_method[target=to](args = (%cell_anchor,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_1 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_1]
    %base_anchors_1 : [num_users=2] = call_method[target=to](args = (%cell_anchor_1,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_2 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_2]
    %base_anchors_2 : [num_users=2] = call_method[target=to](args = (%cell_anchor_2,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_3 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_3]
    %base_anchors_3 : [num_users=2] = call_method[target=to](args = (%cell_anchor_3,), kwargs = {dtype: torch.float32, device: cuda:0})
    %cell_anchor_4 : [num_users=1] = get_attr[target=self___anchor_generator_cell_anchors_4]
    %base_anchors_4 : [num_users=2] = call_method[target=to](args = (%cell_anchor_4,), kwargs = {dtype: torch.float32, device: cuda:0})
    %arange : [num_users=1] = call_function[target=torch.arange](args = (0, 200), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x : [num_users=1] = call_function[target=operator.mul](args = (%arange, %stride_width), kwargs = {})
    %arange_1 : [num_users=1] = call_function[target=torch.arange](args = (0, 200), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y : [num_users=1] = call_function[target=operator.mul](args = (%arange_1, %stride_height), kwargs = {})
    %meshgrid : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y, %shifts_x), kwargs = {indexing: ij})
    %shift_y : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid, 0), kwargs = {})
    %shift_x : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid, 1), kwargs = {})
    %shift_x_1 : [num_users=1] = call_method[target=reshape](args = (%shift_x, -1), kwargs = {})
    %shift_y_1 : [num_users=1] = call_method[target=reshape](args = (%shift_y, -1), kwargs = {})
    %shifts : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_1, %shift_y_1, %shift_x_1, %shift_y_1),), kwargs = {dim: 1})
    %view : [num_users=1] = call_method[target=view](args = (%shifts, -1, 1, 4), kwargs = {})
    %view_1 : [num_users=1] = call_method[target=view](args = (%base_anchors, 1, -1, 4), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%view, %view_1), kwargs = {})
    %anchors_per_feature_map : [num_users=1] = call_method[target=reshape](args = (%add, -1, 4), kwargs = {})
    %arange_2 : [num_users=1] = call_function[target=torch.arange](args = (0, 100), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_1 : [num_users=1] = call_function[target=operator.mul](args = (%arange_2, %stride_width_1), kwargs = {})
    %arange_3 : [num_users=1] = call_function[target=torch.arange](args = (0, 100), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_1 : [num_users=1] = call_function[target=operator.mul](args = (%arange_3, %stride_height_1), kwargs = {})
    %meshgrid_1 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_1, %shifts_x_1), kwargs = {indexing: ij})
    %shift_y_2 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_1, 0), kwargs = {})
    %shift_x_2 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_1, 1), kwargs = {})
    %shift_x_3 : [num_users=1] = call_method[target=reshape](args = (%shift_x_2, -1), kwargs = {})
    %shift_y_3 : [num_users=1] = call_method[target=reshape](args = (%shift_y_2, -1), kwargs = {})
    %shifts_1 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_3, %shift_y_3, %shift_x_3, %shift_y_3),), kwargs = {dim: 1})
    %view_2 : [num_users=1] = call_method[target=view](args = (%shifts_1, -1, 1, 4), kwargs = {})
    %view_3 : [num_users=1] = call_method[target=view](args = (%base_anchors_1, 1, -1, 4), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%view_2, %view_3), kwargs = {})
    %anchors_per_feature_map_1 : [num_users=1] = call_method[target=reshape](args = (%add_1, -1, 4), kwargs = {})
    %arange_4 : [num_users=1] = call_function[target=torch.arange](args = (0, 50), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_2 : [num_users=1] = call_function[target=operator.mul](args = (%arange_4, %stride_width_2), kwargs = {})
    %arange_5 : [num_users=1] = call_function[target=torch.arange](args = (0, 50), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_2 : [num_users=1] = call_function[target=operator.mul](args = (%arange_5, %stride_height_2), kwargs = {})
    %meshgrid_2 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_2, %shifts_x_2), kwargs = {indexing: ij})
    %shift_y_4 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_2, 0), kwargs = {})
    %shift_x_4 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_2, 1), kwargs = {})
    %shift_x_5 : [num_users=1] = call_method[target=reshape](args = (%shift_x_4, -1), kwargs = {})
    %shift_y_5 : [num_users=1] = call_method[target=reshape](args = (%shift_y_4, -1), kwargs = {})
    %shifts_2 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_5, %shift_y_5, %shift_x_5, %shift_y_5),), kwargs = {dim: 1})
    %view_4 : [num_users=1] = call_method[target=view](args = (%shifts_2, -1, 1, 4), kwargs = {})
    %view_5 : [num_users=1] = call_method[target=view](args = (%base_anchors_2, 1, -1, 4), kwargs = {})
    %add_2 : [num_users=1] = call_function[target=operator.add](args = (%view_4, %view_5), kwargs = {})
    %anchors_per_feature_map_2 : [num_users=1] = call_method[target=reshape](args = (%add_2, -1, 4), kwargs = {})
    %arange_6 : [num_users=1] = call_function[target=torch.arange](args = (0, 25), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_3 : [num_users=1] = call_function[target=operator.mul](args = (%arange_6, %stride_width_3), kwargs = {})
    %arange_7 : [num_users=1] = call_function[target=torch.arange](args = (0, 25), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_3 : [num_users=1] = call_function[target=operator.mul](args = (%arange_7, %stride_height_3), kwargs = {})
    %meshgrid_3 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_3, %shifts_x_3), kwargs = {indexing: ij})
    %shift_y_6 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_3, 0), kwargs = {})
    %shift_x_6 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_3, 1), kwargs = {})
    %shift_x_7 : [num_users=1] = call_method[target=reshape](args = (%shift_x_6, -1), kwargs = {})
    %shift_y_7 : [num_users=1] = call_method[target=reshape](args = (%shift_y_6, -1), kwargs = {})
    %shifts_3 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_7, %shift_y_7, %shift_x_7, %shift_y_7),), kwargs = {dim: 1})
    %view_6 : [num_users=1] = call_method[target=view](args = (%shifts_3, -1, 1, 4), kwargs = {})
    %view_7 : [num_users=1] = call_method[target=view](args = (%base_anchors_3, 1, -1, 4), kwargs = {})
    %add_3 : [num_users=1] = call_function[target=operator.add](args = (%view_6, %view_7), kwargs = {})
    %anchors_per_feature_map_3 : [num_users=1] = call_method[target=reshape](args = (%add_3, -1, 4), kwargs = {})
    %arange_8 : [num_users=1] = call_function[target=torch.arange](args = (0, 13), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_x_4 : [num_users=1] = call_function[target=operator.mul](args = (%arange_8, %stride_width_4), kwargs = {})
    %arange_9 : [num_users=1] = call_function[target=torch.arange](args = (0, 13), kwargs = {dtype: torch.int32, device: cuda:0})
    %shifts_y_4 : [num_users=1] = call_function[target=operator.mul](args = (%arange_9, %stride_height_4), kwargs = {})
    %meshgrid_4 : [num_users=2] = call_function[target=torch.functional.meshgrid](args = (%shifts_y_4, %shifts_x_4), kwargs = {indexing: ij})
    %shift_y_8 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_4, 0), kwargs = {})
    %shift_x_8 : [num_users=1] = call_function[target=operator.getitem](args = (%meshgrid_4, 1), kwargs = {})
    %shift_x_9 : [num_users=1] = call_method[target=reshape](args = (%shift_x_8, -1), kwargs = {})
    %shift_y_9 : [num_users=1] = call_method[target=reshape](args = (%shift_y_8, -1), kwargs = {})
    %shifts_4 : [num_users=1] = call_function[target=torch.stack](args = ((%shift_x_9, %shift_y_9, %shift_x_9, %shift_y_9),), kwargs = {dim: 1})
    %view_8 : [num_users=1] = call_method[target=view](args = (%shifts_4, -1, 1, 4), kwargs = {})
    %view_9 : [num_users=1] = call_method[target=view](args = (%base_anchors_4, 1, -1, 4), kwargs = {})
    %add_4 : [num_users=1] = call_function[target=operator.add](args = (%view_8, %view_9), kwargs = {})
    %anchors_per_feature_map_4 : [num_users=1] = call_method[target=reshape](args = (%add_4, -1, 4), kwargs = {})
    %b : [num_users=2] = call_function[target=torch.cat](args = ([%anchors_per_feature_map, %anchors_per_feature_map_1, %anchors_per_feature_map_2, %anchors_per_feature_map_3, %anchors_per_feature_map_4],), kwargs = {})
    %layer : [num_users=1] = call_method[target=view](args = (%box_cls_per_level, 1, -1, 1, 200, 200), kwargs = {})
    %layer_1 : [num_users=1] = call_method[target=permute](args = (%layer, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_1 : [num_users=1] = call_method[target=reshape](args = (%layer_1, 1, -1, 1), kwargs = {})
    %layer_3 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level, 1, -1, 4, 200, 200), kwargs = {})
    %layer_4 : [num_users=1] = call_method[target=permute](args = (%layer_3, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_1 : [num_users=1] = call_method[target=reshape](args = (%layer_4, 1, -1, 4), kwargs = {})
    %layer_6 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_2, 1, -1, 1, 100, 100), kwargs = {})
    %layer_7 : [num_users=1] = call_method[target=permute](args = (%layer_6, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_3 : [num_users=1] = call_method[target=reshape](args = (%layer_7, 1, -1, 1), kwargs = {})
    %layer_9 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_2, 1, -1, 4, 100, 100), kwargs = {})
    %layer_10 : [num_users=1] = call_method[target=permute](args = (%layer_9, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_3 : [num_users=1] = call_method[target=reshape](args = (%layer_10, 1, -1, 4), kwargs = {})
    %layer_12 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_4, 1, -1, 1, 50, 50), kwargs = {})
    %layer_13 : [num_users=1] = call_method[target=permute](args = (%layer_12, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_5 : [num_users=1] = call_method[target=reshape](args = (%layer_13, 1, -1, 1), kwargs = {})
    %layer_15 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_4, 1, -1, 4, 50, 50), kwargs = {})
    %layer_16 : [num_users=1] = call_method[target=permute](args = (%layer_15, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_5 : [num_users=1] = call_method[target=reshape](args = (%layer_16, 1, -1, 4), kwargs = {})
    %layer_18 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_6, 1, -1, 1, 25, 25), kwargs = {})
    %layer_19 : [num_users=1] = call_method[target=permute](args = (%layer_18, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_7 : [num_users=1] = call_method[target=reshape](args = (%layer_19, 1, -1, 1), kwargs = {})
    %layer_21 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_6, 1, -1, 4, 25, 25), kwargs = {})
    %layer_22 : [num_users=1] = call_method[target=permute](args = (%layer_21, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_7 : [num_users=1] = call_method[target=reshape](args = (%layer_22, 1, -1, 4), kwargs = {})
    %layer_24 : [num_users=1] = call_method[target=view](args = (%box_cls_per_level_8, 1, -1, 1, 13, 13), kwargs = {})
    %layer_25 : [num_users=1] = call_method[target=permute](args = (%layer_24, 0, 3, 4, 1, 2), kwargs = {})
    %box_cls_per_level_9 : [num_users=1] = call_method[target=reshape](args = (%layer_25, 1, -1, 1), kwargs = {})
    %layer_27 : [num_users=1] = call_method[target=view](args = (%box_regression_per_level_8, 1, -1, 4, 13, 13), kwargs = {})
    %layer_28 : [num_users=1] = call_method[target=permute](args = (%layer_27, 0, 3, 4, 1, 2), kwargs = {})
    %box_regression_per_level_9 : [num_users=1] = call_method[target=reshape](args = (%layer_28, 1, -1, 4), kwargs = {})
    %cat_1 : [num_users=1] = call_function[target=torch.cat](args = ([%box_cls_per_level_1, %box_cls_per_level_3, %box_cls_per_level_5, %box_cls_per_level_7, %box_cls_per_level_9],), kwargs = {dim: 1})
    %objectness : [num_users=1] = call_method[target=flatten](args = (%cat_1, 0, -2), kwargs = {})
    %cat_2 : [num_users=1] = call_function[target=torch.cat](args = ([%box_regression_per_level_1, %box_regression_per_level_3, %box_regression_per_level_5, %box_regression_per_level_7, %box_regression_per_level_9],), kwargs = {dim: 1})
    %pred_bbox_deltas : [num_users=2] = call_method[target=reshape](args = (%cat_2, -1, 4), kwargs = {})
    %concat_boxes : [num_users=1] = call_function[target=torch.cat](args = ([%b],), kwargs = {dim: 0})
    %rel_codes : [num_users=4] = call_method[target=reshape](args = (%pred_bbox_deltas, 159882, -1), kwargs = {})
    %boxes : [num_users=6] = call_method[target=to](args = (%concat_boxes, torch.float32), kwargs = {})
    %getitem_15 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 2)), kwargs = {})
    %getitem_16 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 0)), kwargs = {})
    %widths : [num_users=3] = call_function[target=operator.sub](args = (%getitem_15, %getitem_16), kwargs = {})
    %getitem_17 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 3)), kwargs = {})
    %getitem_18 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 1)), kwargs = {})
    %heights : [num_users=3] = call_function[target=operator.sub](args = (%getitem_17, %getitem_18), kwargs = {})
    %getitem_19 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 0)), kwargs = {})
    %mul_10 : [num_users=1] = call_function[target=operator.mul](args = (0.5, %widths), kwargs = {})
    %ctr_x : [num_users=1] = call_function[target=operator.add](args = (%getitem_19, %mul_10), kwargs = {})
    %getitem_20 : [num_users=1] = call_function[target=operator.getitem](args = (%boxes, (slice(None, None, None), 1)), kwargs = {})
    %mul_11 : [num_users=1] = call_function[target=operator.mul](args = (0.5, %heights), kwargs = {})
    %ctr_y : [num_users=1] = call_function[target=operator.add](args = (%getitem_20, %mul_11), kwargs = {})
    %getitem_21 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(0, None, 4))), kwargs = {})
    %dx : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_21, 1.0), kwargs = {})
    %getitem_22 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(1, None, 4))), kwargs = {})
    %dy : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_22, 1.0), kwargs = {})
    %getitem_23 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(2, None, 4))), kwargs = {})
    %dw : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_23, 1.0), kwargs = {})
    %getitem_24 : [num_users=1] = call_function[target=operator.getitem](args = (%rel_codes, (slice(None, None, None), slice(3, None, 4))), kwargs = {})
    %dh : [num_users=1] = call_function[target=operator.truediv](args = (%getitem_24, 1.0), kwargs = {})
    %dw_1 : [num_users=1] = call_function[target=torch.clamp](args = (%dw,), kwargs = {max: 4.135166556742356})
    %dh_1 : [num_users=1] = call_function[target=torch.clamp](args = (%dh,), kwargs = {max: 4.135166556742356})
    %getitem_25 : [num_users=1] = call_function[target=operator.getitem](args = (%widths, (slice(None, None, None), None)), kwargs = {})
    %mul_12 : [num_users=1] = call_function[target=operator.mul](args = (%dx, %getitem_25), kwargs = {})
    %getitem_26 : [num_users=1] = call_function[target=operator.getitem](args = (%ctr_x, (slice(None, None, None), None)), kwargs = {})
    %pred_ctr_x : [num_users=2] = call_function[target=operator.add](args = (%mul_12, %getitem_26), kwargs = {})
    %getitem_27 : [num_users=1] = call_function[target=operator.getitem](args = (%heights, (slice(None, None, None), None)), kwargs = {})
    %mul_13 : [num_users=1] = call_function[target=operator.mul](args = (%dy, %getitem_27), kwargs = {})
    %getitem_28 : [num_users=1] = call_function[target=operator.getitem](args = (%ctr_y, (slice(None, None, None), None)), kwargs = {})
    %pred_ctr_y : [num_users=2] = call_function[target=operator.add](args = (%mul_13, %getitem_28), kwargs = {})
    %exp : [num_users=1] = call_function[target=torch.exp](args = (%dw_1,), kwargs = {})
    %getitem_29 : [num_users=1] = call_function[target=operator.getitem](args = (%widths, (slice(None, None, None), None)), kwargs = {})
    %pred_w : [num_users=1] = call_function[target=operator.mul](args = (%exp, %getitem_29), kwargs = {})
    %exp_1 : [num_users=1] = call_function[target=torch.exp](args = (%dh_1,), kwargs = {})
    %getitem_30 : [num_users=1] = call_function[target=operator.getitem](args = (%heights, (slice(None, None, None), None)), kwargs = {})
    %pred_h : [num_users=1] = call_function[target=operator.mul](args = (%exp_1, %getitem_30), kwargs = {})
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (0.5,), kwargs = {dtype: torch.float32, device: cuda:0})
    %c_to_c_h : [num_users=2] = call_function[target=operator.mul](args = (%tensor, %pred_h), kwargs = {})
    %tensor_1 : [num_users=1] = call_function[target=torch.tensor](args = (0.5,), kwargs = {dtype: torch.float32, device: cuda:0})
    %c_to_c_w : [num_users=2] = call_function[target=operator.mul](args = (%tensor_1, %pred_w), kwargs = {})
    %pred_boxes1 : [num_users=1] = call_function[target=operator.sub](args = (%pred_ctr_x, %c_to_c_w), kwargs = {})
    %pred_boxes2 : [num_users=1] = call_function[target=operator.sub](args = (%pred_ctr_y, %c_to_c_h), kwargs = {})
    %pred_boxes3 : [num_users=1] = call_function[target=operator.add](args = (%pred_ctr_x, %c_to_c_w), kwargs = {})
    %pred_boxes4 : [num_users=1] = call_function[target=operator.add](args = (%pred_ctr_y, %c_to_c_h), kwargs = {})
    %stack_5 : [num_users=1] = call_function[target=torch.stack](args = ((%pred_boxes1, %pred_boxes2, %pred_boxes3, %pred_boxes4),), kwargs = {dim: 2})
    %pred_boxes : [num_users=1] = call_method[target=flatten](args = (%stack_5, 1), kwargs = {})
    %proposals : [num_users=1] = call_method[target=reshape](args = (%pred_boxes, 159882, -1, 4), kwargs = {})
    %proposals_1 : [num_users=1] = call_method[target=view](args = (%proposals, 1, -1, 4), kwargs = {})
    return (proposals_1, objectness, pred_bbox_deltas, b, base_anchors, base_anchors_1, base_anchors_2, base_anchors_3, base_anchors_4)
DEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %arg2_1 : [num_users=1] = placeholder[target=arg2_1]
    %arg3_1 : [num_users=1] = placeholder[target=arg3_1]
    %arg4_1 : [num_users=1] = placeholder[target=arg4_1]
    %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg4_1,), kwargs = {})
    %clone_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg3_1,), kwargs = {})
    %clone_2 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg2_1,), kwargs = {})
    %clone_3 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg1_1,), kwargs = {})
    %clone_4 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {})
    %_param_constant0 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%clone_4, %_param_constant0, %_param_constant1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})
    %_param_constant2 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_1 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu, %_param_constant2, %_param_constant3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_2 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu, %_param_constant4, %_param_constant5, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_1 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_1 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_3 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%clone_3, %_param_constant0_1, %_param_constant1_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_3,), kwargs = {})
    %_param_constant2_1 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_1 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_4 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_1, %_param_constant2_1, %_param_constant3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_1 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_1 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_5 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_1, %_param_constant4_1, %_param_constant5_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_2 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_2 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_6 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%clone_2, %_param_constant0_2, %_param_constant1_2, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_2 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_6,), kwargs = {})
    %_param_constant2_2 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_2 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_7 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_2, %_param_constant2_2, %_param_constant3_2, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_2 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_2 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_8 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_2, %_param_constant4_2, %_param_constant5_2, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_3 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_3 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_9 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%clone_1, %_param_constant0_3, %_param_constant1_3, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_3 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_9,), kwargs = {})
    %_param_constant2_3 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_3 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_10 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_3, %_param_constant2_3, %_param_constant3_3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_3 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_3 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_11 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_3, %_param_constant4_3, %_param_constant5_3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_4 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_4 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_12 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%clone, %_param_constant0_4, %_param_constant1_4, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_4 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_12,), kwargs = {})
    %_param_constant2_4 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_4 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_13 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_4, %_param_constant2_4, %_param_constant3_4, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_4 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_4 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_14 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_4, %_param_constant4_4, %_param_constant5_4, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %empty : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty, 4), kwargs = {pin_memory: False})
    %empty_1 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_1 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_1, 4), kwargs = {pin_memory: False})
    %empty_2 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_2 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_2, 8), kwargs = {pin_memory: False})
    %empty_3 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_3 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_3, 8), kwargs = {pin_memory: False})
    %empty_4 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_4 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_4, 16), kwargs = {pin_memory: False})
    %empty_5 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_5 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_5, 16), kwargs = {pin_memory: False})
    %empty_6 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_6 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_6, 32), kwargs = {pin_memory: False})
    %empty_7 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_7 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_7, 32), kwargs = {pin_memory: False})
    %empty_8 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_8 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_8, 61), kwargs = {pin_memory: False})
    %empty_9 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_9 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_9, 61), kwargs = {pin_memory: False})
    %arange : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 200), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange, %full_like_1), kwargs = {})
    %arange_1 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 200), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_1, %full_like), kwargs = {})
    %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_1, [-1, 1]), kwargs = {})
    %expand : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view, [200, 200]), kwargs = {})
    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul, [1, -1]), kwargs = {})
    %expand_1 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_1, [200, 200]), kwargs = {})
    %clone_5 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_1,), kwargs = {memory_format: torch.contiguous_format})
    %view_2 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_5, [40000]), kwargs = {})
    %clone_6 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand,), kwargs = {memory_format: torch.contiguous_format})
    %view_3 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_6, [40000]), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_2, 1), kwargs = {})
    %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_3, 1), kwargs = {})
    %unsqueeze_2 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_2, 1), kwargs = {})
    %unsqueeze_3 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_3, 1), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze, %unsqueeze_1, %unsqueeze_2, %unsqueeze_3], 1), kwargs = {})
    %view_4 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat, [-1, 1, 4]), kwargs = {})
    %_param_constant6 : [num_users=1] = get_attr[target=_param_constant6]
    %view_5 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%_param_constant6, [1, -1, 4]), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_4, %view_5), kwargs = {})
    %view_6 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%add, [-1, 4]), kwargs = {})
    %arange_2 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 100), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_2, %full_like_3), kwargs = {})
    %arange_3 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 100), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_3 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_3, %full_like_2), kwargs = {})
    %view_7 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_3, [-1, 1]), kwargs = {})
    %expand_2 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_7, [100, 100]), kwargs = {})
    %view_8 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_2, [1, -1]), kwargs = {})
    %expand_3 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_8, [100, 100]), kwargs = {})
    %clone_7 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_3,), kwargs = {memory_format: torch.contiguous_format})
    %view_9 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_7, [10000]), kwargs = {})
    %clone_8 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_2,), kwargs = {memory_format: torch.contiguous_format})
    %view_10 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_8, [10000]), kwargs = {})
    %unsqueeze_4 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_9, 1), kwargs = {})
    %unsqueeze_5 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_10, 1), kwargs = {})
    %unsqueeze_6 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_9, 1), kwargs = {})
    %unsqueeze_7 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_10, 1), kwargs = {})
    %cat_1 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze_4, %unsqueeze_5, %unsqueeze_6, %unsqueeze_7], 1), kwargs = {})
    %view_11 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat_1, [-1, 1, 4]), kwargs = {})
    %_param_constant7 : [num_users=1] = get_attr[target=_param_constant7]
    %view_12 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%_param_constant7, [1, -1, 4]), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_11, %view_12), kwargs = {})
    %view_13 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%add_1, [-1, 4]), kwargs = {})
    %arange_4 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 50), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_4 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_4, %full_like_5), kwargs = {})
    %arange_5 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 50), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_5 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_5, %full_like_4), kwargs = {})
    %view_14 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_5, [-1, 1]), kwargs = {})
    %expand_4 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_14, [50, 50]), kwargs = {})
    %view_15 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_4, [1, -1]), kwargs = {})
    %expand_5 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_15, [50, 50]), kwargs = {})
    %clone_9 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_5,), kwargs = {memory_format: torch.contiguous_format})
    %view_16 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_9, [2500]), kwargs = {})
    %clone_10 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_4,), kwargs = {memory_format: torch.contiguous_format})
    %view_17 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_10, [2500]), kwargs = {})
    %unsqueeze_8 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_16, 1), kwargs = {})
    %unsqueeze_9 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_17, 1), kwargs = {})
    %unsqueeze_10 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_16, 1), kwargs = {})
    %unsqueeze_11 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_17, 1), kwargs = {})
    %cat_2 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze_8, %unsqueeze_9, %unsqueeze_10, %unsqueeze_11], 1), kwargs = {})
    %view_18 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat_2, [-1, 1, 4]), kwargs = {})
    %_param_constant8 : [num_users=1] = get_attr[target=_param_constant8]
    %view_19 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%_param_constant8, [1, -1, 4]), kwargs = {})
    %add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_18, %view_19), kwargs = {})
    %view_20 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%add_2, [-1, 4]), kwargs = {})
    %arange_6 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 25), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_6 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_6, %full_like_7), kwargs = {})
    %arange_7 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 25), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_7 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_7, %full_like_6), kwargs = {})
    %view_21 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_7, [-1, 1]), kwargs = {})
    %expand_6 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_21, [25, 25]), kwargs = {})
    %view_22 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_6, [1, -1]), kwargs = {})
    %expand_7 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_22, [25, 25]), kwargs = {})
    %clone_11 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_7,), kwargs = {memory_format: torch.contiguous_format})
    %view_23 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_11, [625]), kwargs = {})
    %clone_12 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_6,), kwargs = {memory_format: torch.contiguous_format})
    %view_24 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_12, [625]), kwargs = {})
    %unsqueeze_12 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_23, 1), kwargs = {})
    %unsqueeze_13 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_24, 1), kwargs = {})
    %unsqueeze_14 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_23, 1), kwargs = {})
    %unsqueeze_15 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_24, 1), kwargs = {})
    %cat_3 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze_12, %unsqueeze_13, %unsqueeze_14, %unsqueeze_15], 1), kwargs = {})
    %view_25 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat_3, [-1, 1, 4]), kwargs = {})
    %_param_constant9 : [num_users=1] = get_attr[target=_param_constant9]
    %view_26 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%_param_constant9, [1, -1, 4]), kwargs = {})
    %add_3 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_25, %view_26), kwargs = {})
    %view_27 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%add_3, [-1, 4]), kwargs = {})
    %arange_8 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 13), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_8 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_8, %full_like_9), kwargs = {})
    %arange_9 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 13), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_9 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_9, %full_like_8), kwargs = {})
    %view_28 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_9, [-1, 1]), kwargs = {})
    %expand_8 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_28, [13, 13]), kwargs = {})
    %view_29 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_8, [1, -1]), kwargs = {})
    %expand_9 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_29, [13, 13]), kwargs = {})
    %clone_13 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_9,), kwargs = {memory_format: torch.contiguous_format})
    %view_30 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_13, [169]), kwargs = {})
    %clone_14 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_8,), kwargs = {memory_format: torch.contiguous_format})
    %view_31 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_14, [169]), kwargs = {})
    %unsqueeze_16 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_30, 1), kwargs = {})
    %unsqueeze_17 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_31, 1), kwargs = {})
    %unsqueeze_18 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_30, 1), kwargs = {})
    %unsqueeze_19 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_31, 1), kwargs = {})
    %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze_16, %unsqueeze_17, %unsqueeze_18, %unsqueeze_19], 1), kwargs = {})
    %view_32 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat_4, [-1, 1, 4]), kwargs = {})
    %_param_constant10 : [num_users=1] = get_attr[target=_param_constant10]
    %view_33 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%_param_constant10, [1, -1, 4]), kwargs = {})
    %add_4 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_32, %view_33), kwargs = {})
    %view_34 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%add_4, [-1, 4]), kwargs = {})
    %cat_5 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%view_6, %view_13, %view_20, %view_27, %view_34],), kwargs = {})
    %view_35 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_1, [1, -1, 1, 200, 200]), kwargs = {})
    %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_35, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_15 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute,), kwargs = {memory_format: torch.contiguous_format})
    %view_36 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_15, [1, 120000, 1]), kwargs = {})
    %view_37 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_2, [1, -1, 4, 200, 200]), kwargs = {})
    %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_37, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_16 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_1,), kwargs = {memory_format: torch.contiguous_format})
    %view_38 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_16, [1, 120000, 4]), kwargs = {})
    %view_39 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_4, [1, -1, 1, 100, 100]), kwargs = {})
    %permute_2 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_39, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_17 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_2,), kwargs = {memory_format: torch.contiguous_format})
    %view_40 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_17, [1, 30000, 1]), kwargs = {})
    %view_41 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_5, [1, -1, 4, 100, 100]), kwargs = {})
    %permute_3 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_41, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_18 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_3,), kwargs = {memory_format: torch.contiguous_format})
    %view_42 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_18, [1, 30000, 4]), kwargs = {})
    %view_43 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_7, [1, -1, 1, 50, 50]), kwargs = {})
    %permute_4 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_43, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_19 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_4,), kwargs = {memory_format: torch.contiguous_format})
    %view_44 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_19, [1, 7500, 1]), kwargs = {})
    %view_45 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_8, [1, -1, 4, 50, 50]), kwargs = {})
    %permute_5 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_45, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_20 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_5,), kwargs = {memory_format: torch.contiguous_format})
    %view_46 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_20, [1, 7500, 4]), kwargs = {})
    %view_47 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_10, [1, -1, 1, 25, 25]), kwargs = {})
    %permute_6 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_47, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_21 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_6,), kwargs = {memory_format: torch.contiguous_format})
    %view_48 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_21, [1, 1875, 1]), kwargs = {})
    %view_49 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_11, [1, -1, 4, 25, 25]), kwargs = {})
    %permute_7 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_49, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_22 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_7,), kwargs = {memory_format: torch.contiguous_format})
    %view_50 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_22, [1, 1875, 4]), kwargs = {})
    %view_51 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_13, [1, -1, 1, 13, 13]), kwargs = {})
    %permute_8 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_51, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_23 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_8,), kwargs = {memory_format: torch.contiguous_format})
    %view_52 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_23, [1, 507, 1]), kwargs = {})
    %view_53 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_14, [1, -1, 4, 13, 13]), kwargs = {})
    %permute_9 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_53, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_24 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_9,), kwargs = {memory_format: torch.contiguous_format})
    %view_54 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_24, [1, 507, 4]), kwargs = {})
    %cat_6 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%view_36, %view_40, %view_44, %view_48, %view_52], 1), kwargs = {})
    %view_55 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat_6, [159882, 1]), kwargs = {})
    %cat_7 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%view_38, %view_42, %view_46, %view_50, %view_54], 1), kwargs = {})
    %view_56 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%cat_7, [-1, 4]), kwargs = {})
    %cat_8 : [num_users=6] = call_function[target=torch.ops.aten.cat.default](args = ([%cat_5],), kwargs = {})
    %view_57 : [num_users=4] = call_function[target=torch.ops.aten.view.default](args = (%view_56, [159882, -1]), kwargs = {})
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%cat_8, 0, 0, 9223372036854775807), kwargs = {})
    %select : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%slice_1, 1, 2), kwargs = {})
    %slice_2 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%cat_8, 0, 0, 9223372036854775807), kwargs = {})
    %select_1 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%slice_2, 1, 0), kwargs = {})
    %sub : [num_users=3] = call_function[target=torch.ops.aten.sub.Tensor](args = (%select, %select_1), kwargs = {})
    %slice_3 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%cat_8, 0, 0, 9223372036854775807), kwargs = {})
    %select_2 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%slice_3, 1, 3), kwargs = {})
    %slice_4 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%cat_8, 0, 0, 9223372036854775807), kwargs = {})
    %select_3 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%slice_4, 1, 1), kwargs = {})
    %sub_1 : [num_users=3] = call_function[target=torch.ops.aten.sub.Tensor](args = (%select_2, %select_3), kwargs = {})
    %slice_5 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%cat_8, 0, 0, 9223372036854775807), kwargs = {})
    %select_4 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%slice_5, 1, 0), kwargs = {})
    %mul_10 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub, 0.5), kwargs = {})
    %add_5 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%select_4, %mul_10), kwargs = {})
    %slice_6 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%cat_8, 0, 0, 9223372036854775807), kwargs = {})
    %select_5 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%slice_6, 1, 1), kwargs = {})
    %mul_11 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_1, 0.5), kwargs = {})
    %add_6 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%select_5, %mul_11), kwargs = {})
    %slice_7 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%view_57, 0, 0, 9223372036854775807), kwargs = {})
    %slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 1, 0, 9223372036854775807, 4), kwargs = {})
    %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_8, 1.0), kwargs = {})
    %slice_9 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%view_57, 0, 0, 9223372036854775807), kwargs = {})
    %slice_10 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_9, 1, 1, 9223372036854775807, 4), kwargs = {})
    %div_1 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_10, 1.0), kwargs = {})
    %slice_11 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%view_57, 0, 0, 9223372036854775807), kwargs = {})
    %slice_12 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_11, 1, 2, 9223372036854775807, 4), kwargs = {})
    %div_2 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_12, 1.0), kwargs = {})
    %slice_13 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%view_57, 0, 0, 9223372036854775807), kwargs = {})
    %slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 1, 3, 9223372036854775807, 4), kwargs = {})
    %div_3 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_14, 1.0), kwargs = {})
    %clamp : [num_users=1] = call_function[target=torch.ops.aten.clamp.default](args = (%div_2, None, 4.135166556742356), kwargs = {})
    %clamp_1 : [num_users=1] = call_function[target=torch.ops.aten.clamp.default](args = (%div_3, None, 4.135166556742356), kwargs = {})
    %slice_15 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%sub, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze_20 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_15, 1), kwargs = {})
    %mul_12 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div, %unsqueeze_20), kwargs = {})
    %slice_16 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%add_5, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze_21 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_16, 1), kwargs = {})
    %add_7 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_12, %unsqueeze_21), kwargs = {})
    %slice_17 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%sub_1, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze_22 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_17, 1), kwargs = {})
    %mul_13 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div_1, %unsqueeze_22), kwargs = {})
    %slice_18 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%add_6, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze_23 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_18, 1), kwargs = {})
    %add_8 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_13, %unsqueeze_23), kwargs = {})
    %exp : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%clamp,), kwargs = {})
    %slice_19 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%sub, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze_24 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_19, 1), kwargs = {})
    %mul_14 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%exp, %unsqueeze_24), kwargs = {})
    %exp_1 : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%clamp_1,), kwargs = {})
    %slice_20 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%sub_1, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze_25 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_20, 1), kwargs = {})
    %mul_15 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%exp_1, %unsqueeze_25), kwargs = {})
    %_tensor_constant0 : [num_users=1] = get_attr[target=_tensor_constant0]
    %lift_fresh_copy : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant0,), kwargs = {})
    %mul_16 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%lift_fresh_copy, %mul_15), kwargs = {})
    %_tensor_constant1 : [num_users=1] = get_attr[target=_tensor_constant1]
    %lift_fresh_copy_1 : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant1,), kwargs = {})
    %mul_17 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%lift_fresh_copy_1, %mul_14), kwargs = {})
    %sub_2 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_7, %mul_17), kwargs = {})
    %sub_3 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_8, %mul_16), kwargs = {})
    %add_9 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_7, %mul_17), kwargs = {})
    %add_10 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_8, %mul_16), kwargs = {})
    %unsqueeze_26 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%sub_2, 2), kwargs = {})
    %unsqueeze_27 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%sub_3, 2), kwargs = {})
    %unsqueeze_28 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%add_9, 2), kwargs = {})
    %unsqueeze_29 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%add_10, 2), kwargs = {})
    %cat_9 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze_26, %unsqueeze_27, %unsqueeze_28, %unsqueeze_29], 2), kwargs = {})
    %view_58 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat_9, [159882, 4]), kwargs = {})
    %view_59 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%view_58, [159882, -1, 4]), kwargs = {})
    %view_60 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%view_59, [1, -1, 4]), kwargs = {})
    %_param_constant6_1 : [num_users=1] = get_attr[target=_param_constant6]
    %_param_constant7_1 : [num_users=1] = get_attr[target=_param_constant7]
    %_param_constant8_1 : [num_users=1] = get_attr[target=_param_constant8]
    %_param_constant9_1 : [num_users=1] = get_attr[target=_param_constant9]
    %_param_constant10_1 : [num_users=1] = get_attr[target=_param_constant10]
    return (view_60, view_55, view_56, cat_5, _param_constant6_1, _param_constant7_1, _param_constant8_1, _param_constant9_1, _param_constant10_1)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone_4 from graph, since it is a clone node which is the only user of placeholder arg0_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone_3 from graph, since it is a clone node which is the only user of placeholder arg1_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone_2 from graph, since it is a clone node which is the only user of placeholder arg2_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone_1 from graph, since it is a clone node which is the only user of placeholder arg3_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone from graph, since it is a clone node which is the only user of placeholder arg4_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removed auxiliary clone nodes for placeholders:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %arg2_1 : [num_users=1] = placeholder[target=arg2_1]
    %arg3_1 : [num_users=1] = placeholder[target=arg3_1]
    %arg4_1 : [num_users=1] = placeholder[target=arg4_1]
    %_param_constant0 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg0_1, %_param_constant0, %_param_constant1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})
    %_param_constant2 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_1 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu, %_param_constant2, %_param_constant3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_2 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu, %_param_constant4, %_param_constant5, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_1 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_1 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_3 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg1_1, %_param_constant0_1, %_param_constant1_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_3,), kwargs = {})
    %_param_constant2_1 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_1 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_4 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_1, %_param_constant2_1, %_param_constant3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_1 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_1 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_5 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_1, %_param_constant4_1, %_param_constant5_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_2 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_2 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_6 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg2_1, %_param_constant0_2, %_param_constant1_2, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_2 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_6,), kwargs = {})
    %_param_constant2_2 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_2 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_7 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_2, %_param_constant2_2, %_param_constant3_2, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_2 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_2 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_8 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_2, %_param_constant4_2, %_param_constant5_2, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_3 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_3 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_9 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg3_1, %_param_constant0_3, %_param_constant1_3, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_3 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_9,), kwargs = {})
    %_param_constant2_3 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_3 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_10 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_3, %_param_constant2_3, %_param_constant3_3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_3 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_3 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_11 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_3, %_param_constant4_3, %_param_constant5_3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_4 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_4 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_12 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg4_1, %_param_constant0_4, %_param_constant1_4, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_4 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_12,), kwargs = {})
    %_param_constant2_4 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_4 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_13 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_4, %_param_constant2_4, %_param_constant3_4, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_4 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_4 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_14 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_4, %_param_constant4_4, %_param_constant5_4, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %empty : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty, 4), kwargs = {pin_memory: False})
    %empty_1 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_1 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_1, 4), kwargs = {pin_memory: False})
    %empty_2 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_2 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_2, 8), kwargs = {pin_memory: False})
    %empty_3 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_3 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_3, 8), kwargs = {pin_memory: False})
    %empty_4 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_4 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_4, 16), kwargs = {pin_memory: False})
    %empty_5 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_5 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_5, 16), kwargs = {pin_memory: False})
    %empty_6 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_6 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_6, 32), kwargs = {pin_memory: False})
    %empty_7 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_7 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_7, 32), kwargs = {pin_memory: False})
    %empty_8 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_8 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_8, 61), kwargs = {pin_memory: False})
    %empty_9 : [num_users=1] = call_function[target=torch.ops.aten.empty.memory_format](args = ([],), kwargs = {dtype: torch.int64, device: cuda:0, pin_memory: False})
    %full_like_9 : [num_users=1] = call_function[target=torch.ops.aten.full_like.default](args = (%empty_9, 61), kwargs = {pin_memory: False})
    %arange : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 200), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange, %full_like_1), kwargs = {})
    %arange_1 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 200), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_1, %full_like), kwargs = {})
    %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_1, [-1, 1]), kwargs = {})
    %expand : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view, [200, 200]), kwargs = {})
    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul, [1, -1]), kwargs = {})
    %expand_1 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_1, [200, 200]), kwargs = {})
    %clone_5 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_1,), kwargs = {memory_format: torch.contiguous_format})
    %view_2 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_5, [40000]), kwargs = {})
    %clone_6 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand,), kwargs = {memory_format: torch.contiguous_format})
    %view_3 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_6, [40000]), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_2, 1), kwargs = {})
    %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_3, 1), kwargs = {})
    %unsqueeze_2 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_2, 1), kwargs = {})
    %unsqueeze_3 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_3, 1), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze, %unsqueeze_1, %unsqueeze_2, %unsqueeze_3], 1), kwargs = {})
    %view_4 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat, [-1, 1, 4]), kwargs = {})
    %_param_constant6 : [num_users=1] = get_attr[target=_param_constant6]
    %view_5 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%_param_constant6, [1, -1, 4]), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_4, %view_5), kwargs = {})
    %view_6 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%add, [-1, 4]), kwargs = {})
    %arange_2 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 100), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_2, %full_like_3), kwargs = {})
    %arange_3 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 100), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_3 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_3, %full_like_2), kwargs = {})
    %view_7 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_3, [-1, 1]), kwargs = {})
    %expand_2 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_7, [100, 100]), kwargs = {})
    %view_8 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_2, [1, -1]), kwargs = {})
    %expand_3 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_8, [100, 100]), kwargs = {})
    %clone_7 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_3,), kwargs = {memory_format: torch.contiguous_format})
    %view_9 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_7, [10000]), kwargs = {})
    %clone_8 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_2,), kwargs = {memory_format: torch.contiguous_format})
    %view_10 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_8, [10000]), kwargs = {})
    %unsqueeze_4 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_9, 1), kwargs = {})
    %unsqueeze_5 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_10, 1), kwargs = {})
    %unsqueeze_6 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_9, 1), kwargs = {})
    %unsqueeze_7 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_10, 1), kwargs = {})
    %cat_1 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze_4, %unsqueeze_5, %unsqueeze_6, %unsqueeze_7], 1), kwargs = {})
    %view_11 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat_1, [-1, 1, 4]), kwargs = {})
    %_param_constant7 : [num_users=1] = get_attr[target=_param_constant7]
    %view_12 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%_param_constant7, [1, -1, 4]), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_11, %view_12), kwargs = {})
    %view_13 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%add_1, [-1, 4]), kwargs = {})
    %arange_4 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 50), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_4 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_4, %full_like_5), kwargs = {})
    %arange_5 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 50), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_5 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_5, %full_like_4), kwargs = {})
    %view_14 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_5, [-1, 1]), kwargs = {})
    %expand_4 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_14, [50, 50]), kwargs = {})
    %view_15 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_4, [1, -1]), kwargs = {})
    %expand_5 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_15, [50, 50]), kwargs = {})
    %clone_9 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_5,), kwargs = {memory_format: torch.contiguous_format})
    %view_16 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_9, [2500]), kwargs = {})
    %clone_10 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_4,), kwargs = {memory_format: torch.contiguous_format})
    %view_17 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_10, [2500]), kwargs = {})
    %unsqueeze_8 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_16, 1), kwargs = {})
    %unsqueeze_9 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_17, 1), kwargs = {})
    %unsqueeze_10 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_16, 1), kwargs = {})
    %unsqueeze_11 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_17, 1), kwargs = {})
    %cat_2 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze_8, %unsqueeze_9, %unsqueeze_10, %unsqueeze_11], 1), kwargs = {})
    %view_18 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat_2, [-1, 1, 4]), kwargs = {})
    %_param_constant8 : [num_users=1] = get_attr[target=_param_constant8]
    %view_19 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%_param_constant8, [1, -1, 4]), kwargs = {})
    %add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_18, %view_19), kwargs = {})
    %view_20 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%add_2, [-1, 4]), kwargs = {})
    %arange_6 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 25), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_6 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_6, %full_like_7), kwargs = {})
    %arange_7 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 25), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_7 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_7, %full_like_6), kwargs = {})
    %view_21 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_7, [-1, 1]), kwargs = {})
    %expand_6 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_21, [25, 25]), kwargs = {})
    %view_22 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_6, [1, -1]), kwargs = {})
    %expand_7 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_22, [25, 25]), kwargs = {})
    %clone_11 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_7,), kwargs = {memory_format: torch.contiguous_format})
    %view_23 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_11, [625]), kwargs = {})
    %clone_12 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_6,), kwargs = {memory_format: torch.contiguous_format})
    %view_24 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_12, [625]), kwargs = {})
    %unsqueeze_12 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_23, 1), kwargs = {})
    %unsqueeze_13 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_24, 1), kwargs = {})
    %unsqueeze_14 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_23, 1), kwargs = {})
    %unsqueeze_15 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_24, 1), kwargs = {})
    %cat_3 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze_12, %unsqueeze_13, %unsqueeze_14, %unsqueeze_15], 1), kwargs = {})
    %view_25 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat_3, [-1, 1, 4]), kwargs = {})
    %_param_constant9 : [num_users=1] = get_attr[target=_param_constant9]
    %view_26 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%_param_constant9, [1, -1, 4]), kwargs = {})
    %add_3 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_25, %view_26), kwargs = {})
    %view_27 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%add_3, [-1, 4]), kwargs = {})
    %arange_8 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 13), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_8 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_8, %full_like_9), kwargs = {})
    %arange_9 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_step](args = (0, 13), kwargs = {dtype: torch.int32, layout: torch.strided, device: cuda:0, pin_memory: False})
    %mul_9 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange_9, %full_like_8), kwargs = {})
    %view_28 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_9, [-1, 1]), kwargs = {})
    %expand_8 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_28, [13, 13]), kwargs = {})
    %view_29 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mul_8, [1, -1]), kwargs = {})
    %expand_9 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_29, [13, 13]), kwargs = {})
    %clone_13 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_9,), kwargs = {memory_format: torch.contiguous_format})
    %view_30 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_13, [169]), kwargs = {})
    %clone_14 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%expand_8,), kwargs = {memory_format: torch.contiguous_format})
    %view_31 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%clone_14, [169]), kwargs = {})
    %unsqueeze_16 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_30, 1), kwargs = {})
    %unsqueeze_17 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_31, 1), kwargs = {})
    %unsqueeze_18 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_30, 1), kwargs = {})
    %unsqueeze_19 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_31, 1), kwargs = {})
    %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze_16, %unsqueeze_17, %unsqueeze_18, %unsqueeze_19], 1), kwargs = {})
    %view_32 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat_4, [-1, 1, 4]), kwargs = {})
    %_param_constant10 : [num_users=1] = get_attr[target=_param_constant10]
    %view_33 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%_param_constant10, [1, -1, 4]), kwargs = {})
    %add_4 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_32, %view_33), kwargs = {})
    %view_34 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%add_4, [-1, 4]), kwargs = {})
    %cat_5 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%view_6, %view_13, %view_20, %view_27, %view_34],), kwargs = {})
    %view_35 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_1, [1, -1, 1, 200, 200]), kwargs = {})
    %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_35, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_15 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute,), kwargs = {memory_format: torch.contiguous_format})
    %view_36 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_15, [1, 120000, 1]), kwargs = {})
    %view_37 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_2, [1, -1, 4, 200, 200]), kwargs = {})
    %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_37, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_16 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_1,), kwargs = {memory_format: torch.contiguous_format})
    %view_38 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_16, [1, 120000, 4]), kwargs = {})
    %view_39 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_4, [1, -1, 1, 100, 100]), kwargs = {})
    %permute_2 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_39, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_17 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_2,), kwargs = {memory_format: torch.contiguous_format})
    %view_40 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_17, [1, 30000, 1]), kwargs = {})
    %view_41 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_5, [1, -1, 4, 100, 100]), kwargs = {})
    %permute_3 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_41, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_18 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_3,), kwargs = {memory_format: torch.contiguous_format})
    %view_42 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_18, [1, 30000, 4]), kwargs = {})
    %view_43 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_7, [1, -1, 1, 50, 50]), kwargs = {})
    %permute_4 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_43, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_19 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_4,), kwargs = {memory_format: torch.contiguous_format})
    %view_44 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_19, [1, 7500, 1]), kwargs = {})
    %view_45 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_8, [1, -1, 4, 50, 50]), kwargs = {})
    %permute_5 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_45, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_20 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_5,), kwargs = {memory_format: torch.contiguous_format})
    %view_46 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_20, [1, 7500, 4]), kwargs = {})
    %view_47 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_10, [1, -1, 1, 25, 25]), kwargs = {})
    %permute_6 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_47, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_21 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_6,), kwargs = {memory_format: torch.contiguous_format})
    %view_48 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_21, [1, 1875, 1]), kwargs = {})
    %view_49 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_11, [1, -1, 4, 25, 25]), kwargs = {})
    %permute_7 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_49, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_22 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_7,), kwargs = {memory_format: torch.contiguous_format})
    %view_50 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_22, [1, 1875, 4]), kwargs = {})
    %view_51 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_13, [1, -1, 1, 13, 13]), kwargs = {})
    %permute_8 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_51, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_23 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_8,), kwargs = {memory_format: torch.contiguous_format})
    %view_52 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_23, [1, 507, 1]), kwargs = {})
    %view_53 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_14, [1, -1, 4, 13, 13]), kwargs = {})
    %permute_9 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_53, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_24 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_9,), kwargs = {memory_format: torch.contiguous_format})
    %view_54 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_24, [1, 507, 4]), kwargs = {})
    %cat_6 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%view_36, %view_40, %view_44, %view_48, %view_52], 1), kwargs = {})
    %view_55 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat_6, [159882, 1]), kwargs = {})
    %cat_7 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%view_38, %view_42, %view_46, %view_50, %view_54], 1), kwargs = {})
    %view_56 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%cat_7, [-1, 4]), kwargs = {})
    %cat_8 : [num_users=6] = call_function[target=torch.ops.aten.cat.default](args = ([%cat_5],), kwargs = {})
    %view_57 : [num_users=4] = call_function[target=torch.ops.aten.view.default](args = (%view_56, [159882, -1]), kwargs = {})
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%cat_8, 0, 0, 9223372036854775807), kwargs = {})
    %select : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%slice_1, 1, 2), kwargs = {})
    %slice_2 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%cat_8, 0, 0, 9223372036854775807), kwargs = {})
    %select_1 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%slice_2, 1, 0), kwargs = {})
    %sub : [num_users=3] = call_function[target=torch.ops.aten.sub.Tensor](args = (%select, %select_1), kwargs = {})
    %slice_3 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%cat_8, 0, 0, 9223372036854775807), kwargs = {})
    %select_2 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%slice_3, 1, 3), kwargs = {})
    %slice_4 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%cat_8, 0, 0, 9223372036854775807), kwargs = {})
    %select_3 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%slice_4, 1, 1), kwargs = {})
    %sub_1 : [num_users=3] = call_function[target=torch.ops.aten.sub.Tensor](args = (%select_2, %select_3), kwargs = {})
    %slice_5 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%cat_8, 0, 0, 9223372036854775807), kwargs = {})
    %select_4 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%slice_5, 1, 0), kwargs = {})
    %mul_10 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub, 0.5), kwargs = {})
    %add_5 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%select_4, %mul_10), kwargs = {})
    %slice_6 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%cat_8, 0, 0, 9223372036854775807), kwargs = {})
    %select_5 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%slice_6, 1, 1), kwargs = {})
    %mul_11 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_1, 0.5), kwargs = {})
    %add_6 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%select_5, %mul_11), kwargs = {})
    %slice_7 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%view_57, 0, 0, 9223372036854775807), kwargs = {})
    %slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 1, 0, 9223372036854775807, 4), kwargs = {})
    %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_8, 1.0), kwargs = {})
    %slice_9 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%view_57, 0, 0, 9223372036854775807), kwargs = {})
    %slice_10 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_9, 1, 1, 9223372036854775807, 4), kwargs = {})
    %div_1 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_10, 1.0), kwargs = {})
    %slice_11 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%view_57, 0, 0, 9223372036854775807), kwargs = {})
    %slice_12 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_11, 1, 2, 9223372036854775807, 4), kwargs = {})
    %div_2 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_12, 1.0), kwargs = {})
    %slice_13 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%view_57, 0, 0, 9223372036854775807), kwargs = {})
    %slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 1, 3, 9223372036854775807, 4), kwargs = {})
    %div_3 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_14, 1.0), kwargs = {})
    %clamp : [num_users=1] = call_function[target=torch.ops.aten.clamp.default](args = (%div_2, None, 4.135166556742356), kwargs = {})
    %clamp_1 : [num_users=1] = call_function[target=torch.ops.aten.clamp.default](args = (%div_3, None, 4.135166556742356), kwargs = {})
    %slice_15 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%sub, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze_20 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_15, 1), kwargs = {})
    %mul_12 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div, %unsqueeze_20), kwargs = {})
    %slice_16 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%add_5, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze_21 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_16, 1), kwargs = {})
    %add_7 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_12, %unsqueeze_21), kwargs = {})
    %slice_17 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%sub_1, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze_22 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_17, 1), kwargs = {})
    %mul_13 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div_1, %unsqueeze_22), kwargs = {})
    %slice_18 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%add_6, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze_23 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_18, 1), kwargs = {})
    %add_8 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_13, %unsqueeze_23), kwargs = {})
    %exp : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%clamp,), kwargs = {})
    %slice_19 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%sub, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze_24 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_19, 1), kwargs = {})
    %mul_14 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%exp, %unsqueeze_24), kwargs = {})
    %exp_1 : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%clamp_1,), kwargs = {})
    %slice_20 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%sub_1, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze_25 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_20, 1), kwargs = {})
    %mul_15 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%exp_1, %unsqueeze_25), kwargs = {})
    %_tensor_constant0 : [num_users=1] = get_attr[target=_tensor_constant0]
    %lift_fresh_copy : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant0,), kwargs = {})
    %mul_16 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%lift_fresh_copy, %mul_15), kwargs = {})
    %_tensor_constant1 : [num_users=1] = get_attr[target=_tensor_constant1]
    %lift_fresh_copy_1 : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant1,), kwargs = {})
    %mul_17 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%lift_fresh_copy_1, %mul_14), kwargs = {})
    %sub_2 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_7, %mul_17), kwargs = {})
    %sub_3 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_8, %mul_16), kwargs = {})
    %add_9 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_7, %mul_17), kwargs = {})
    %add_10 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_8, %mul_16), kwargs = {})
    %unsqueeze_26 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%sub_2, 2), kwargs = {})
    %unsqueeze_27 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%sub_3, 2), kwargs = {})
    %unsqueeze_28 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%add_9, 2), kwargs = {})
    %unsqueeze_29 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%add_10, 2), kwargs = {})
    %cat_9 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze_26, %unsqueeze_27, %unsqueeze_28, %unsqueeze_29], 2), kwargs = {})
    %view_58 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat_9, [159882, 4]), kwargs = {})
    %view_59 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%view_58, [159882, -1, 4]), kwargs = {})
    %view_60 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%view_59, [1, -1, 4]), kwargs = {})
    %_param_constant6_1 : [num_users=1] = get_attr[target=_param_constant6]
    %_param_constant7_1 : [num_users=1] = get_attr[target=_param_constant7]
    %_param_constant8_1 : [num_users=1] = get_attr[target=_param_constant8]
    %_param_constant9_1 : [num_users=1] = get_attr[target=_param_constant9]
    %_param_constant10_1 : [num_users=1] = get_attr[target=_param_constant10]
    return (view_60, view_55, view_56, cat_5, _param_constant6_1, _param_constant7_1, _param_constant8_1, _param_constant9_1, _param_constant10_1)
DEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %arg2_1 : [num_users=1] = placeholder[target=arg2_1]
    %arg3_1 : [num_users=1] = placeholder[target=arg3_1]
    %arg4_1 : [num_users=1] = placeholder[target=arg4_1]
    %_param_constant0 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg0_1, %_param_constant0, %_param_constant1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})
    %_param_constant2 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_1 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu, %_param_constant2, %_param_constant3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_2 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu, %_param_constant4, %_param_constant5, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_1 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_1 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_3 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg1_1, %_param_constant0_1, %_param_constant1_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_3,), kwargs = {})
    %_param_constant2_1 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_1 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_4 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_1, %_param_constant2_1, %_param_constant3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_1 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_1 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_5 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_1, %_param_constant4_1, %_param_constant5_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_2 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_2 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_6 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg2_1, %_param_constant0_2, %_param_constant1_2, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_2 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_6,), kwargs = {})
    %_param_constant2_2 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_2 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_7 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_2, %_param_constant2_2, %_param_constant3_2, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_2 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_2 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_8 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_2, %_param_constant4_2, %_param_constant5_2, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_3 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_3 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_9 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg3_1, %_param_constant0_3, %_param_constant1_3, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_3 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_9,), kwargs = {})
    %_param_constant2_3 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_3 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_10 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_3, %_param_constant2_3, %_param_constant3_3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_3 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_3 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_11 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_3, %_param_constant4_3, %_param_constant5_3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_4 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_4 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_12 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg4_1, %_param_constant0_4, %_param_constant1_4, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_4 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_12,), kwargs = {})
    %_param_constant2_4 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_4 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_13 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_4, %_param_constant2_4, %_param_constant3_4, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_4 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_4 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_14 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_4, %_param_constant4_4, %_param_constant5_4, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]
    %view_35 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_1, [1, -1, 1, 200, 200]), kwargs = {})
    %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_35, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_15 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute,), kwargs = {memory_format: torch.contiguous_format})
    %view_36 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_15, [1, 120000, 1]), kwargs = {})
    %view_37 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_2, [1, -1, 4, 200, 200]), kwargs = {})
    %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_37, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_16 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_1,), kwargs = {memory_format: torch.contiguous_format})
    %view_38 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_16, [1, 120000, 4]), kwargs = {})
    %view_39 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_4, [1, -1, 1, 100, 100]), kwargs = {})
    %permute_2 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_39, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_17 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_2,), kwargs = {memory_format: torch.contiguous_format})
    %view_40 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_17, [1, 30000, 1]), kwargs = {})
    %view_41 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_5, [1, -1, 4, 100, 100]), kwargs = {})
    %permute_3 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_41, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_18 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_3,), kwargs = {memory_format: torch.contiguous_format})
    %view_42 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_18, [1, 30000, 4]), kwargs = {})
    %view_43 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_7, [1, -1, 1, 50, 50]), kwargs = {})
    %permute_4 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_43, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_19 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_4,), kwargs = {memory_format: torch.contiguous_format})
    %view_44 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_19, [1, 7500, 1]), kwargs = {})
    %view_45 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_8, [1, -1, 4, 50, 50]), kwargs = {})
    %permute_5 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_45, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_20 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_5,), kwargs = {memory_format: torch.contiguous_format})
    %view_46 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_20, [1, 7500, 4]), kwargs = {})
    %view_47 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_10, [1, -1, 1, 25, 25]), kwargs = {})
    %permute_6 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_47, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_21 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_6,), kwargs = {memory_format: torch.contiguous_format})
    %view_48 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_21, [1, 1875, 1]), kwargs = {})
    %view_49 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_11, [1, -1, 4, 25, 25]), kwargs = {})
    %permute_7 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_49, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_22 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_7,), kwargs = {memory_format: torch.contiguous_format})
    %view_50 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_22, [1, 1875, 4]), kwargs = {})
    %view_51 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_13, [1, -1, 1, 13, 13]), kwargs = {})
    %permute_8 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_51, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_23 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_8,), kwargs = {memory_format: torch.contiguous_format})
    %view_52 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_23, [1, 507, 1]), kwargs = {})
    %view_53 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%convolution_14, [1, -1, 4, 13, 13]), kwargs = {})
    %permute_9 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_53, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_24 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_9,), kwargs = {memory_format: torch.contiguous_format})
    %view_54 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone_24, [1, 507, 4]), kwargs = {})
    %cat_6 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%view_36, %view_40, %view_44, %view_48, %view_52], 1), kwargs = {})
    %view_55 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat_6, [159882, 1]), kwargs = {})
    %cat_7 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%view_38, %view_42, %view_46, %view_50, %view_54], 1), kwargs = {})
    %view_56 : [num_users=2] = call_function[target=torch.ops.aten.view.default](args = (%cat_7, [-1, 4]), kwargs = {})
    %view_57 : [num_users=4] = call_function[target=torch.ops.aten.view.default](args = (%view_56, [159882, -1]), kwargs = {})
    %slice_7 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%view_57, 0, 0, 9223372036854775807), kwargs = {})
    %slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 1, 0, 9223372036854775807, 4), kwargs = {})
    %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_8, 1.0), kwargs = {})
    %slice_9 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%view_57, 0, 0, 9223372036854775807), kwargs = {})
    %slice_10 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_9, 1, 1, 9223372036854775807, 4), kwargs = {})
    %div_1 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_10, 1.0), kwargs = {})
    %slice_11 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%view_57, 0, 0, 9223372036854775807), kwargs = {})
    %slice_12 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_11, 1, 2, 9223372036854775807, 4), kwargs = {})
    %div_2 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_12, 1.0), kwargs = {})
    %slice_13 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%view_57, 0, 0, 9223372036854775807), kwargs = {})
    %slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 1, 3, 9223372036854775807, 4), kwargs = {})
    %div_3 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_14, 1.0), kwargs = {})
    %clamp : [num_users=1] = call_function[target=torch.ops.aten.clamp.default](args = (%div_2, None, 4.135166556742356), kwargs = {})
    %clamp_1 : [num_users=1] = call_function[target=torch.ops.aten.clamp.default](args = (%div_3, None, 4.135166556742356), kwargs = {})
    %_frozen_param1 : [num_users=1] = get_attr[target=_frozen_param1]
    %mul_12 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div, %_frozen_param1), kwargs = {})
    %_frozen_param2 : [num_users=1] = get_attr[target=_frozen_param2]
    %add_7 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_12, %_frozen_param2), kwargs = {})
    %_frozen_param3 : [num_users=1] = get_attr[target=_frozen_param3]
    %mul_13 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div_1, %_frozen_param3), kwargs = {})
    %_frozen_param4 : [num_users=1] = get_attr[target=_frozen_param4]
    %add_8 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_13, %_frozen_param4), kwargs = {})
    %exp : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%clamp,), kwargs = {})
    %_frozen_param5 : [num_users=1] = get_attr[target=_frozen_param5]
    %mul_14 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%exp, %_frozen_param5), kwargs = {})
    %exp_1 : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%clamp_1,), kwargs = {})
    %_frozen_param6 : [num_users=1] = get_attr[target=_frozen_param6]
    %mul_15 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%exp_1, %_frozen_param6), kwargs = {})
    %_frozen_param7 : [num_users=1] = get_attr[target=_frozen_param7]
    %mul_16 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_frozen_param7, %mul_15), kwargs = {})
    %_frozen_param8 : [num_users=1] = get_attr[target=_frozen_param8]
    %mul_17 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_frozen_param8, %mul_14), kwargs = {})
    %sub_2 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_7, %mul_17), kwargs = {})
    %sub_3 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_8, %mul_16), kwargs = {})
    %add_9 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_7, %mul_17), kwargs = {})
    %add_10 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_8, %mul_16), kwargs = {})
    %unsqueeze_26 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%sub_2, 2), kwargs = {})
    %unsqueeze_27 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%sub_3, 2), kwargs = {})
    %unsqueeze_28 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%add_9, 2), kwargs = {})
    %unsqueeze_29 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%add_10, 2), kwargs = {})
    %cat_9 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze_26, %unsqueeze_27, %unsqueeze_28, %unsqueeze_29], 2), kwargs = {})
    %view_58 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%cat_9, [159882, 4]), kwargs = {})
    %view_59 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%view_58, [159882, -1, 4]), kwargs = {})
    %view_60 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%view_59, [1, -1, 4]), kwargs = {})
    %_param_constant6_1 : [num_users=1] = get_attr[target=_param_constant6]
    %_param_constant7_1 : [num_users=1] = get_attr[target=_param_constant7]
    %_param_constant8_1 : [num_users=1] = get_attr[target=_param_constant8]
    %_param_constant9_1 : [num_users=1] = get_attr[target=_param_constant9]
    %_param_constant10_1 : [num_users=1] = get_attr[target=_param_constant10]
    return (view_60, view_55, view_56, _frozen_param0, _param_constant6_1, _param_constant7_1, _param_constant8_1, _param_constant9_1, _param_constant10_1)
DEBUG:torch_tensorrt.dynamo.lowering.passes.view_to_reshape:Graph after replacing view with reshape:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %arg2_1 : [num_users=1] = placeholder[target=arg2_1]
    %arg3_1 : [num_users=1] = placeholder[target=arg3_1]
    %arg4_1 : [num_users=1] = placeholder[target=arg4_1]
    %_param_constant0 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg0_1, %_param_constant0, %_param_constant1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})
    %_param_constant2 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_1 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu, %_param_constant2, %_param_constant3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_2 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu, %_param_constant4, %_param_constant5, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_1 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_1 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_3 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg1_1, %_param_constant0_1, %_param_constant1_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_3,), kwargs = {})
    %_param_constant2_1 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_1 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_4 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_1, %_param_constant2_1, %_param_constant3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_1 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_1 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_5 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_1, %_param_constant4_1, %_param_constant5_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_2 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_2 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_6 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg2_1, %_param_constant0_2, %_param_constant1_2, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_2 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_6,), kwargs = {})
    %_param_constant2_2 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_2 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_7 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_2, %_param_constant2_2, %_param_constant3_2, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_2 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_2 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_8 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_2, %_param_constant4_2, %_param_constant5_2, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_3 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_3 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_9 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg3_1, %_param_constant0_3, %_param_constant1_3, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_3 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_9,), kwargs = {})
    %_param_constant2_3 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_3 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_10 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_3, %_param_constant2_3, %_param_constant3_3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_3 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_3 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_11 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_3, %_param_constant4_3, %_param_constant5_3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_4 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_4 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_12 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg4_1, %_param_constant0_4, %_param_constant1_4, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_4 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_12,), kwargs = {})
    %_param_constant2_4 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_4 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_13 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_4, %_param_constant2_4, %_param_constant3_4, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_4 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_4 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_14 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_4, %_param_constant4_4, %_param_constant5_4, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]
    %reshape_default : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_1, [1, -1, 1, 200, 200]), kwargs = {})
    %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_15 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_2, [1, -1, 4, 200, 200]), kwargs = {})
    %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_2, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_16 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_1,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_4 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_4, [1, -1, 1, 100, 100]), kwargs = {})
    %permute_2 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_4, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_17 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_2,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_6 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_5, [1, -1, 4, 100, 100]), kwargs = {})
    %permute_3 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_6, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_18 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_3,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_8 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_7, [1, -1, 1, 50, 50]), kwargs = {})
    %permute_4 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_8, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_19 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_4,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_10 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_8, [1, -1, 4, 50, 50]), kwargs = {})
    %permute_5 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_10, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_20 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_5,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_12 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_10, [1, -1, 1, 25, 25]), kwargs = {})
    %permute_6 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_12, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_21 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_6,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_14 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_11, [1, -1, 4, 25, 25]), kwargs = {})
    %permute_7 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_14, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_22 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_7,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_16 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_13, [1, -1, 1, 13, 13]), kwargs = {})
    %permute_8 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_16, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_23 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_8,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_18 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_14, [1, -1, 4, 13, 13]), kwargs = {})
    %permute_9 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_18, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_24 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_9,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_15, [1, 120000, 1]), kwargs = {})
    %reshape_default_5 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_17, [1, 30000, 1]), kwargs = {})
    %reshape_default_9 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_19, [1, 7500, 1]), kwargs = {})
    %reshape_default_13 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_21, [1, 1875, 1]), kwargs = {})
    %reshape_default_17 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_23, [1, 507, 1]), kwargs = {})
    %cat_6 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%reshape_default_1, %reshape_default_5, %reshape_default_9, %reshape_default_13, %reshape_default_17], 1), kwargs = {})
    %reshape_default_3 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_16, [1, 120000, 4]), kwargs = {})
    %reshape_default_7 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_18, [1, 30000, 4]), kwargs = {})
    %reshape_default_11 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_20, [1, 7500, 4]), kwargs = {})
    %reshape_default_15 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_22, [1, 1875, 4]), kwargs = {})
    %reshape_default_19 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_24, [1, 507, 4]), kwargs = {})
    %cat_7 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%reshape_default_3, %reshape_default_7, %reshape_default_11, %reshape_default_15, %reshape_default_19], 1), kwargs = {})
    %reshape_default_21 : [num_users=2] = call_function[target=torch.ops.aten.reshape.default](args = (%cat_7, [-1, 4]), kwargs = {})
    %reshape_default_22 : [num_users=4] = call_function[target=torch.ops.aten.reshape.default](args = (%reshape_default_21, [159882, -1]), kwargs = {})
    %slice_7 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%reshape_default_22, 0, 0, 9223372036854775807), kwargs = {})
    %slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 1, 0, 9223372036854775807, 4), kwargs = {})
    %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_8, 1.0), kwargs = {})
    %slice_9 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%reshape_default_22, 0, 0, 9223372036854775807), kwargs = {})
    %slice_10 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_9, 1, 1, 9223372036854775807, 4), kwargs = {})
    %div_1 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_10, 1.0), kwargs = {})
    %slice_11 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%reshape_default_22, 0, 0, 9223372036854775807), kwargs = {})
    %slice_12 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_11, 1, 2, 9223372036854775807, 4), kwargs = {})
    %div_2 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_12, 1.0), kwargs = {})
    %slice_13 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%reshape_default_22, 0, 0, 9223372036854775807), kwargs = {})
    %slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 1, 3, 9223372036854775807, 4), kwargs = {})
    %div_3 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_14, 1.0), kwargs = {})
    %clamp : [num_users=1] = call_function[target=torch.ops.aten.clamp.default](args = (%div_2, None, 4.135166556742356), kwargs = {})
    %clamp_1 : [num_users=1] = call_function[target=torch.ops.aten.clamp.default](args = (%div_3, None, 4.135166556742356), kwargs = {})
    %_frozen_param1 : [num_users=1] = get_attr[target=_frozen_param1]
    %mul_12 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div, %_frozen_param1), kwargs = {})
    %_frozen_param2 : [num_users=1] = get_attr[target=_frozen_param2]
    %add_7 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_12, %_frozen_param2), kwargs = {})
    %_frozen_param3 : [num_users=1] = get_attr[target=_frozen_param3]
    %mul_13 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div_1, %_frozen_param3), kwargs = {})
    %_frozen_param4 : [num_users=1] = get_attr[target=_frozen_param4]
    %add_8 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_13, %_frozen_param4), kwargs = {})
    %exp : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%clamp,), kwargs = {})
    %_frozen_param5 : [num_users=1] = get_attr[target=_frozen_param5]
    %mul_14 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%exp, %_frozen_param5), kwargs = {})
    %exp_1 : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%clamp_1,), kwargs = {})
    %_frozen_param6 : [num_users=1] = get_attr[target=_frozen_param6]
    %mul_15 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%exp_1, %_frozen_param6), kwargs = {})
    %_frozen_param7 : [num_users=1] = get_attr[target=_frozen_param7]
    %mul_16 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_frozen_param7, %mul_15), kwargs = {})
    %_frozen_param8 : [num_users=1] = get_attr[target=_frozen_param8]
    %mul_17 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_frozen_param8, %mul_14), kwargs = {})
    %sub_2 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_7, %mul_17), kwargs = {})
    %sub_3 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_8, %mul_16), kwargs = {})
    %add_9 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_7, %mul_17), kwargs = {})
    %add_10 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_8, %mul_16), kwargs = {})
    %unsqueeze_26 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%sub_2, 2), kwargs = {})
    %unsqueeze_27 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%sub_3, 2), kwargs = {})
    %unsqueeze_28 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%add_9, 2), kwargs = {})
    %unsqueeze_29 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%add_10, 2), kwargs = {})
    %cat_9 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze_26, %unsqueeze_27, %unsqueeze_28, %unsqueeze_29], 2), kwargs = {})
    %reshape_default_23 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%cat_9, [159882, 4]), kwargs = {})
    %reshape_default_24 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%reshape_default_23, [159882, -1, 4]), kwargs = {})
    %_param_constant6_1 : [num_users=1] = get_attr[target=_param_constant6]
    %_param_constant7_1 : [num_users=1] = get_attr[target=_param_constant7]
    %_param_constant8_1 : [num_users=1] = get_attr[target=_param_constant8]
    %_param_constant9_1 : [num_users=1] = get_attr[target=_param_constant9]
    %_param_constant10_1 : [num_users=1] = get_attr[target=_param_constant10]
    %reshape_default_20 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%cat_6, [159882, 1]), kwargs = {})
    %reshape_default_25 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%reshape_default_24, [1, -1, 4]), kwargs = {})
    return (reshape_default_25, reshape_default_20, reshape_default_21, _frozen_param0, _param_constant6_1, _param_constant7_1, _param_constant8_1, _param_constant9_1, _param_constant10_1)
DEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:
 graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %arg2_1 : [num_users=1] = placeholder[target=arg2_1]
    %arg3_1 : [num_users=1] = placeholder[target=arg3_1]
    %arg4_1 : [num_users=1] = placeholder[target=arg4_1]
    %_param_constant0 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg0_1, %_param_constant0, %_param_constant1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})
    %_param_constant2 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_1 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu, %_param_constant2, %_param_constant3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_2 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu, %_param_constant4, %_param_constant5, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_1 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_1 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_3 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg1_1, %_param_constant0_1, %_param_constant1_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_3,), kwargs = {})
    %_param_constant2_1 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_1 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_4 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_1, %_param_constant2_1, %_param_constant3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_1 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_1 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_5 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_1, %_param_constant4_1, %_param_constant5_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_2 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_2 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_6 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg2_1, %_param_constant0_2, %_param_constant1_2, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_2 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_6,), kwargs = {})
    %_param_constant2_2 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_2 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_7 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_2, %_param_constant2_2, %_param_constant3_2, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_2 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_2 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_8 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_2, %_param_constant4_2, %_param_constant5_2, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_3 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_3 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_9 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg3_1, %_param_constant0_3, %_param_constant1_3, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_3 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_9,), kwargs = {})
    %_param_constant2_3 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_3 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_10 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_3, %_param_constant2_3, %_param_constant3_3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_3 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_3 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_11 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_3, %_param_constant4_3, %_param_constant5_3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant0_4 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_4 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_12 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg4_1, %_param_constant0_4, %_param_constant1_4, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_4 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_12,), kwargs = {})
    %_param_constant2_4 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_4 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_13 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_4, %_param_constant2_4, %_param_constant3_4, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_4 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_4 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_14 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_4, %_param_constant4_4, %_param_constant5_4, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]
    %reshape_default : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_1, [1, -1, 1, 200, 200]), kwargs = {})
    %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_15 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_2, [1, -1, 4, 200, 200]), kwargs = {})
    %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_2, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_16 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_1,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_4 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_4, [1, -1, 1, 100, 100]), kwargs = {})
    %permute_2 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_4, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_17 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_2,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_6 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_5, [1, -1, 4, 100, 100]), kwargs = {})
    %permute_3 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_6, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_18 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_3,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_8 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_7, [1, -1, 1, 50, 50]), kwargs = {})
    %permute_4 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_8, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_19 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_4,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_10 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_8, [1, -1, 4, 50, 50]), kwargs = {})
    %permute_5 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_10, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_20 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_5,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_12 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_10, [1, -1, 1, 25, 25]), kwargs = {})
    %permute_6 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_12, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_21 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_6,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_14 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_11, [1, -1, 4, 25, 25]), kwargs = {})
    %permute_7 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_14, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_22 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_7,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_16 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_13, [1, -1, 1, 13, 13]), kwargs = {})
    %permute_8 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_16, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_23 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_8,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_18 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_14, [1, -1, 4, 13, 13]), kwargs = {})
    %permute_9 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_18, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_24 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_9,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_15, [1, 120000, 1]), kwargs = {})
    %reshape_default_5 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_17, [1, 30000, 1]), kwargs = {})
    %reshape_default_9 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_19, [1, 7500, 1]), kwargs = {})
    %reshape_default_13 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_21, [1, 1875, 1]), kwargs = {})
    %reshape_default_17 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_23, [1, 507, 1]), kwargs = {})
    %cat_6 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%reshape_default_1, %reshape_default_5, %reshape_default_9, %reshape_default_13, %reshape_default_17], 1), kwargs = {})
    %reshape_default_3 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_16, [1, 120000, 4]), kwargs = {})
    %reshape_default_7 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_18, [1, 30000, 4]), kwargs = {})
    %reshape_default_11 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_20, [1, 7500, 4]), kwargs = {})
    %reshape_default_15 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_22, [1, 1875, 4]), kwargs = {})
    %reshape_default_19 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_24, [1, 507, 4]), kwargs = {})
    %cat_7 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%reshape_default_3, %reshape_default_7, %reshape_default_11, %reshape_default_15, %reshape_default_19], 1), kwargs = {})
    %reshape_default_21 : [num_users=2] = call_function[target=torch.ops.aten.reshape.default](args = (%cat_7, [-1, 4]), kwargs = {})
    %reshape_default_22 : [num_users=4] = call_function[target=torch.ops.aten.reshape.default](args = (%reshape_default_21, [159882, -1]), kwargs = {})
    %slice_7 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%reshape_default_22, 0, 0, 9223372036854775807), kwargs = {})
    %slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 1, 0, 9223372036854775807, 4), kwargs = {})
    %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_8, 1.0), kwargs = {})
    %slice_9 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%reshape_default_22, 0, 0, 9223372036854775807), kwargs = {})
    %slice_10 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_9, 1, 1, 9223372036854775807, 4), kwargs = {})
    %div_1 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_10, 1.0), kwargs = {})
    %slice_11 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%reshape_default_22, 0, 0, 9223372036854775807), kwargs = {})
    %slice_12 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_11, 1, 2, 9223372036854775807, 4), kwargs = {})
    %div_2 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_12, 1.0), kwargs = {})
    %slice_13 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%reshape_default_22, 0, 0, 9223372036854775807), kwargs = {})
    %slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 1, 3, 9223372036854775807, 4), kwargs = {})
    %div_3 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_14, 1.0), kwargs = {})
    %clamp : [num_users=1] = call_function[target=torch.ops.aten.clamp.default](args = (%div_2, None, 4.135166556742356), kwargs = {})
    %clamp_1 : [num_users=1] = call_function[target=torch.ops.aten.clamp.default](args = (%div_3, None, 4.135166556742356), kwargs = {})
    %_frozen_param1 : [num_users=1] = get_attr[target=_frozen_param1]
    %mul_12 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div, %_frozen_param1), kwargs = {})
    %_frozen_param2 : [num_users=1] = get_attr[target=_frozen_param2]
    %add_7 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_12, %_frozen_param2), kwargs = {})
    %_frozen_param3 : [num_users=1] = get_attr[target=_frozen_param3]
    %mul_13 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div_1, %_frozen_param3), kwargs = {})
    %_frozen_param4 : [num_users=1] = get_attr[target=_frozen_param4]
    %add_8 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_13, %_frozen_param4), kwargs = {})
    %exp : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%clamp,), kwargs = {})
    %_frozen_param5 : [num_users=1] = get_attr[target=_frozen_param5]
    %mul_14 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%exp, %_frozen_param5), kwargs = {})
    %exp_1 : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%clamp_1,), kwargs = {})
    %_frozen_param6 : [num_users=1] = get_attr[target=_frozen_param6]
    %mul_15 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%exp_1, %_frozen_param6), kwargs = {})
    %_frozen_param7 : [num_users=1] = get_attr[target=_frozen_param7]
    %mul_16 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_frozen_param7, %mul_15), kwargs = {})
    %_frozen_param8 : [num_users=1] = get_attr[target=_frozen_param8]
    %mul_17 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_frozen_param8, %mul_14), kwargs = {})
    %sub_2 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_7, %mul_17), kwargs = {})
    %sub_3 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_8, %mul_16), kwargs = {})
    %add_9 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_7, %mul_17), kwargs = {})
    %add_10 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_8, %mul_16), kwargs = {})
    %unsqueeze_26 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%sub_2, 2), kwargs = {})
    %unsqueeze_27 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%sub_3, 2), kwargs = {})
    %unsqueeze_28 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%add_9, 2), kwargs = {})
    %unsqueeze_29 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%add_10, 2), kwargs = {})
    %cat_9 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze_26, %unsqueeze_27, %unsqueeze_28, %unsqueeze_29], 2), kwargs = {})
    %reshape_default_23 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%cat_9, [159882, 4]), kwargs = {})
    %reshape_default_24 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%reshape_default_23, [159882, -1, 4]), kwargs = {})
    %_param_constant6_1 : [num_users=1] = get_attr[target=_param_constant6]
    %_param_constant7_1 : [num_users=1] = get_attr[target=_param_constant7]
    %_param_constant8_1 : [num_users=1] = get_attr[target=_param_constant8]
    %_param_constant9_1 : [num_users=1] = get_attr[target=_param_constant9]
    %_param_constant10_1 : [num_users=1] = get_attr[target=_param_constant10]
    %reshape_default_20 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%cat_6, [159882, 1]), kwargs = {})
    %reshape_default_25 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%reshape_default_24, [1, -1, 4]), kwargs = {})
    return (reshape_default_25, reshape_default_20, reshape_default_21, _frozen_param0, _param_constant6_1, _param_constant7_1, _param_constant8_1, _param_constant9_1, _param_constant10_1)
DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Supported Nodes:
- torch.ops.aten.convolution.default + Operator Count: 15
- torch.ops.aten.relu.default + Operator Count: 5
- torch.ops.aten.reshape.default + Operator Count: 26
- torch.ops.aten.permute.default + Operator Count: 10
- torch.ops.aten.clone.default + Operator Count: 10
- torch.ops.aten.cat.default + Operator Count: 3
- torch.ops.aten.slice.Tensor + Operator Count: 8
- torch.ops.aten.div.Tensor + Operator Count: 4
- torch.ops.aten.clamp.default + Operator Count: 2
- torch.ops.aten.mul.Tensor + Operator Count: 6
- torch.ops.aten.add.Tensor + Operator Count: 4
- torch.ops.aten.exp.default + Operator Count: 2
- torch.ops.aten.sub.Tensor + Operator Count: 2
- torch.ops.aten.unsqueeze.default + Operator Count: 4

DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
All Nodes Supported

DEBUG:torch_tensorrt.dynamo._compiler:Detected support for 101 operators out of 101 in subgraph.
WARNING:torch_tensorrt.dynamo._compiler:Node _param_constant0 of op type get_attr does not have metadata. This could sometimes lead to undefined behavior.
WARNING:torch_tensorrt.dynamo._compiler:Some nodes do not have metadata (shape and dtype information). This could lead to problems sometimes if the graph has PyTorch and TensorRT segments.
DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
Number of TensorRT-Accelerated Engines Generated: 1
DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
Supported Nodes:
- torch.ops.aten.convolution.default + Operator Count: 15
- torch.ops.aten.relu.default + Operator Count: 5
- torch.ops.aten.reshape.default + Operator Count: 26
- torch.ops.aten.permute.default + Operator Count: 10
- torch.ops.aten.clone.default + Operator Count: 10
- torch.ops.aten.cat.default + Operator Count: 3
- torch.ops.aten.slice.Tensor + Operator Count: 8
- torch.ops.aten.div.Tensor + Operator Count: 4
- torch.ops.aten.clamp.default + Operator Count: 2
- torch.ops.aten.mul.Tensor + Operator Count: 6
- torch.ops.aten.add.Tensor + Operator Count: 4
- torch.ops.aten.exp.default + Operator Count: 2
- torch.ops.aten.sub.Tensor + Operator Count: 2
- torch.ops.aten.unsqueeze.default + Operator Count: 4

DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
All Nodes Supported

DEBUG:torch_tensorrt.dynamo._compiler:Submodule name: _run_on_acc_0
 Input shapes: [(1, 256, 200, 200), (1, 256, 100, 100), (1, 256, 50, 50), (1, 256, 25, 25), (1, 256, 13, 13)]
 graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %_param_constant0 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg0_1, %_param_constant0, %_param_constant1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution,), kwargs = {})
    %_param_constant2 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_1 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu, %_param_constant2, %_param_constant3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_2 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu, %_param_constant4, %_param_constant5, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %_param_constant0_1 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_1 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_3 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg1_1, %_param_constant0_1, %_param_constant1_1, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_3,), kwargs = {})
    %_param_constant2_1 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_1 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_4 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_1, %_param_constant2_1, %_param_constant3_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_1 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_1 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_5 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_1, %_param_constant4_1, %_param_constant5_1, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %arg2_1 : [num_users=1] = placeholder[target=arg2_1]
    %_param_constant0_2 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_2 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_6 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg2_1, %_param_constant0_2, %_param_constant1_2, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_2 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_6,), kwargs = {})
    %_param_constant2_2 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_2 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_7 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_2, %_param_constant2_2, %_param_constant3_2, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_2 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_2 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_8 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_2, %_param_constant4_2, %_param_constant5_2, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %arg3_1 : [num_users=1] = placeholder[target=arg3_1]
    %_param_constant0_3 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_3 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_9 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg3_1, %_param_constant0_3, %_param_constant1_3, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_3 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_9,), kwargs = {})
    %_param_constant2_3 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_3 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_10 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_3, %_param_constant2_3, %_param_constant3_3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_3 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_3 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_11 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_3, %_param_constant4_3, %_param_constant5_3, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %arg4_1 : [num_users=1] = placeholder[target=arg4_1]
    %_param_constant0_4 : [num_users=1] = get_attr[target=_param_constant0]
    %_param_constant1_4 : [num_users=1] = get_attr[target=_param_constant1]
    %convolution_12 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%arg4_1, %_param_constant0_4, %_param_constant1_4, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), kwargs = {})
    %relu_4 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%convolution_12,), kwargs = {})
    %_param_constant2_4 : [num_users=1] = get_attr[target=_param_constant2]
    %_param_constant3_4 : [num_users=1] = get_attr[target=_param_constant3]
    %convolution_13 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_4, %_param_constant2_4, %_param_constant3_4, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %_param_constant4_4 : [num_users=1] = get_attr[target=_param_constant4]
    %_param_constant5_4 : [num_users=1] = get_attr[target=_param_constant5]
    %convolution_14 : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%relu_4, %_param_constant4_4, %_param_constant5_4, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %reshape_default : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_1, [1, -1, 1, 200, 200]), kwargs = {})
    %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_15 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_2, [1, -1, 4, 200, 200]), kwargs = {})
    %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_2, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_16 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_1,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_4 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_4, [1, -1, 1, 100, 100]), kwargs = {})
    %permute_2 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_4, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_17 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_2,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_6 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_5, [1, -1, 4, 100, 100]), kwargs = {})
    %permute_3 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_6, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_18 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_3,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_8 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_7, [1, -1, 1, 50, 50]), kwargs = {})
    %permute_4 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_8, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_19 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_4,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_10 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_8, [1, -1, 4, 50, 50]), kwargs = {})
    %permute_5 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_10, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_20 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_5,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_12 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_10, [1, -1, 1, 25, 25]), kwargs = {})
    %permute_6 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_12, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_21 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_6,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_14 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_11, [1, -1, 4, 25, 25]), kwargs = {})
    %permute_7 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_14, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_22 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_7,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_16 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_13, [1, -1, 1, 13, 13]), kwargs = {})
    %permute_8 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_16, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_23 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_8,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_18 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%convolution_14, [1, -1, 4, 13, 13]), kwargs = {})
    %permute_9 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%reshape_default_18, [0, 3, 4, 1, 2]), kwargs = {})
    %clone_24 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_9,), kwargs = {memory_format: torch.contiguous_format})
    %reshape_default_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_15, [1, 120000, 1]), kwargs = {})
    %reshape_default_5 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_17, [1, 30000, 1]), kwargs = {})
    %reshape_default_9 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_19, [1, 7500, 1]), kwargs = {})
    %reshape_default_13 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_21, [1, 1875, 1]), kwargs = {})
    %reshape_default_17 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_23, [1, 507, 1]), kwargs = {})
    %cat_6 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%reshape_default_1, %reshape_default_5, %reshape_default_9, %reshape_default_13, %reshape_default_17], 1), kwargs = {})
    %reshape_default_3 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_16, [1, 120000, 4]), kwargs = {})
    %reshape_default_7 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_18, [1, 30000, 4]), kwargs = {})
    %reshape_default_11 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_20, [1, 7500, 4]), kwargs = {})
    %reshape_default_15 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_22, [1, 1875, 4]), kwargs = {})
    %reshape_default_19 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%clone_24, [1, 507, 4]), kwargs = {})
    %cat_7 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%reshape_default_3, %reshape_default_7, %reshape_default_11, %reshape_default_15, %reshape_default_19], 1), kwargs = {})
    %reshape_default_21 : [num_users=2] = call_function[target=torch.ops.aten.reshape.default](args = (%cat_7, [-1, 4]), kwargs = {})
    %reshape_default_22 : [num_users=4] = call_function[target=torch.ops.aten.reshape.default](args = (%reshape_default_21, [159882, -1]), kwargs = {})
    %slice_7 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%reshape_default_22, 0, 0, 9223372036854775807), kwargs = {})
    %slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 1, 0, 9223372036854775807, 4), kwargs = {})
    %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_8, 1.0), kwargs = {})
    %slice_9 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%reshape_default_22, 0, 0, 9223372036854775807), kwargs = {})
    %slice_10 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_9, 1, 1, 9223372036854775807, 4), kwargs = {})
    %div_1 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_10, 1.0), kwargs = {})
    %slice_11 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%reshape_default_22, 0, 0, 9223372036854775807), kwargs = {})
    %slice_12 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_11, 1, 2, 9223372036854775807, 4), kwargs = {})
    %div_2 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_12, 1.0), kwargs = {})
    %slice_13 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%reshape_default_22, 0, 0, 9223372036854775807), kwargs = {})
    %slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 1, 3, 9223372036854775807, 4), kwargs = {})
    %div_3 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%slice_14, 1.0), kwargs = {})
    %clamp : [num_users=1] = call_function[target=torch.ops.aten.clamp.default](args = (%div_2, None, 4.135166556742356), kwargs = {})
    %clamp_1 : [num_users=1] = call_function[target=torch.ops.aten.clamp.default](args = (%div_3, None, 4.135166556742356), kwargs = {})
    %_frozen_param1 : [num_users=1] = get_attr[target=_frozen_param1]
    %mul_12 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div, %_frozen_param1), kwargs = {})
    %_frozen_param2 : [num_users=1] = get_attr[target=_frozen_param2]
    %add_7 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_12, %_frozen_param2), kwargs = {})
    %_frozen_param3 : [num_users=1] = get_attr[target=_frozen_param3]
    %mul_13 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%div_1, %_frozen_param3), kwargs = {})
    %_frozen_param4 : [num_users=1] = get_attr[target=_frozen_param4]
    %add_8 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_13, %_frozen_param4), kwargs = {})
    %exp : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%clamp,), kwargs = {})
    %_frozen_param5 : [num_users=1] = get_attr[target=_frozen_param5]
    %mul_14 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%exp, %_frozen_param5), kwargs = {})
    %exp_1 : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%clamp_1,), kwargs = {})
    %_frozen_param6 : [num_users=1] = get_attr[target=_frozen_param6]
    %mul_15 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%exp_1, %_frozen_param6), kwargs = {})
    %_frozen_param7 : [num_users=1] = get_attr[target=_frozen_param7]
    %mul_16 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_frozen_param7, %mul_15), kwargs = {})
    %_frozen_param8 : [num_users=1] = get_attr[target=_frozen_param8]
    %mul_17 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_frozen_param8, %mul_14), kwargs = {})
    %sub_2 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_7, %mul_17), kwargs = {})
    %sub_3 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_8, %mul_16), kwargs = {})
    %add_9 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_7, %mul_17), kwargs = {})
    %add_10 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_8, %mul_16), kwargs = {})
    %unsqueeze_26 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%sub_2, 2), kwargs = {})
    %unsqueeze_27 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%sub_3, 2), kwargs = {})
    %unsqueeze_28 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%add_9, 2), kwargs = {})
    %unsqueeze_29 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%add_10, 2), kwargs = {})
    %cat_9 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze_26, %unsqueeze_27, %unsqueeze_28, %unsqueeze_29], 2), kwargs = {})
    %reshape_default_23 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%cat_9, [159882, 4]), kwargs = {})
    %reshape_default_24 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%reshape_default_23, [159882, -1, 4]), kwargs = {})
    %reshape_default_20 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%cat_6, [159882, 1]), kwargs = {})
    %reshape_default_25 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%reshape_default_24, [1, -1, 4]), kwargs = {})
    return (reshape_default_25, reshape_default_20, reshape_default_21)
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Adding input to in-progress INetwork: arg0_1 [shape=[1, 256, 200, 200], dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution (kind: aten.convolution.default, args: ('arg0_1 &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu (kind: aten.relu.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_1 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(3, 256, 1, 1), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(3,), dtype=float32]&gt;', [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_2 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu]_output &lt;tensorrt.ITensor [shape=(1, 256, 200, 200), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(12, 256, 1, 1), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(12,), dtype=float32]&gt;', [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Adding input to in-progress INetwork: arg1_1 [shape=[1, 256, 100, 100], dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_3 (kind: aten.convolution.default, args: ('arg1_1 &lt;tensorrt.ITensor [shape=(1, 256, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_1 (kind: aten.relu.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_3]_output &lt;tensorrt.ITensor [shape=(1, 256, 100, 100), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_4 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_1]_output &lt;tensorrt.ITensor [shape=(1, 256, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(3, 256, 1, 1), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(3,), dtype=float32]&gt;', [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_5 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_1]_output &lt;tensorrt.ITensor [shape=(1, 256, 100, 100), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(12, 256, 1, 1), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(12,), dtype=float32]&gt;', [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Adding input to in-progress INetwork: arg2_1 [shape=[1, 256, 50, 50], dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_6 (kind: aten.convolution.default, args: ('arg2_1 &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_2 (kind: aten.relu.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_6]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_7 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_2]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(3, 256, 1, 1), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(3,), dtype=float32]&gt;', [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_8 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_2]_output &lt;tensorrt.ITensor [shape=(1, 256, 50, 50), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(12, 256, 1, 1), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(12,), dtype=float32]&gt;', [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Adding input to in-progress INetwork: arg3_1 [shape=[1, 256, 25, 25], dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_9 (kind: aten.convolution.default, args: ('arg3_1 &lt;tensorrt.ITensor [shape=(1, 256, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_3 (kind: aten.relu.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_9]_output &lt;tensorrt.ITensor [shape=(1, 256, 25, 25), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_10 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_3]_output &lt;tensorrt.ITensor [shape=(1, 256, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(3, 256, 1, 1), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(3,), dtype=float32]&gt;', [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_11 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_3]_output &lt;tensorrt.ITensor [shape=(1, 256, 25, 25), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(12, 256, 1, 1), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(12,), dtype=float32]&gt;', [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Adding input to in-progress INetwork: arg4_1 [shape=[1, 256, 13, 13], dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_12 (kind: aten.convolution.default, args: ('arg4_1 &lt;tensorrt.ITensor [shape=(1, 256, 13, 13), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256, 256, 3, 3), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(256,), dtype=float32]&gt;', [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_4 (kind: aten.relu.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_12]_output &lt;tensorrt.ITensor [shape=(1, 256, 13, 13), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_13 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_4]_output &lt;tensorrt.ITensor [shape=(1, 256, 13, 13), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(3, 256, 1, 1), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(3,), dtype=float32]&gt;', [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node convolution_14 (kind: aten.convolution.default, args: ('[RELU]-[aten_ops.relu.default]-[relu_4]_output &lt;tensorrt.ITensor [shape=(1, 256, 13, 13), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(12, 256, 1, 1), dtype=float32]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(12,), dtype=float32]&gt;', [1, 1], [0, 0], [1, 1], False, [0, 0], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default (kind: aten.reshape.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_1]_output &lt;tensorrt.ITensor [shape=(1, 3, 200, 200), dtype=DataType.FLOAT]&gt;', [1, -1, 1, 200, 200]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node permute (kind: aten.permute.default, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default]_output &lt;tensorrt.ITensor [shape=(1, 3, 1, 200, 200), dtype=DataType.FLOAT]&gt;', [0, 3, 4, 1, 2]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node clone_15 (kind: aten.clone.default, args: ('[SHUFFLE]-[aten_ops.permute.default]-[permute]_output &lt;tensorrt.ITensor [shape=(1, 200, 200, 3, 1), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_2 (kind: aten.reshape.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_2]_output &lt;tensorrt.ITensor [shape=(1, 12, 200, 200), dtype=DataType.FLOAT]&gt;', [1, -1, 4, 200, 200]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node permute_1 (kind: aten.permute.default, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_2]_output &lt;tensorrt.ITensor [shape=(1, 3, 4, 200, 200), dtype=DataType.FLOAT]&gt;', [0, 3, 4, 1, 2]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node clone_16 (kind: aten.clone.default, args: ('[SHUFFLE]-[aten_ops.permute.default]-[permute_1]_output &lt;tensorrt.ITensor [shape=(1, 200, 200, 3, 4), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_4 (kind: aten.reshape.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_4]_output &lt;tensorrt.ITensor [shape=(1, 3, 100, 100), dtype=DataType.FLOAT]&gt;', [1, -1, 1, 100, 100]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node permute_2 (kind: aten.permute.default, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_4]_output &lt;tensorrt.ITensor [shape=(1, 3, 1, 100, 100), dtype=DataType.FLOAT]&gt;', [0, 3, 4, 1, 2]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node clone_17 (kind: aten.clone.default, args: ('[SHUFFLE]-[aten_ops.permute.default]-[permute_2]_output &lt;tensorrt.ITensor [shape=(1, 100, 100, 3, 1), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_6 (kind: aten.reshape.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_5]_output &lt;tensorrt.ITensor [shape=(1, 12, 100, 100), dtype=DataType.FLOAT]&gt;', [1, -1, 4, 100, 100]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node permute_3 (kind: aten.permute.default, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_6]_output &lt;tensorrt.ITensor [shape=(1, 3, 4, 100, 100), dtype=DataType.FLOAT]&gt;', [0, 3, 4, 1, 2]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node clone_18 (kind: aten.clone.default, args: ('[SHUFFLE]-[aten_ops.permute.default]-[permute_3]_output &lt;tensorrt.ITensor [shape=(1, 100, 100, 3, 4), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_8 (kind: aten.reshape.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_7]_output &lt;tensorrt.ITensor [shape=(1, 3, 50, 50), dtype=DataType.FLOAT]&gt;', [1, -1, 1, 50, 50]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node permute_4 (kind: aten.permute.default, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_8]_output &lt;tensorrt.ITensor [shape=(1, 3, 1, 50, 50), dtype=DataType.FLOAT]&gt;', [0, 3, 4, 1, 2]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node clone_19 (kind: aten.clone.default, args: ('[SHUFFLE]-[aten_ops.permute.default]-[permute_4]_output &lt;tensorrt.ITensor [shape=(1, 50, 50, 3, 1), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_10 (kind: aten.reshape.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_8]_output &lt;tensorrt.ITensor [shape=(1, 12, 50, 50), dtype=DataType.FLOAT]&gt;', [1, -1, 4, 50, 50]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node permute_5 (kind: aten.permute.default, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_10]_output &lt;tensorrt.ITensor [shape=(1, 3, 4, 50, 50), dtype=DataType.FLOAT]&gt;', [0, 3, 4, 1, 2]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node clone_20 (kind: aten.clone.default, args: ('[SHUFFLE]-[aten_ops.permute.default]-[permute_5]_output &lt;tensorrt.ITensor [shape=(1, 50, 50, 3, 4), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_12 (kind: aten.reshape.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_10]_output &lt;tensorrt.ITensor [shape=(1, 3, 25, 25), dtype=DataType.FLOAT]&gt;', [1, -1, 1, 25, 25]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node permute_6 (kind: aten.permute.default, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_12]_output &lt;tensorrt.ITensor [shape=(1, 3, 1, 25, 25), dtype=DataType.FLOAT]&gt;', [0, 3, 4, 1, 2]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node clone_21 (kind: aten.clone.default, args: ('[SHUFFLE]-[aten_ops.permute.default]-[permute_6]_output &lt;tensorrt.ITensor [shape=(1, 25, 25, 3, 1), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_14 (kind: aten.reshape.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_11]_output &lt;tensorrt.ITensor [shape=(1, 12, 25, 25), dtype=DataType.FLOAT]&gt;', [1, -1, 4, 25, 25]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node permute_7 (kind: aten.permute.default, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_14]_output &lt;tensorrt.ITensor [shape=(1, 3, 4, 25, 25), dtype=DataType.FLOAT]&gt;', [0, 3, 4, 1, 2]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node clone_22 (kind: aten.clone.default, args: ('[SHUFFLE]-[aten_ops.permute.default]-[permute_7]_output &lt;tensorrt.ITensor [shape=(1, 25, 25, 3, 4), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_16 (kind: aten.reshape.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_13]_output &lt;tensorrt.ITensor [shape=(1, 3, 13, 13), dtype=DataType.FLOAT]&gt;', [1, -1, 1, 13, 13]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node permute_8 (kind: aten.permute.default, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_16]_output &lt;tensorrt.ITensor [shape=(1, 3, 1, 13, 13), dtype=DataType.FLOAT]&gt;', [0, 3, 4, 1, 2]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node clone_23 (kind: aten.clone.default, args: ('[SHUFFLE]-[aten_ops.permute.default]-[permute_8]_output &lt;tensorrt.ITensor [shape=(1, 13, 13, 3, 1), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_18 (kind: aten.reshape.default, args: ('[CONVOLUTION]-[aten_ops.convolution.default]-[convolution_14]_output &lt;tensorrt.ITensor [shape=(1, 12, 13, 13), dtype=DataType.FLOAT]&gt;', [1, -1, 4, 13, 13]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node permute_9 (kind: aten.permute.default, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_18]_output &lt;tensorrt.ITensor [shape=(1, 3, 4, 13, 13), dtype=DataType.FLOAT]&gt;', [0, 3, 4, 1, 2]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node clone_24 (kind: aten.clone.default, args: ('[SHUFFLE]-[aten_ops.permute.default]-[permute_9]_output &lt;tensorrt.ITensor [shape=(1, 13, 13, 3, 4), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_1 (kind: aten.reshape.default, args: ('Forced Cast ITensor [SHUFFLE]-[aten_ops.permute.default]-[permute]_output from DataType.FLOAT to DataType.FLOAT - [aten_ops.torch.ops.aten.clone.default]-[clone_15]_output &lt;tensorrt.ITensor [shape=(1, 200, 200, 3, 1), dtype=DataType.FLOAT]&gt;', [1, 120000, 1]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_5 (kind: aten.reshape.default, args: ('Forced Cast ITensor [SHUFFLE]-[aten_ops.permute.default]-[permute_2]_output from DataType.FLOAT to DataType.FLOAT - [aten_ops.torch.ops.aten.clone.default]-[clone_17]_output &lt;tensorrt.ITensor [shape=(1, 100, 100, 3, 1), dtype=DataType.FLOAT]&gt;', [1, 30000, 1]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_9 (kind: aten.reshape.default, args: ('Forced Cast ITensor [SHUFFLE]-[aten_ops.permute.default]-[permute_4]_output from DataType.FLOAT to DataType.FLOAT - [aten_ops.torch.ops.aten.clone.default]-[clone_19]_output &lt;tensorrt.ITensor [shape=(1, 50, 50, 3, 1), dtype=DataType.FLOAT]&gt;', [1, 7500, 1]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_13 (kind: aten.reshape.default, args: ('Forced Cast ITensor [SHUFFLE]-[aten_ops.permute.default]-[permute_6]_output from DataType.FLOAT to DataType.FLOAT - [aten_ops.torch.ops.aten.clone.default]-[clone_21]_output &lt;tensorrt.ITensor [shape=(1, 25, 25, 3, 1), dtype=DataType.FLOAT]&gt;', [1, 1875, 1]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_17 (kind: aten.reshape.default, args: ('Forced Cast ITensor [SHUFFLE]-[aten_ops.permute.default]-[permute_8]_output from DataType.FLOAT to DataType.FLOAT - [aten_ops.torch.ops.aten.clone.default]-[clone_23]_output &lt;tensorrt.ITensor [shape=(1, 13, 13, 3, 1), dtype=DataType.FLOAT]&gt;', [1, 507, 1]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node cat_6 (kind: aten.cat.default, args: (['[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_1]_output &lt;tensorrt.ITensor [shape=(1, 120000, 1), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_5]_output &lt;tensorrt.ITensor [shape=(1, 30000, 1), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_9]_output &lt;tensorrt.ITensor [shape=(1, 7500, 1), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_13]_output &lt;tensorrt.ITensor [shape=(1, 1875, 1), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_17]_output &lt;tensorrt.ITensor [shape=(1, 507, 1), dtype=DataType.FLOAT]&gt;'], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_3 (kind: aten.reshape.default, args: ('Forced Cast ITensor [SHUFFLE]-[aten_ops.permute.default]-[permute_1]_output from DataType.FLOAT to DataType.FLOAT - [aten_ops.torch.ops.aten.clone.default]-[clone_16]_output &lt;tensorrt.ITensor [shape=(1, 200, 200, 3, 4), dtype=DataType.FLOAT]&gt;', [1, 120000, 4]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_7 (kind: aten.reshape.default, args: ('Forced Cast ITensor [SHUFFLE]-[aten_ops.permute.default]-[permute_3]_output from DataType.FLOAT to DataType.FLOAT - [aten_ops.torch.ops.aten.clone.default]-[clone_18]_output &lt;tensorrt.ITensor [shape=(1, 100, 100, 3, 4), dtype=DataType.FLOAT]&gt;', [1, 30000, 4]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_11 (kind: aten.reshape.default, args: ('Forced Cast ITensor [SHUFFLE]-[aten_ops.permute.default]-[permute_5]_output from DataType.FLOAT to DataType.FLOAT - [aten_ops.torch.ops.aten.clone.default]-[clone_20]_output &lt;tensorrt.ITensor [shape=(1, 50, 50, 3, 4), dtype=DataType.FLOAT]&gt;', [1, 7500, 4]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_15 (kind: aten.reshape.default, args: ('Forced Cast ITensor [SHUFFLE]-[aten_ops.permute.default]-[permute_7]_output from DataType.FLOAT to DataType.FLOAT - [aten_ops.torch.ops.aten.clone.default]-[clone_22]_output &lt;tensorrt.ITensor [shape=(1, 25, 25, 3, 4), dtype=DataType.FLOAT]&gt;', [1, 1875, 4]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_19 (kind: aten.reshape.default, args: ('Forced Cast ITensor [SHUFFLE]-[aten_ops.permute.default]-[permute_9]_output from DataType.FLOAT to DataType.FLOAT - [aten_ops.torch.ops.aten.clone.default]-[clone_24]_output &lt;tensorrt.ITensor [shape=(1, 13, 13, 3, 4), dtype=DataType.FLOAT]&gt;', [1, 507, 4]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node cat_7 (kind: aten.cat.default, args: (['[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_3]_output &lt;tensorrt.ITensor [shape=(1, 120000, 4), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_7]_output &lt;tensorrt.ITensor [shape=(1, 30000, 4), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_11]_output &lt;tensorrt.ITensor [shape=(1, 7500, 4), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_15]_output &lt;tensorrt.ITensor [shape=(1, 1875, 4), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_19]_output &lt;tensorrt.ITensor [shape=(1, 507, 4), dtype=DataType.FLOAT]&gt;'], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_21 (kind: aten.reshape.default, args: ('[CONCATENATION]-[aten_ops.cat.default]-[cat_7_gather]_output &lt;tensorrt.ITensor [shape=(1, 159882, 4), dtype=DataType.FLOAT]&gt;', [-1, 4]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_22 (kind: aten.reshape.default, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_21]_output &lt;tensorrt.ITensor [shape=(159882, 4), dtype=DataType.FLOAT]&gt;', [159882, -1]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node slice_7 (kind: aten.slice.Tensor, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_22]_output &lt;tensorrt.ITensor [shape=(159882, 4), dtype=DataType.FLOAT]&gt;', 0, 0, 9223372036854775807))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node slice_8 (kind: aten.slice.Tensor, args: ('[SLICE]-[unknown_ir_ops.slice.Tensor]-[slice_7]_output &lt;tensorrt.ITensor [shape=(159882, 4), dtype=DataType.FLOAT]&gt;', 1, 0, 9223372036854775807, 4))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node div (kind: aten.div.Tensor, args: ('[SLICE]-[unknown_ir_ops.slice.Tensor]-[slice_8]_output &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', 1.0))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node slice_9 (kind: aten.slice.Tensor, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_22]_output &lt;tensorrt.ITensor [shape=(159882, 4), dtype=DataType.FLOAT]&gt;', 0, 0, 9223372036854775807))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node slice_10 (kind: aten.slice.Tensor, args: ('[SLICE]-[unknown_ir_ops.slice.Tensor]-[slice_9]_output &lt;tensorrt.ITensor [shape=(159882, 4), dtype=DataType.FLOAT]&gt;', 1, 1, 9223372036854775807, 4))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node div_1 (kind: aten.div.Tensor, args: ('[SLICE]-[unknown_ir_ops.slice.Tensor]-[slice_10]_output &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', 1.0))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node slice_11 (kind: aten.slice.Tensor, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_22]_output &lt;tensorrt.ITensor [shape=(159882, 4), dtype=DataType.FLOAT]&gt;', 0, 0, 9223372036854775807))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node slice_12 (kind: aten.slice.Tensor, args: ('[SLICE]-[unknown_ir_ops.slice.Tensor]-[slice_11]_output &lt;tensorrt.ITensor [shape=(159882, 4), dtype=DataType.FLOAT]&gt;', 1, 2, 9223372036854775807, 4))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node div_2 (kind: aten.div.Tensor, args: ('[SLICE]-[unknown_ir_ops.slice.Tensor]-[slice_12]_output &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', 1.0))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node slice_13 (kind: aten.slice.Tensor, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_22]_output &lt;tensorrt.ITensor [shape=(159882, 4), dtype=DataType.FLOAT]&gt;', 0, 0, 9223372036854775807))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node slice_14 (kind: aten.slice.Tensor, args: ('[SLICE]-[unknown_ir_ops.slice.Tensor]-[slice_13]_output &lt;tensorrt.ITensor [shape=(159882, 4), dtype=DataType.FLOAT]&gt;', 1, 3, 9223372036854775807, 4))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node div_3 (kind: aten.div.Tensor, args: ('[SLICE]-[unknown_ir_ops.slice.Tensor]-[slice_14]_output &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', 1.0))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node clamp (kind: aten.clamp.default, args: ('[ELEMENTWISE]-[aten_ops.div.Tensor]-[div_2]_output_div.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', None, 4.135166556742356))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node clamp_1 (kind: aten.clamp.default, args: ('[ELEMENTWISE]-[aten_ops.div.Tensor]-[div_3]_output_div.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', None, 4.135166556742356))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node mul_12 (kind: aten.mul.Tensor, args: ('[ELEMENTWISE]-[aten_ops.div.Tensor]-[div]_output_div.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(159882, 1), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_7 (kind: aten.add.Tensor, args: ('[ELEMENTWISE]-[aten_ops.mul.Tensor]-[mul_12]_output_mul.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(159882, 1), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node mul_13 (kind: aten.mul.Tensor, args: ('[ELEMENTWISE]-[aten_ops.div.Tensor]-[div_1]_output_div.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(159882, 1), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_8 (kind: aten.add.Tensor, args: ('[ELEMENTWISE]-[aten_ops.mul.Tensor]-[mul_13]_output_mul.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(159882, 1), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node exp (kind: aten.exp.default, args: ('[ELEMENTWISE]-[aten_ops.clamp.default]-[clamp_min]_output_clamp.default &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node mul_14 (kind: aten.mul.Tensor, args: ('[UNARY]-[aten_ops.exp.default]-[exp]_output_exp.default &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(159882, 1), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node exp_1 (kind: aten.exp.default, args: ('[ELEMENTWISE]-[aten_ops.clamp.default]-[clamp_1_min]_output_clamp.default &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node mul_15 (kind: aten.mul.Tensor, args: ('[UNARY]-[aten_ops.exp.default]-[exp_1]_output_exp.default &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(159882, 1), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node mul_16 (kind: aten.mul.Tensor, args: ('&lt;torch.Tensor as np.ndarray [shape=(), dtype=float32]&gt;', '[ELEMENTWISE]-[aten_ops.mul.Tensor]-[mul_15]_output_mul.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node mul_17 (kind: aten.mul.Tensor, args: ('&lt;torch.Tensor as np.ndarray [shape=(), dtype=float32]&gt;', '[ELEMENTWISE]-[aten_ops.mul.Tensor]-[mul_14]_output_mul.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node sub_2 (kind: aten.sub.Tensor, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_7]_output_add.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', '[ELEMENTWISE]-[aten_ops.mul.Tensor]-[mul_17]_output_mul.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node sub_3 (kind: aten.sub.Tensor, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_8]_output_add.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', '[ELEMENTWISE]-[aten_ops.mul.Tensor]-[mul_16]_output_mul.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_9 (kind: aten.add.Tensor, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_7]_output_add.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', '[ELEMENTWISE]-[aten_ops.mul.Tensor]-[mul_17]_output_mul.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_10 (kind: aten.add.Tensor, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_8]_output_add.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', '[ELEMENTWISE]-[aten_ops.mul.Tensor]-[mul_16]_output_mul.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node unsqueeze_26 (kind: aten.unsqueeze.default, args: ('[ELEMENTWISE]-[aten_ops.sub.Tensor]-[sub_2]_output_sub.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', 2))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node unsqueeze_27 (kind: aten.unsqueeze.default, args: ('[ELEMENTWISE]-[aten_ops.sub.Tensor]-[sub_3]_output_sub.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', 2))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node unsqueeze_28 (kind: aten.unsqueeze.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_9]_output_add.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', 2))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node unsqueeze_29 (kind: aten.unsqueeze.default, args: ('[ELEMENTWISE]-[aten_ops.add.Tensor]-[add_10]_output_add.Tensor &lt;tensorrt.ITensor [shape=(159882, 1), dtype=DataType.FLOAT]&gt;', 2))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node cat_9 (kind: aten.cat.default, args: (['[SHUFFLE]-[aten_ops.unsqueeze.default]-[unsqueeze_26]_output &lt;tensorrt.ITensor [shape=(159882, 1, 1), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.unsqueeze.default]-[unsqueeze_27]_output &lt;tensorrt.ITensor [shape=(159882, 1, 1), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.unsqueeze.default]-[unsqueeze_28]_output &lt;tensorrt.ITensor [shape=(159882, 1, 1), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.unsqueeze.default]-[unsqueeze_29]_output &lt;tensorrt.ITensor [shape=(159882, 1, 1), dtype=DataType.FLOAT]&gt;'], 2))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_23 (kind: aten.reshape.default, args: ('[CONCATENATION]-[aten_ops.cat.default]-[cat_9_gather]_output &lt;tensorrt.ITensor [shape=(159882, 1, 4), dtype=DataType.FLOAT]&gt;', [159882, 4]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_24 (kind: aten.reshape.default, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_23]_output &lt;tensorrt.ITensor [shape=(159882, 4), dtype=DataType.FLOAT]&gt;', [159882, -1, 4]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_20 (kind: aten.reshape.default, args: ('[CONCATENATION]-[aten_ops.cat.default]-[cat_6_gather]_output &lt;tensorrt.ITensor [shape=(1, 159882, 1), dtype=DataType.FLOAT]&gt;', [159882, 1]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default_25 (kind: aten.reshape.default, args: ('[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default_24]_output &lt;tensorrt.ITensor [shape=(159882, 1, 4), dtype=DataType.FLOAT]&gt;', [1, -1, 4]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Marking output output0 [shape=(1, 159882, 4), dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Marking output output1 [shape=(159882, 1), dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Marking output output2 [shape=(159882, 4), dtype=DataType.FLOAT]
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT INetwork construction elapsed time: 0:00:00.455186
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Build TRT engine elapsed time: 0:00:22.104894
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT Engine uses: 10642124 bytes of Memory
DEBUG:torch_tensorrt.dynamo._DryRunTracker:
++++++++++++++++++++++++++++++++++++++++++++++++++ Dry-Run Results for Graph ++++++++++++++++++++++++++++++++++++++++++++++++++

The graph consists of 101 Total Operators, of which 101 operators are supported, 100.0% coverage

Compiled with: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

  Graph Structure:

   Inputs: List[Tensor: (1, 256, 200, 200)@float32, Tensor: (1, 256, 100, 100)@float32, Tensor: (1, 256, 50, 50)@float32, Tensor: (1, 256, 25, 25)@float32, Tensor: (1, 256, 13, 13)@float32]
    ...
    TRT Engine #1 - Submodule name: _run_on_acc_0
     Engine Inputs: List[Tensor: (1, 256, 200, 200)@float32, Tensor: (1, 256, 100, 100)@float32, Tensor: (1, 256, 50, 50)@float32, Tensor: (1, 256, 25, 25)@float32, Tensor: (1, 256, 13, 13)@float32]
     Number of Operators in Engine: 101
     Engine Outputs: Tuple(Tensor: (1, 159882, 4)@float32, Tensor: (159882, 1)@float32, Tensor: (159882, 4)@float32)
    ...
   Outputs: Tuple(Tensor: (1, 159882, 4)@float32, Tensor: (159882, 1)@float32, Tensor: (159882, 4)@float32, Tensor: (159882, 4)@float32, Tensor: (3, 4)@float32, Tensor: (3, 4)@float32, Tensor: (3, 4)@float32, Tensor: (3, 4)@float32, Tensor: (3, 4)@float32)

  ------------------------- Aggregate Stats -------------------------

   Average Number of Operators per TRT Engine: 101.0
   Most Operators in a TRT Engine: 101

  ********** Recommendations **********

   - For minimal graph segmentation, select min_block_size=101 which would generate 1 TRT engine(s)
   - The current level of graph segmentation is equivalent to selecting min_block_size=101 which generates 1 TRT engine(s)
INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)
INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.
INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

DEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:
graph():
    %l_objectness_ : torch.Tensor [num_users=1] = placeholder[target=L_objectness_]
    %split : [num_users=5] = call_method[target=split](args = (%l_objectness_, [120000, 30000, 7500, 1875, 507], 1), kwargs = {})
    %ob : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})
    %ob_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})
    %ob_2 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 2), kwargs = {})
    %ob_3 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 3), kwargs = {})
    %ob_4 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 4), kwargs = {})
    %topk : [num_users=1] = call_method[target=topk](args = (%ob, 1000), kwargs = {dim: 1})
    %top_n_idx : [num_users=1] = call_function[target=operator.getitem](args = (%topk, 1), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx, 0), kwargs = {})
    %topk_1 : [num_users=1] = call_method[target=topk](args = (%ob_1, 1000), kwargs = {dim: 1})
    %top_n_idx_1 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_1, 120000), kwargs = {})
    %topk_2 : [num_users=1] = call_method[target=topk](args = (%ob_2, 1000), kwargs = {dim: 1})
    %top_n_idx_2 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_2, 1), kwargs = {})
    %add_2 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_2, 150000), kwargs = {})
    %topk_3 : [num_users=1] = call_method[target=topk](args = (%ob_3, 1000), kwargs = {dim: 1})
    %top_n_idx_3 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_3, 1), kwargs = {})
    %add_3 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_3, 157500), kwargs = {})
    %topk_4 : [num_users=1] = call_method[target=topk](args = (%ob_4, 507), kwargs = {dim: 1})
    %top_n_idx_4 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_4, 1), kwargs = {})
    %add_4 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_4, 159375), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.cat](args = ([%add, %add_1, %add_2, %add_3, %add_4],), kwargs = {dim: 1})
    return (cat,)
DEBUG:torch_tensorrt.dynamo.lowering._repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:
graph():
    %l_objectness_ : torch.Tensor [num_users=1] = placeholder[target=L_objectness_]
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_objectness_,), kwargs = {})
    %split : [num_users=5] = call_method[target=split](args = (%clone_default, [120000, 30000, 7500, 1875, 507], 1), kwargs = {})
    %ob : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})
    %ob_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})
    %ob_2 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 2), kwargs = {})
    %ob_3 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 3), kwargs = {})
    %ob_4 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 4), kwargs = {})
    %topk : [num_users=1] = call_method[target=topk](args = (%ob, 1000), kwargs = {dim: 1})
    %top_n_idx : [num_users=1] = call_function[target=operator.getitem](args = (%topk, 1), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx, 0), kwargs = {})
    %topk_1 : [num_users=1] = call_method[target=topk](args = (%ob_1, 1000), kwargs = {dim: 1})
    %top_n_idx_1 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_1, 120000), kwargs = {})
    %topk_2 : [num_users=1] = call_method[target=topk](args = (%ob_2, 1000), kwargs = {dim: 1})
    %top_n_idx_2 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_2, 1), kwargs = {})
    %add_2 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_2, 150000), kwargs = {})
    %topk_3 : [num_users=1] = call_method[target=topk](args = (%ob_3, 1000), kwargs = {dim: 1})
    %top_n_idx_3 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_3, 1), kwargs = {})
    %add_3 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_3, 157500), kwargs = {})
    %topk_4 : [num_users=1] = call_method[target=topk](args = (%ob_4, 507), kwargs = {dim: 1})
    %top_n_idx_4 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_4, 1), kwargs = {})
    %add_4 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_4, 159375), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.cat](args = ([%add, %add_1, %add_2, %add_3, %add_4],), kwargs = {dim: 1})
    return (cat,)
DEBUG:torch_tensorrt.dynamo.lowering._remove_sym_nodes:Removed SymInt placeholders:
graph():
    %l_objectness_ : torch.Tensor [num_users=1] = placeholder[target=L_objectness_]
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_objectness_,), kwargs = {})
    %split : [num_users=5] = call_method[target=split](args = (%clone_default, [120000, 30000, 7500, 1875, 507], 1), kwargs = {})
    %ob : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})
    %ob_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})
    %ob_2 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 2), kwargs = {})
    %ob_3 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 3), kwargs = {})
    %ob_4 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 4), kwargs = {})
    %topk : [num_users=1] = call_method[target=topk](args = (%ob, 1000), kwargs = {dim: 1})
    %top_n_idx : [num_users=1] = call_function[target=operator.getitem](args = (%topk, 1), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx, 0), kwargs = {})
    %topk_1 : [num_users=1] = call_method[target=topk](args = (%ob_1, 1000), kwargs = {dim: 1})
    %top_n_idx_1 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_1, 120000), kwargs = {})
    %topk_2 : [num_users=1] = call_method[target=topk](args = (%ob_2, 1000), kwargs = {dim: 1})
    %top_n_idx_2 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_2, 1), kwargs = {})
    %add_2 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_2, 150000), kwargs = {})
    %topk_3 : [num_users=1] = call_method[target=topk](args = (%ob_3, 1000), kwargs = {dim: 1})
    %top_n_idx_3 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_3, 1), kwargs = {})
    %add_3 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_3, 157500), kwargs = {})
    %topk_4 : [num_users=1] = call_method[target=topk](args = (%ob_4, 507), kwargs = {dim: 1})
    %top_n_idx_4 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_4, 1), kwargs = {})
    %add_4 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_4, 159375), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.cat](args = ([%add, %add_1, %add_2, %add_3, %add_4],), kwargs = {dim: 1})
    return (cat,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 0 detach nodes:
graph():
    %l_objectness_ : torch.Tensor [num_users=1] = placeholder[target=L_objectness_]
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_objectness_,), kwargs = {})
    %split : [num_users=5] = call_method[target=split](args = (%clone_default, [120000, 30000, 7500, 1875, 507], 1), kwargs = {})
    %ob : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})
    %ob_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})
    %ob_2 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 2), kwargs = {})
    %ob_3 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 3), kwargs = {})
    %ob_4 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 4), kwargs = {})
    %topk : [num_users=1] = call_method[target=topk](args = (%ob, 1000), kwargs = {dim: 1})
    %top_n_idx : [num_users=1] = call_function[target=operator.getitem](args = (%topk, 1), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx, 0), kwargs = {})
    %topk_1 : [num_users=1] = call_method[target=topk](args = (%ob_1, 1000), kwargs = {dim: 1})
    %top_n_idx_1 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_1, 120000), kwargs = {})
    %topk_2 : [num_users=1] = call_method[target=topk](args = (%ob_2, 1000), kwargs = {dim: 1})
    %top_n_idx_2 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_2, 1), kwargs = {})
    %add_2 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_2, 150000), kwargs = {})
    %topk_3 : [num_users=1] = call_method[target=topk](args = (%ob_3, 1000), kwargs = {dim: 1})
    %top_n_idx_3 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_3, 1), kwargs = {})
    %add_3 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_3, 157500), kwargs = {})
    %topk_4 : [num_users=1] = call_method[target=topk](args = (%ob_4, 507), kwargs = {dim: 1})
    %top_n_idx_4 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_4, 1), kwargs = {})
    %add_4 : [num_users=1] = call_function[target=operator.add](args = (%top_n_idx_4, 159375), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.cat](args = ([%add, %add_1, %add_2, %add_3, %add_4],), kwargs = {dim: 1})
    return (cat,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {})
    %split_with_sizes : [num_users=5] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%clone, [120000, 30000, 7500, 1875, 507], 1), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 0), kwargs = {})
    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 1), kwargs = {})
    %getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 2), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 4), kwargs = {})
    %topk : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem, 1000, 1), kwargs = {})
    %getitem_6 : [num_users=1] = call_function[target=operator.getitem](args = (%topk, 1), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_6, 0), kwargs = {})
    %topk_1 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_1, 1000, 1), kwargs = {})
    %getitem_8 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_8, 120000), kwargs = {})
    %topk_2 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_2, 1000, 1), kwargs = {})
    %getitem_10 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_2, 1), kwargs = {})
    %add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_10, 150000), kwargs = {})
    %topk_3 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_3, 1000, 1), kwargs = {})
    %getitem_12 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_3, 1), kwargs = {})
    %add_3 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_12, 157500), kwargs = {})
    %topk_4 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_4, 507, 1), kwargs = {})
    %getitem_14 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_4, 1), kwargs = {})
    %add_4 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_14, 159375), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%add, %add_1, %add_2, %add_3, %add_4], 1), kwargs = {})
    return (cat,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone from graph, since it is a clone node which is the only user of placeholder arg0_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removed auxiliary clone nodes for placeholders:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %split_with_sizes : [num_users=5] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%arg0_1, [120000, 30000, 7500, 1875, 507], 1), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 0), kwargs = {})
    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 1), kwargs = {})
    %getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 2), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 4), kwargs = {})
    %topk : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem, 1000, 1), kwargs = {})
    %getitem_6 : [num_users=1] = call_function[target=operator.getitem](args = (%topk, 1), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_6, 0), kwargs = {})
    %topk_1 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_1, 1000, 1), kwargs = {})
    %getitem_8 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_8, 120000), kwargs = {})
    %topk_2 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_2, 1000, 1), kwargs = {})
    %getitem_10 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_2, 1), kwargs = {})
    %add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_10, 150000), kwargs = {})
    %topk_3 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_3, 1000, 1), kwargs = {})
    %getitem_12 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_3, 1), kwargs = {})
    %add_3 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_12, 157500), kwargs = {})
    %topk_4 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_4, 507, 1), kwargs = {})
    %getitem_14 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_4, 1), kwargs = {})
    %add_4 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_14, 159375), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%add, %add_1, %add_2, %add_3, %add_4], 1), kwargs = {})
    return (cat,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %split_with_sizes : [num_users=5] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%arg0_1, [120000, 30000, 7500, 1875, 507], 1), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 0), kwargs = {})
    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 1), kwargs = {})
    %getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 2), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 4), kwargs = {})
    %topk : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem, 1000, 1), kwargs = {})
    %getitem_6 : [num_users=1] = call_function[target=operator.getitem](args = (%topk, 1), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_6, 0), kwargs = {})
    %topk_1 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_1, 1000, 1), kwargs = {})
    %getitem_8 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_8, 120000), kwargs = {})
    %topk_2 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_2, 1000, 1), kwargs = {})
    %getitem_10 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_2, 1), kwargs = {})
    %add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_10, 150000), kwargs = {})
    %topk_3 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_3, 1000, 1), kwargs = {})
    %getitem_12 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_3, 1), kwargs = {})
    %add_3 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_12, 157500), kwargs = {})
    %topk_4 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_4, 507, 1), kwargs = {})
    %getitem_14 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_4, 1), kwargs = {})
    %add_4 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_14, 159375), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%add, %add_1, %add_2, %add_3, %add_4], 1), kwargs = {})
    return (cat,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:
 graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %split_with_sizes : [num_users=5] = call_function[target=torch.ops.aten.split_with_sizes.default](args = (%arg0_1, [120000, 30000, 7500, 1875, 507], 1), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 0), kwargs = {})
    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 1), kwargs = {})
    %getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 2), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%split_with_sizes, 4), kwargs = {})
    %topk : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem, 1000, 1), kwargs = {})
    %getitem_6 : [num_users=1] = call_function[target=operator.getitem](args = (%topk, 1), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_6, 0), kwargs = {})
    %topk_1 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_1, 1000, 1), kwargs = {})
    %getitem_8 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_8, 120000), kwargs = {})
    %topk_2 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_2, 1000, 1), kwargs = {})
    %getitem_10 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_2, 1), kwargs = {})
    %add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_10, 150000), kwargs = {})
    %topk_3 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_3, 1000, 1), kwargs = {})
    %getitem_12 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_3, 1), kwargs = {})
    %add_3 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_12, 157500), kwargs = {})
    %topk_4 : [num_users=1] = call_function[target=torch.ops.aten.topk.default](args = (%getitem_4, 507, 1), kwargs = {})
    %getitem_14 : [num_users=1] = call_function[target=operator.getitem](args = (%topk_4, 1), kwargs = {})
    %add_4 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_14, 159375), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%add, %add_1, %add_2, %add_3, %add_4], 1), kwargs = {})
    return (cat,)
DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Supported Nodes:
- torch.ops.aten.split_with_sizes.default + Operator Count: 1
- _operator.getitem + Operator Count: 5
- torch.ops.aten.add.Tensor + Operator Count: 5
- torch.ops.aten.cat.default + Operator Count: 1

DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Unsupported or Excluded Nodes:
- torch.ops.aten.topk.default + Operator Count: 5
- _operator.getitem + Operator Count: 5

DEBUG:torch_tensorrt.dynamo._compiler:Detected support for 12 operators out of 22 in subgraph.
DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:Eliminating acc subgraph because it's smaller than the threshold: 6 &lt; 7
DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:Eliminating acc subgraph because it's smaller than the threshold: 6 &lt; 7
DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
Number of TensorRT-Accelerated Engines Generated: 0
DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
Supported Nodes:
- torch.ops.aten.split_with_sizes.default + Operator Count: 1
- _operator.getitem + Operator Count: 5
- torch.ops.aten.add.Tensor + Operator Count: 5
- torch.ops.aten.cat.default + Operator Count: 1

DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
Unsupported or Excluded Nodes:
- torch.ops.aten.topk.default + Operator Count: 5
- _operator.getitem + Operator Count: 5

DEBUG:torch_tensorrt.dynamo._DryRunTracker:
++++++++++++++++++++++++++++++++++++++++++++++++++ Dry-Run Results for Graph ++++++++++++++++++++++++++++++++++++++++++++++++++

The graph consists of 22 Total Operators, of which 12 operators are supported, 54.55% coverage

The following ops are currently unsupported or excluded from conversion, and are listed with their op-count in the graph:
 torch.ops.aten.topk.default: 5
_operator.getitem: 5

The following nodes are currently set to run in Torch:
Node: torch.ops.aten.split_with_sizes.default, with layer location: split_with_sizes
Node: torch.ops.aten.topk.default, with layer location: topk
Node: torch.ops.aten.add.Tensor, with layer location: add
Node: torch.ops.aten.topk.default, with layer location: topk_1
Node: torch.ops.aten.add.Tensor, with layer location: add_1
Node: torch.ops.aten.topk.default, with layer location: topk_2
Node: torch.ops.aten.add.Tensor, with layer location: add_2
Node: torch.ops.aten.topk.default, with layer location: topk_3
Node: torch.ops.aten.add.Tensor, with layer location: add_3
Node: torch.ops.aten.topk.default, with layer location: topk_4
Node: torch.ops.aten.add.Tensor, with layer location: add_4
Node: torch.ops.aten.cat.default, with layer location: cat
Note: Some of the above nodes may be supported, but were not included in a TRT graph by the partitioner

Compiled with: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

  Graph Structure:

   Inputs: List[Tensor: (1, 159882)@float32]
    ...
   Outputs: Tuple(Tensor: (1, 4507)@int64)

  Aggregate stats not available since no TRT Engines were generated.
WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torch/_library/abstract_impl.py:127: UserWarning: create_unbacked_symint is deprecated, please use new_dynamic_size instead
  warnings.warn(

INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)
INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.
INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

DEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.batched_nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.lowering._repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.batched_nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.lowering._remove_sym_nodes:Removed SymInt placeholders:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.batched_nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 0 detach nodes:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.batched_nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:
graph():
    return ()
DEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:
graph():
    return ()
DEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:
 graph():
    return ()
DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Supported Nodes:

DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
All Nodes Supported

WARNING:torch_tensorrt.dynamo._compiler:0 supported operations detected in subgraph containing 0 computational nodes. Skipping this subgraph, since min_block_size was detected to be 7
WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torch/_library/abstract_impl.py:127: UserWarning: create_unbacked_symint is deprecated, please use new_dynamic_size instead
  warnings.warn(

INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)
INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.
INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

DEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:
graph():
    %l_boxes_ : torch.Tensor [num_users=4] = placeholder[target=L_boxes_]
    %l_idxs_ : torch.Tensor [num_users=1] = placeholder[target=L_idxs_]
    %max_coordinate : [num_users=1] = call_method[target=max](args = (%l_boxes_,), kwargs = {})
    %to : [num_users=1] = call_method[target=to](args = (%l_idxs_, %l_boxes_), kwargs = {})
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (1,), kwargs = {})
    %to_1 : [num_users=1] = call_method[target=to](args = (%tensor, %l_boxes_), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%max_coordinate, %to_1), kwargs = {})
    %offsets : [num_users=1] = call_function[target=operator.mul](args = (%to, %add), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%offsets, (slice(None, None, None), None)), kwargs = {})
    %boxes_for_nms : [num_users=1] = call_function[target=operator.add](args = (%l_boxes_, %getitem), kwargs = {})
    return (boxes_for_nms,)
DEBUG:torch_tensorrt.dynamo.lowering._repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:
graph():
    %l_boxes_ : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_idxs_ : torch.Tensor [num_users=1] = placeholder[target=L_idxs_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_idxs_,), kwargs = {})
    %clone_default : [num_users=4] = call_function[target=torch.ops.aten.clone.default](args = (%l_boxes_,), kwargs = {})
    %max_coordinate : [num_users=1] = call_method[target=max](args = (%clone_default,), kwargs = {})
    %to : [num_users=1] = call_method[target=to](args = (%clone_default_1, %clone_default), kwargs = {})
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (1,), kwargs = {})
    %to_1 : [num_users=1] = call_method[target=to](args = (%tensor, %clone_default), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%max_coordinate, %to_1), kwargs = {})
    %offsets : [num_users=1] = call_function[target=operator.mul](args = (%to, %add), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%offsets, (slice(None, None, None), None)), kwargs = {})
    %boxes_for_nms : [num_users=1] = call_function[target=operator.add](args = (%clone_default, %getitem), kwargs = {})
    return (boxes_for_nms,)
DEBUG:torch_tensorrt.dynamo.lowering._remove_sym_nodes:Removed SymInt placeholders:
graph():
    %l_boxes_ : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_idxs_ : torch.Tensor [num_users=1] = placeholder[target=L_idxs_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_idxs_,), kwargs = {})
    %clone_default : [num_users=4] = call_function[target=torch.ops.aten.clone.default](args = (%l_boxes_,), kwargs = {})
    %max_coordinate : [num_users=1] = call_method[target=max](args = (%clone_default,), kwargs = {})
    %to : [num_users=1] = call_method[target=to](args = (%clone_default_1, %clone_default), kwargs = {})
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (1,), kwargs = {})
    %to_1 : [num_users=1] = call_method[target=to](args = (%tensor, %clone_default), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%max_coordinate, %to_1), kwargs = {})
    %offsets : [num_users=1] = call_function[target=operator.mul](args = (%to, %add), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%offsets, (slice(None, None, None), None)), kwargs = {})
    %boxes_for_nms : [num_users=1] = call_function[target=operator.add](args = (%clone_default, %getitem), kwargs = {})
    return (boxes_for_nms,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 0 detach nodes:
graph():
    %l_boxes_ : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_idxs_ : torch.Tensor [num_users=1] = placeholder[target=L_idxs_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_idxs_,), kwargs = {})
    %clone_default : [num_users=4] = call_function[target=torch.ops.aten.clone.default](args = (%l_boxes_,), kwargs = {})
    %max_coordinate : [num_users=1] = call_method[target=max](args = (%clone_default,), kwargs = {})
    %to : [num_users=1] = call_method[target=to](args = (%clone_default_1, %clone_default), kwargs = {})
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (1,), kwargs = {})
    %to_1 : [num_users=1] = call_method[target=to](args = (%tensor, %clone_default), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%max_coordinate, %to_1), kwargs = {})
    %offsets : [num_users=1] = call_function[target=operator.mul](args = (%to, %add), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%offsets, (slice(None, None, None), None)), kwargs = {})
    %boxes_for_nms : [num_users=1] = call_function[target=operator.add](args = (%clone_default, %getitem), kwargs = {})
    return (boxes_for_nms,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg1_1,), kwargs = {})
    %clone_1 : [num_users=2] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {})
    %max_1 : [num_users=1] = call_function[target=torch.ops.aten.max.default](args = (%clone_1,), kwargs = {})
    %_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%clone,), kwargs = {dtype: torch.float32, device: cuda:0})
    %_tensor_constant0 : [num_users=1] = get_attr[target=_tensor_constant0]
    %lift_fresh_copy : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant0,), kwargs = {})
    %_to_copy_1 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%lift_fresh_copy,), kwargs = {dtype: torch.float32, device: cuda:0})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%max_1, %_to_copy_1), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_to_copy, %add), kwargs = {})
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%mul, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_1, %unsqueeze), kwargs = {})
    return (add_1,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone_1 from graph, since it is a clone node which is the only user of placeholder arg0_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone from graph, since it is a clone node which is the only user of placeholder arg1_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removed auxiliary clone nodes for placeholders:
graph():
    %arg0_1 : [num_users=2] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %max_1 : [num_users=1] = call_function[target=torch.ops.aten.max.default](args = (%arg0_1,), kwargs = {})
    %_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%arg1_1,), kwargs = {dtype: torch.float32, device: cuda:0})
    %_tensor_constant0 : [num_users=1] = get_attr[target=_tensor_constant0]
    %lift_fresh_copy : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant0,), kwargs = {})
    %_to_copy_1 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%lift_fresh_copy,), kwargs = {dtype: torch.float32, device: cuda:0})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%max_1, %_to_copy_1), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_to_copy, %add), kwargs = {})
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%mul, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg0_1, %unsqueeze), kwargs = {})
    return (add_1,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:
graph():
    %arg0_1 : [num_users=2] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %max_1 : [num_users=1] = call_function[target=torch.ops.aten.max.default](args = (%arg0_1,), kwargs = {})
    %_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%arg1_1,), kwargs = {dtype: torch.float32, device: cuda:0})
    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%max_1, %_frozen_param0), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_to_copy, %add), kwargs = {})
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%mul, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg0_1, %unsqueeze), kwargs = {})
    return (add_1,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:
 graph():
    %arg0_1 : [num_users=2] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %max_1 : [num_users=1] = call_function[target=torch.ops.aten.max.default](args = (%arg0_1,), kwargs = {})
    %_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%arg1_1,), kwargs = {dtype: torch.float32, device: cuda:0})
    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%max_1, %_frozen_param0), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_to_copy, %add), kwargs = {})
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%mul, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg0_1, %unsqueeze), kwargs = {})
    return (add_1,)
DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Supported Nodes:
- torch.ops.aten.max.default + Operator Count: 1
- torch.ops.aten._to_copy.default + Operator Count: 1
- torch.ops.aten.add.Tensor + Operator Count: 2
- torch.ops.aten.mul.Tensor + Operator Count: 1
- torch.ops.aten.slice.Tensor + Operator Count: 1
- torch.ops.aten.unsqueeze.default + Operator Count: 1

DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
All Nodes Supported

DEBUG:torch_tensorrt.dynamo._compiler:Detected support for 7 operators out of 7 in subgraph.
DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
Number of TensorRT-Accelerated Engines Generated: 1
DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
Supported Nodes:
- torch.ops.aten.max.default + Operator Count: 1
- torch.ops.aten._to_copy.default + Operator Count: 1
- torch.ops.aten.add.Tensor + Operator Count: 2
- torch.ops.aten.mul.Tensor + Operator Count: 1
- torch.ops.aten.slice.Tensor + Operator Count: 1
- torch.ops.aten.unsqueeze.default + Operator Count: 1

DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
All Nodes Supported

DEBUG:torch_tensorrt.dynamo._compiler:Submodule name: _run_on_acc_0
 Input shapes: [(4090, 4), (4090,)]
 graph():
    %arg0_1 : [num_users=2] = placeholder[target=arg0_1]
    %max_1 : [num_users=1] = call_function[target=torch.ops.aten.max.default](args = (%arg0_1,), kwargs = {})
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%arg1_1,), kwargs = {dtype: torch.float32, device: cuda:0})
    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%max_1, %_frozen_param0), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_to_copy, %add), kwargs = {})
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%mul, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg0_1, %unsqueeze), kwargs = {})
    return add_1
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Adding input to in-progress INetwork: arg0_1 [shape=[4090, 4], dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node max_1 (kind: aten.max.default, args: ('arg0_1 &lt;tensorrt.ITensor [shape=(4090, 4), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Adding input to in-progress INetwork: arg1_1 [shape=[4090], dtype=DataType.INT64]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _to_copy (kind: aten._to_copy.default, args: ('arg1_1 &lt;tensorrt.ITensor [shape=(4090,), dtype=DataType.INT64]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add (kind: aten.add.Tensor, args: ('[REDUCE]-[aten_ops.max.default]-[max_1]_output &lt;tensorrt.ITensor [shape=(), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node mul (kind: aten.mul.Tensor, args: ('Forced Cast ITensor arg1_1 from DataType.INT64 to DataType.FLOAT - [aten_ops.torch.ops.aten._to_copy.default]-[_to_copy]_output &lt;tensorrt.ITensor [shape=(4090,), dtype=DataType.FLOAT]&gt;', '[ELEMENTWISE]-[aten_ops.add.Tensor]-[add]_output_add.Tensor &lt;tensorrt.ITensor [shape=(), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node slice_1 (kind: aten.slice.Tensor, args: ('[ELEMENTWISE]-[aten_ops.mul.Tensor]-[mul]_output_mul.Tensor &lt;tensorrt.ITensor [shape=(4090,), dtype=DataType.FLOAT]&gt;', 0, 0, 9223372036854775807))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node unsqueeze (kind: aten.unsqueeze.default, args: ('[SLICE]-[unknown_ir_ops.slice.Tensor]-[slice_1]_output &lt;tensorrt.ITensor [shape=(4090,), dtype=DataType.FLOAT]&gt;', 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_1 (kind: aten.add.Tensor, args: ('arg0_1 &lt;tensorrt.ITensor [shape=(4090, 4), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.unsqueeze.default]-[unsqueeze]_output &lt;tensorrt.ITensor [shape=(4090, 1), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Marking output output0 [shape=(4090, 4), dtype=DataType.FLOAT]
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT INetwork construction elapsed time: 0:00:00.020515
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Build TRT engine elapsed time: 0:00:00.156491
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT Engine uses: 17084 bytes of Memory
DEBUG:torch_tensorrt.dynamo._DryRunTracker:
++++++++++++++++++++++++++++++++++++++++++++++++++ Dry-Run Results for Graph ++++++++++++++++++++++++++++++++++++++++++++++++++

The graph consists of 7 Total Operators, of which 7 operators are supported, 100.0% coverage

Compiled with: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

  Graph Structure:

   Inputs: List[Tensor: (4090, 4)@float32, Tensor: (4090,)@int64]
    ...
    TRT Engine #1 - Submodule name: _run_on_acc_0
     Engine Inputs: List[Tensor: (4090, 4)@float32, Tensor: (4090,)@int64]
     Number of Operators in Engine: 7
     Engine Outputs: Tensor: (4090, 4)@float32
    ...
   Outputs: Tuple(Tensor: (4090, 4)@float32)

  ------------------------- Aggregate Stats -------------------------

   Average Number of Operators per TRT Engine: 7.0
   Most Operators in a TRT Engine: 7

  ********** Recommendations **********

   - For minimal graph segmentation, select min_block_size=7 which would generate 1 TRT engine(s)
   - The current level of graph segmentation is equivalent to selecting min_block_size=7 which generates 1 TRT engine(s)
WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torch/_library/abstract_impl.py:127: UserWarning: create_unbacked_symint is deprecated, please use new_dynamic_size instead
  warnings.warn(

INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)
INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.
INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

DEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.lowering._repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.lowering._remove_sym_nodes:Removed SymInt placeholders:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 0 detach nodes:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:
graph():
    return ()
DEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:
graph():
    return ()
DEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:
 graph():
    return ()
DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Supported Nodes:

DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
All Nodes Supported

WARNING:torch_tensorrt.dynamo._compiler:0 supported operations detected in subgraph containing 0 computational nodes. Skipping this subgraph, since min_block_size was detected to be 7
INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)
INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.
INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

DEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%l_input_, %rois, 0.25, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering._repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_input_,), kwargs = {})
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%rois,), kwargs = {})
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%clone_default_1, %clone_default, 0.25, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering._remove_sym_nodes:Removed SymInt placeholders:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_input_,), kwargs = {})
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%rois,), kwargs = {})
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%clone_default_1, %clone_default, 0.25, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 0 detach nodes:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_input_,), kwargs = {})
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%rois,), kwargs = {})
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%clone_default_1, %clone_default, 0.25, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg1_1,), kwargs = {})
    %clone_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%clone, %clone_1, 0.25, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone_1 from graph, since it is a clone node which is the only user of placeholder arg0_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone from graph, since it is a clone node which is the only user of placeholder arg1_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removed auxiliary clone nodes for placeholders:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%arg1_1, %arg0_1, 0.25, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%arg1_1, %arg0_1, 0.25, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:
 graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%arg1_1, %arg0_1, 0.25, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Supported Nodes:

DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Unsupported or Excluded Nodes:
- torch.ops.torchvision.roi_align.default + Operator Count: 1

WARNING:torch_tensorrt.dynamo._compiler:0 supported operations detected in subgraph containing 1 computational nodes. Skipping this subgraph, since min_block_size was detected to be 7
INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)
INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.
INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

DEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%l_input_, %rois, 0.125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering._repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_input_,), kwargs = {})
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%rois,), kwargs = {})
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%clone_default_1, %clone_default, 0.125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering._remove_sym_nodes:Removed SymInt placeholders:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_input_,), kwargs = {})
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%rois,), kwargs = {})
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%clone_default_1, %clone_default, 0.125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 0 detach nodes:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_input_,), kwargs = {})
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%rois,), kwargs = {})
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%clone_default_1, %clone_default, 0.125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg1_1,), kwargs = {})
    %clone_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%clone, %clone_1, 0.125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone_1 from graph, since it is a clone node which is the only user of placeholder arg0_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone from graph, since it is a clone node which is the only user of placeholder arg1_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removed auxiliary clone nodes for placeholders:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%arg1_1, %arg0_1, 0.125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%arg1_1, %arg0_1, 0.125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:
 graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%arg1_1, %arg0_1, 0.125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Supported Nodes:

DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Unsupported or Excluded Nodes:
- torch.ops.torchvision.roi_align.default + Operator Count: 1

WARNING:torch_tensorrt.dynamo._compiler:0 supported operations detected in subgraph containing 1 computational nodes. Skipping this subgraph, since min_block_size was detected to be 7
INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)
INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.
INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

DEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%l_input_, %rois, 0.0625, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering._repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_input_,), kwargs = {})
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%rois,), kwargs = {})
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%clone_default_1, %clone_default, 0.0625, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering._remove_sym_nodes:Removed SymInt placeholders:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_input_,), kwargs = {})
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%rois,), kwargs = {})
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%clone_default_1, %clone_default, 0.0625, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 0 detach nodes:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_input_,), kwargs = {})
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%rois,), kwargs = {})
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%clone_default_1, %clone_default, 0.0625, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg1_1,), kwargs = {})
    %clone_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%clone, %clone_1, 0.0625, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone_1 from graph, since it is a clone node which is the only user of placeholder arg0_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone from graph, since it is a clone node which is the only user of placeholder arg1_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removed auxiliary clone nodes for placeholders:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%arg1_1, %arg0_1, 0.0625, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%arg1_1, %arg0_1, 0.0625, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:
 graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%arg1_1, %arg0_1, 0.0625, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Supported Nodes:

DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Unsupported or Excluded Nodes:
- torch.ops.torchvision.roi_align.default + Operator Count: 1

WARNING:torch_tensorrt.dynamo._compiler:0 supported operations detected in subgraph containing 1 computational nodes. Skipping this subgraph, since min_block_size was detected to be 7
INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)
INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.
INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

DEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%l_input_, %rois, 0.03125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering._repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_input_,), kwargs = {})
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%rois,), kwargs = {})
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%clone_default_1, %clone_default, 0.03125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering._remove_sym_nodes:Removed SymInt placeholders:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_input_,), kwargs = {})
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%rois,), kwargs = {})
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%clone_default_1, %clone_default, 0.03125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 0 detach nodes:
graph():
    %rois : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_input_,), kwargs = {})
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%rois,), kwargs = {})
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.roi_align.roi_align,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align](args = (%clone_default_1, %clone_default, 0.03125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg1_1,), kwargs = {})
    %clone_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {})
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%clone, %clone_1, 0.03125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone_1 from graph, since it is a clone node which is the only user of placeholder arg0_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone from graph, since it is a clone node which is the only user of placeholder arg1_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removed auxiliary clone nodes for placeholders:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%arg1_1, %arg0_1, 0.03125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%arg1_1, %arg0_1, 0.03125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:
 graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %roi_align : [num_users=1] = call_function[target=torch.ops.torchvision.roi_align.default](args = (%arg1_1, %arg0_1, 0.03125, 7, 7, 2, False), kwargs = {})
    return (roi_align,)
DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Supported Nodes:

DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Unsupported or Excluded Nodes:
- torch.ops.torchvision.roi_align.default + Operator Count: 1

WARNING:torch_tensorrt.dynamo._compiler:0 supported operations detected in subgraph containing 1 computational nodes. Skipping this subgraph, since min_block_size was detected to be 7
INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)
INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.
INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

DEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:
graph():
    %box_features : torch.Tensor [num_users=1] = placeholder[target=L_stack0_]
    %x : [num_users=1] = call_method[target=flatten](args = (%box_features,), kwargs = {start_dim: 1})
    %l__self___box_head_fc6 : [num_users=1] = call_module[target=L__self___box_head_fc6](args = (%x,), kwargs = {})
    %x_1 : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%l__self___box_head_fc6,), kwargs = {})
    %l__self___box_head_fc7 : [num_users=1] = call_module[target=L__self___box_head_fc7](args = (%x_1,), kwargs = {})
    %box_features_1 : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%l__self___box_head_fc7,), kwargs = {})
    %x_3 : [num_users=2] = call_method[target=flatten](args = (%box_features_1,), kwargs = {start_dim: 1})
    %class_logits : [num_users=1] = call_module[target=L__self___box_predictor_cls_score](args = (%x_3,), kwargs = {})
    %box_regression : [num_users=1] = call_module[target=L__self___box_predictor_bbox_pred](args = (%x_3,), kwargs = {})
    return (class_logits, box_regression)
DEBUG:torch_tensorrt.dynamo.lowering._repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:
graph():
    %box_features : torch.Tensor [num_users=1] = placeholder[target=L_stack0_]
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%box_features,), kwargs = {})
    %x : [num_users=1] = call_method[target=flatten](args = (%clone_default,), kwargs = {start_dim: 1})
    %l__self___box_head_fc6 : [num_users=1] = call_module[target=L__self___box_head_fc6](args = (%x,), kwargs = {})
    %x_1 : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%l__self___box_head_fc6,), kwargs = {})
    %l__self___box_head_fc7 : [num_users=1] = call_module[target=L__self___box_head_fc7](args = (%x_1,), kwargs = {})
    %box_features_1 : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%l__self___box_head_fc7,), kwargs = {})
    %x_3 : [num_users=2] = call_method[target=flatten](args = (%box_features_1,), kwargs = {start_dim: 1})
    %class_logits : [num_users=1] = call_module[target=L__self___box_predictor_cls_score](args = (%x_3,), kwargs = {})
    %box_regression : [num_users=1] = call_module[target=L__self___box_predictor_bbox_pred](args = (%x_3,), kwargs = {})
    return (class_logits, box_regression)
DEBUG:torch_tensorrt.dynamo.lowering._remove_sym_nodes:Removed SymInt placeholders:
graph():
    %box_features : torch.Tensor [num_users=1] = placeholder[target=L_stack0_]
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%box_features,), kwargs = {})
    %x : [num_users=1] = call_method[target=flatten](args = (%clone_default,), kwargs = {start_dim: 1})
    %l__self___box_head_fc6 : [num_users=1] = call_module[target=L__self___box_head_fc6](args = (%x,), kwargs = {})
    %x_1 : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%l__self___box_head_fc6,), kwargs = {})
    %l__self___box_head_fc7 : [num_users=1] = call_module[target=L__self___box_head_fc7](args = (%x_1,), kwargs = {})
    %box_features_1 : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%l__self___box_head_fc7,), kwargs = {})
    %x_3 : [num_users=2] = call_method[target=flatten](args = (%box_features_1,), kwargs = {start_dim: 1})
    %class_logits : [num_users=1] = call_module[target=L__self___box_predictor_cls_score](args = (%x_3,), kwargs = {})
    %box_regression : [num_users=1] = call_module[target=L__self___box_predictor_bbox_pred](args = (%x_3,), kwargs = {})
    return (class_logits, box_regression)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 0 detach nodes:
graph():
    %box_features : torch.Tensor [num_users=1] = placeholder[target=L_stack0_]
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%box_features,), kwargs = {})
    %x : [num_users=1] = call_method[target=flatten](args = (%clone_default,), kwargs = {start_dim: 1})
    %l__self___box_head_fc6 : [num_users=1] = call_module[target=L__self___box_head_fc6](args = (%x,), kwargs = {})
    %x_1 : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%l__self___box_head_fc6,), kwargs = {})
    %l__self___box_head_fc7 : [num_users=1] = call_module[target=L__self___box_head_fc7](args = (%x_1,), kwargs = {})
    %box_features_1 : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%l__self___box_head_fc7,), kwargs = {})
    %x_3 : [num_users=2] = call_method[target=flatten](args = (%box_features_1,), kwargs = {start_dim: 1})
    %class_logits : [num_users=1] = call_module[target=L__self___box_predictor_cls_score](args = (%x_3,), kwargs = {})
    %box_regression : [num_users=1] = call_module[target=L__self___box_predictor_bbox_pred](args = (%x_3,), kwargs = {})
    return (class_logits, box_regression)
DEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {})
    %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%clone, [1000, 12544]), kwargs = {})
    %_param_constant0 : [num_users=1] = get_attr[target=_param_constant0]
    %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%_param_constant0, [1, 0]), kwargs = {})
    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]
    %addmm : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1, %view, %permute), kwargs = {})
    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%addmm,), kwargs = {})
    %_param_constant2 : [num_users=1] = get_attr[target=_param_constant2]
    %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%_param_constant2, [1, 0]), kwargs = {})
    %_param_constant3 : [num_users=1] = get_attr[target=_param_constant3]
    %addmm_1 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant3, %relu, %permute_1), kwargs = {})
    %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%addmm_1,), kwargs = {})
    %_param_constant4 : [num_users=1] = get_attr[target=_param_constant4]
    %permute_2 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%_param_constant4, [1, 0]), kwargs = {})
    %_param_constant5 : [num_users=1] = get_attr[target=_param_constant5]
    %addmm_2 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant5, %relu_1, %permute_2), kwargs = {})
    %_param_constant6 : [num_users=1] = get_attr[target=_param_constant6]
    %permute_3 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%_param_constant6, [1, 0]), kwargs = {})
    %_param_constant7 : [num_users=1] = get_attr[target=_param_constant7]
    %addmm_3 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant7, %relu_1, %permute_3), kwargs = {})
    return (addmm_2, addmm_3)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone from graph, since it is a clone node which is the only user of placeholder arg0_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removed auxiliary clone nodes for placeholders:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arg0_1, [1000, 12544]), kwargs = {})
    %_param_constant0 : [num_users=1] = get_attr[target=_param_constant0]
    %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%_param_constant0, [1, 0]), kwargs = {})
    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]
    %addmm : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1, %view, %permute), kwargs = {})
    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%addmm,), kwargs = {})
    %_param_constant2 : [num_users=1] = get_attr[target=_param_constant2]
    %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%_param_constant2, [1, 0]), kwargs = {})
    %_param_constant3 : [num_users=1] = get_attr[target=_param_constant3]
    %addmm_1 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant3, %relu, %permute_1), kwargs = {})
    %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%addmm_1,), kwargs = {})
    %_param_constant4 : [num_users=1] = get_attr[target=_param_constant4]
    %permute_2 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%_param_constant4, [1, 0]), kwargs = {})
    %_param_constant5 : [num_users=1] = get_attr[target=_param_constant5]
    %addmm_2 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant5, %relu_1, %permute_2), kwargs = {})
    %_param_constant6 : [num_users=1] = get_attr[target=_param_constant6]
    %permute_3 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%_param_constant6, [1, 0]), kwargs = {})
    %_param_constant7 : [num_users=1] = get_attr[target=_param_constant7]
    %addmm_3 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant7, %relu_1, %permute_3), kwargs = {})
    return (addmm_2, addmm_3)
DEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arg0_1, [1000, 12544]), kwargs = {})
    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]
    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]
    %addmm : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1, %view, %_frozen_param0), kwargs = {})
    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%addmm,), kwargs = {})
    %_frozen_param1 : [num_users=1] = get_attr[target=_frozen_param1]
    %_param_constant3 : [num_users=1] = get_attr[target=_param_constant3]
    %addmm_1 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant3, %relu, %_frozen_param1), kwargs = {})
    %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%addmm_1,), kwargs = {})
    %_frozen_param2 : [num_users=1] = get_attr[target=_frozen_param2]
    %_param_constant5 : [num_users=1] = get_attr[target=_param_constant5]
    %addmm_2 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant5, %relu_1, %_frozen_param2), kwargs = {})
    %_frozen_param3 : [num_users=1] = get_attr[target=_frozen_param3]
    %_param_constant7 : [num_users=1] = get_attr[target=_param_constant7]
    %addmm_3 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant7, %relu_1, %_frozen_param3), kwargs = {})
    return (addmm_2, addmm_3)
DEBUG:torch_tensorrt.dynamo.lowering.passes.view_to_reshape:Graph after replacing view with reshape:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]
    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]
    %reshape_default : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%arg0_1, [1000, 12544]), kwargs = {})
    %addmm : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1, %reshape_default, %_frozen_param0), kwargs = {})
    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%addmm,), kwargs = {})
    %_frozen_param1 : [num_users=1] = get_attr[target=_frozen_param1]
    %_param_constant3 : [num_users=1] = get_attr[target=_param_constant3]
    %addmm_1 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant3, %relu, %_frozen_param1), kwargs = {})
    %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%addmm_1,), kwargs = {})
    %_frozen_param2 : [num_users=1] = get_attr[target=_frozen_param2]
    %_param_constant5 : [num_users=1] = get_attr[target=_param_constant5]
    %addmm_2 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant5, %relu_1, %_frozen_param2), kwargs = {})
    %_frozen_param3 : [num_users=1] = get_attr[target=_frozen_param3]
    %_param_constant7 : [num_users=1] = get_attr[target=_param_constant7]
    %addmm_3 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant7, %relu_1, %_frozen_param3), kwargs = {})
    return (addmm_2, addmm_3)
DEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:
 graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]
    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]
    %reshape_default : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%arg0_1, [1000, 12544]), kwargs = {})
    %addmm : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1, %reshape_default, %_frozen_param0), kwargs = {})
    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%addmm,), kwargs = {})
    %_frozen_param1 : [num_users=1] = get_attr[target=_frozen_param1]
    %_param_constant3 : [num_users=1] = get_attr[target=_param_constant3]
    %addmm_1 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant3, %relu, %_frozen_param1), kwargs = {})
    %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%addmm_1,), kwargs = {})
    %_frozen_param2 : [num_users=1] = get_attr[target=_frozen_param2]
    %_param_constant5 : [num_users=1] = get_attr[target=_param_constant5]
    %addmm_2 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant5, %relu_1, %_frozen_param2), kwargs = {})
    %_frozen_param3 : [num_users=1] = get_attr[target=_frozen_param3]
    %_param_constant7 : [num_users=1] = get_attr[target=_param_constant7]
    %addmm_3 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant7, %relu_1, %_frozen_param3), kwargs = {})
    return (addmm_2, addmm_3)
DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Supported Nodes:
- torch.ops.aten.reshape.default + Operator Count: 1
- torch.ops.aten.addmm.default + Operator Count: 4
- torch.ops.aten.relu.default + Operator Count: 2

DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
All Nodes Supported

DEBUG:torch_tensorrt.dynamo._compiler:Detected support for 7 operators out of 7 in subgraph.
WARNING:torch_tensorrt.dynamo._compiler:Node _param_constant1 of op type get_attr does not have metadata. This could sometimes lead to undefined behavior.
WARNING:torch_tensorrt.dynamo._compiler:Some nodes do not have metadata (shape and dtype information). This could lead to problems sometimes if the graph has PyTorch and TensorRT segments.
DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
Number of TensorRT-Accelerated Engines Generated: 1
DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
Supported Nodes:
- torch.ops.aten.reshape.default + Operator Count: 1
- torch.ops.aten.addmm.default + Operator Count: 4
- torch.ops.aten.relu.default + Operator Count: 2

DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
All Nodes Supported

DEBUG:torch_tensorrt.dynamo._compiler:Submodule name: _run_on_acc_0
 Input shapes: [(1000, 256, 7, 7)]
 graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %reshape_default : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%arg0_1, [1000, 12544]), kwargs = {})
    %_param_constant1 : [num_users=1] = get_attr[target=_param_constant1]
    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]
    %addmm : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant1, %reshape_default, %_frozen_param0), kwargs = {})
    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%addmm,), kwargs = {})
    %_param_constant3 : [num_users=1] = get_attr[target=_param_constant3]
    %_frozen_param1 : [num_users=1] = get_attr[target=_frozen_param1]
    %addmm_1 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant3, %relu, %_frozen_param1), kwargs = {})
    %relu_1 : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%addmm_1,), kwargs = {})
    %_param_constant5 : [num_users=1] = get_attr[target=_param_constant5]
    %_frozen_param2 : [num_users=1] = get_attr[target=_frozen_param2]
    %addmm_2 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant5, %relu_1, %_frozen_param2), kwargs = {})
    %_param_constant7 : [num_users=1] = get_attr[target=_param_constant7]
    %_frozen_param3 : [num_users=1] = get_attr[target=_frozen_param3]
    %addmm_3 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%_param_constant7, %relu_1, %_frozen_param3), kwargs = {})
    return (addmm_2, addmm_3)
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Adding input to in-progress INetwork: arg0_1 [shape=[1000, 256, 7, 7], dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node reshape_default (kind: aten.reshape.default, args: ('arg0_1 &lt;tensorrt.ITensor [shape=(1000, 256, 7, 7), dtype=DataType.FLOAT]&gt;', [1000, 12544]))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node addmm (kind: aten.addmm.default, args: ('&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '[SHUFFLE]-[aten_ops.reshape.default]-[reshape_default]_output &lt;tensorrt.ITensor [shape=(1000, 12544), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(12544, 1024), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion.converter_utils:Freezing tensor addmm_constant_0 to TRT IConstantLayer
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.addmm.default]-[addmm_add]_output_addmm.default &lt;tensorrt.ITensor [shape=(1000, 1024), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node addmm_1 (kind: aten.addmm.default, args: ('&lt;torch.Tensor as np.ndarray [shape=(1024,), dtype=float32]&gt;', '[RELU]-[aten_ops.relu.default]-[relu]_output &lt;tensorrt.ITensor [shape=(1000, 1024), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 1024), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion.converter_utils:Freezing tensor addmm_1_constant_0 to TRT IConstantLayer
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node relu_1 (kind: aten.relu.default, args: ('[ELEMENTWISE]-[aten_ops.addmm.default]-[addmm_1_add]_output_addmm.default &lt;tensorrt.ITensor [shape=(1000, 1024), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node addmm_2 (kind: aten.addmm.default, args: ('&lt;torch.Tensor as np.ndarray [shape=(2,), dtype=float32]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_1]_output &lt;tensorrt.ITensor [shape=(1000, 1024), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 2), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion.converter_utils:Freezing tensor addmm_2_constant_0 to TRT IConstantLayer
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node addmm_3 (kind: aten.addmm.default, args: ('&lt;torch.Tensor as np.ndarray [shape=(8,), dtype=float32]&gt;', '[RELU]-[aten_ops.relu.default]-[relu_1]_output &lt;tensorrt.ITensor [shape=(1000, 1024), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(1024, 8), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion.converter_utils:Freezing tensor addmm_3_constant_0 to TRT IConstantLayer
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Marking output output0 [shape=(1000, 2), dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Marking output output1 [shape=(1000, 8), dtype=DataType.FLOAT]
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT INetwork construction elapsed time: 0:00:00.163414
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Build TRT engine elapsed time: 0:00:10.052012
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT Engine uses: 56201444 bytes of Memory
DEBUG:torch_tensorrt.dynamo._DryRunTracker:
++++++++++++++++++++++++++++++++++++++++++++++++++ Dry-Run Results for Graph ++++++++++++++++++++++++++++++++++++++++++++++++++

The graph consists of 7 Total Operators, of which 7 operators are supported, 100.0% coverage

Compiled with: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

  Graph Structure:

   Inputs: List[Tensor: (1000, 256, 7, 7)@float32]
    ...
    TRT Engine #1 - Submodule name: _run_on_acc_0
     Engine Inputs: List[Tensor: (1000, 256, 7, 7)@float32]
     Number of Operators in Engine: 7
     Engine Outputs: Tuple(Tensor: (1000, 2)@float32, Tensor: (1000, 8)@float32)
    ...
   Outputs: Tuple(Tensor: (1000, 2)@float32, Tensor: (1000, 8)@float32)

  ------------------------- Aggregate Stats -------------------------

   Average Number of Operators per TRT Engine: 7.0
   Most Operators in a TRT Engine: 7

  ********** Recommendations **********

   - For minimal graph segmentation, select min_block_size=7 which would generate 1 TRT engine(s)
   - The current level of graph segmentation is equivalent to selecting min_block_size=7 which generates 1 TRT engine(s)
WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torch/_library/abstract_impl.py:127: UserWarning: create_unbacked_symint is deprecated, please use new_dynamic_size instead
  warnings.warn(

INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)
INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.
INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

DEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.batched_nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.lowering._repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.batched_nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.lowering._remove_sym_nodes:Removed SymInt placeholders:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.batched_nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 0 detach nodes:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.batched_nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:
graph():
    return ()
DEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:
graph():
    return ()
DEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:
 graph():
    return ()
DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Supported Nodes:

DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
All Nodes Supported

WARNING:torch_tensorrt.dynamo._compiler:0 supported operations detected in subgraph containing 0 computational nodes. Skipping this subgraph, since min_block_size was detected to be 7
WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torch/_library/abstract_impl.py:127: UserWarning: create_unbacked_symint is deprecated, please use new_dynamic_size instead
  warnings.warn(

INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)
INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.
INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

DEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:
graph():
    %l_boxes_ : torch.Tensor [num_users=4] = placeholder[target=L_boxes_]
    %l_idxs_ : torch.Tensor [num_users=1] = placeholder[target=L_idxs_]
    %max_coordinate : [num_users=1] = call_method[target=max](args = (%l_boxes_,), kwargs = {})
    %to : [num_users=1] = call_method[target=to](args = (%l_idxs_, %l_boxes_), kwargs = {})
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (1,), kwargs = {})
    %to_1 : [num_users=1] = call_method[target=to](args = (%tensor, %l_boxes_), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%max_coordinate, %to_1), kwargs = {})
    %offsets : [num_users=1] = call_function[target=operator.mul](args = (%to, %add), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%offsets, (slice(None, None, None), None)), kwargs = {})
    %boxes_for_nms : [num_users=1] = call_function[target=operator.add](args = (%l_boxes_, %getitem), kwargs = {})
    return (boxes_for_nms,)
DEBUG:torch_tensorrt.dynamo.lowering._repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:
graph():
    %l_boxes_ : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_idxs_ : torch.Tensor [num_users=1] = placeholder[target=L_idxs_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_idxs_,), kwargs = {})
    %clone_default : [num_users=4] = call_function[target=torch.ops.aten.clone.default](args = (%l_boxes_,), kwargs = {})
    %max_coordinate : [num_users=1] = call_method[target=max](args = (%clone_default,), kwargs = {})
    %to : [num_users=1] = call_method[target=to](args = (%clone_default_1, %clone_default), kwargs = {})
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (1,), kwargs = {})
    %to_1 : [num_users=1] = call_method[target=to](args = (%tensor, %clone_default), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%max_coordinate, %to_1), kwargs = {})
    %offsets : [num_users=1] = call_function[target=operator.mul](args = (%to, %add), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%offsets, (slice(None, None, None), None)), kwargs = {})
    %boxes_for_nms : [num_users=1] = call_function[target=operator.add](args = (%clone_default, %getitem), kwargs = {})
    return (boxes_for_nms,)
DEBUG:torch_tensorrt.dynamo.lowering._remove_sym_nodes:Removed SymInt placeholders:
graph():
    %l_boxes_ : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_idxs_ : torch.Tensor [num_users=1] = placeholder[target=L_idxs_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_idxs_,), kwargs = {})
    %clone_default : [num_users=4] = call_function[target=torch.ops.aten.clone.default](args = (%l_boxes_,), kwargs = {})
    %max_coordinate : [num_users=1] = call_method[target=max](args = (%clone_default,), kwargs = {})
    %to : [num_users=1] = call_method[target=to](args = (%clone_default_1, %clone_default), kwargs = {})
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (1,), kwargs = {})
    %to_1 : [num_users=1] = call_method[target=to](args = (%tensor, %clone_default), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%max_coordinate, %to_1), kwargs = {})
    %offsets : [num_users=1] = call_function[target=operator.mul](args = (%to, %add), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%offsets, (slice(None, None, None), None)), kwargs = {})
    %boxes_for_nms : [num_users=1] = call_function[target=operator.add](args = (%clone_default, %getitem), kwargs = {})
    return (boxes_for_nms,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 0 detach nodes:
graph():
    %l_boxes_ : torch.Tensor [num_users=1] = placeholder[target=L_boxes_]
    %l_idxs_ : torch.Tensor [num_users=1] = placeholder[target=L_idxs_]
    %clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%l_idxs_,), kwargs = {})
    %clone_default : [num_users=4] = call_function[target=torch.ops.aten.clone.default](args = (%l_boxes_,), kwargs = {})
    %max_coordinate : [num_users=1] = call_method[target=max](args = (%clone_default,), kwargs = {})
    %to : [num_users=1] = call_method[target=to](args = (%clone_default_1, %clone_default), kwargs = {})
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (1,), kwargs = {})
    %to_1 : [num_users=1] = call_method[target=to](args = (%tensor, %clone_default), kwargs = {})
    %add : [num_users=1] = call_function[target=operator.add](args = (%max_coordinate, %to_1), kwargs = {})
    %offsets : [num_users=1] = call_function[target=operator.mul](args = (%to, %add), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%offsets, (slice(None, None, None), None)), kwargs = {})
    %boxes_for_nms : [num_users=1] = call_function[target=operator.add](args = (%clone_default, %getitem), kwargs = {})
    return (boxes_for_nms,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg1_1,), kwargs = {})
    %clone_1 : [num_users=2] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {})
    %max_1 : [num_users=1] = call_function[target=torch.ops.aten.max.default](args = (%clone_1,), kwargs = {})
    %_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%clone,), kwargs = {dtype: torch.float32, device: cuda:0})
    %_tensor_constant0 : [num_users=1] = get_attr[target=_tensor_constant0]
    %lift_fresh_copy : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant0,), kwargs = {})
    %_to_copy_1 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%lift_fresh_copy,), kwargs = {dtype: torch.float32, device: cuda:0})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%max_1, %_to_copy_1), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_to_copy, %add), kwargs = {})
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%mul, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_1, %unsqueeze), kwargs = {})
    return (add_1,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone_1 from graph, since it is a clone node which is the only user of placeholder arg0_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone from graph, since it is a clone node which is the only user of placeholder arg1_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removed auxiliary clone nodes for placeholders:
graph():
    %arg0_1 : [num_users=2] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %max_1 : [num_users=1] = call_function[target=torch.ops.aten.max.default](args = (%arg0_1,), kwargs = {})
    %_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%arg1_1,), kwargs = {dtype: torch.float32, device: cuda:0})
    %_tensor_constant0 : [num_users=1] = get_attr[target=_tensor_constant0]
    %lift_fresh_copy : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant0,), kwargs = {})
    %_to_copy_1 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%lift_fresh_copy,), kwargs = {dtype: torch.float32, device: cuda:0})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%max_1, %_to_copy_1), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_to_copy, %add), kwargs = {})
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%mul, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg0_1, %unsqueeze), kwargs = {})
    return (add_1,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:
graph():
    %arg0_1 : [num_users=2] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %max_1 : [num_users=1] = call_function[target=torch.ops.aten.max.default](args = (%arg0_1,), kwargs = {})
    %_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%arg1_1,), kwargs = {dtype: torch.float32, device: cuda:0})
    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%max_1, %_frozen_param0), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_to_copy, %add), kwargs = {})
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%mul, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg0_1, %unsqueeze), kwargs = {})
    return (add_1,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:
 graph():
    %arg0_1 : [num_users=2] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %max_1 : [num_users=1] = call_function[target=torch.ops.aten.max.default](args = (%arg0_1,), kwargs = {})
    %_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%arg1_1,), kwargs = {dtype: torch.float32, device: cuda:0})
    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%max_1, %_frozen_param0), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_to_copy, %add), kwargs = {})
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%mul, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg0_1, %unsqueeze), kwargs = {})
    return (add_1,)
DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Supported Nodes:
- torch.ops.aten.max.default + Operator Count: 1
- torch.ops.aten._to_copy.default + Operator Count: 1
- torch.ops.aten.add.Tensor + Operator Count: 2
- torch.ops.aten.mul.Tensor + Operator Count: 1
- torch.ops.aten.slice.Tensor + Operator Count: 1
- torch.ops.aten.unsqueeze.default + Operator Count: 1

DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
All Nodes Supported

DEBUG:torch_tensorrt.dynamo._compiler:Detected support for 7 operators out of 7 in subgraph.
DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
Number of TensorRT-Accelerated Engines Generated: 1
DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
Supported Nodes:
- torch.ops.aten.max.default + Operator Count: 1
- torch.ops.aten._to_copy.default + Operator Count: 1
- torch.ops.aten.add.Tensor + Operator Count: 2
- torch.ops.aten.mul.Tensor + Operator Count: 1
- torch.ops.aten.slice.Tensor + Operator Count: 1
- torch.ops.aten.unsqueeze.default + Operator Count: 1

DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
All Nodes Supported

DEBUG:torch_tensorrt.dynamo._compiler:Submodule name: _run_on_acc_0
 Input shapes: [(454, 4), (454,)]
 graph():
    %arg0_1 : [num_users=2] = placeholder[target=arg0_1]
    %max_1 : [num_users=1] = call_function[target=torch.ops.aten.max.default](args = (%arg0_1,), kwargs = {})
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%arg1_1,), kwargs = {dtype: torch.float32, device: cuda:0})
    %_frozen_param0 : [num_users=1] = get_attr[target=_frozen_param0]
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%max_1, %_frozen_param0), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_to_copy, %add), kwargs = {})
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%mul, 0, 0, 9223372036854775807), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_1, 1), kwargs = {})
    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg0_1, %unsqueeze), kwargs = {})
    return add_1
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Adding input to in-progress INetwork: arg0_1 [shape=[454, 4], dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node max_1 (kind: aten.max.default, args: ('arg0_1 &lt;tensorrt.ITensor [shape=(454, 4), dtype=DataType.FLOAT]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Adding input to in-progress INetwork: arg1_1 [shape=[454], dtype=DataType.INT64]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node _to_copy (kind: aten._to_copy.default, args: ('arg1_1 &lt;tensorrt.ITensor [shape=(454,), dtype=DataType.INT64]&gt;',))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add (kind: aten.add.Tensor, args: ('[REDUCE]-[aten_ops.max.default]-[max_1]_output &lt;tensorrt.ITensor [shape=(), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node mul (kind: aten.mul.Tensor, args: ('Forced Cast ITensor arg1_1 from DataType.INT64 to DataType.FLOAT - [aten_ops.torch.ops.aten._to_copy.default]-[_to_copy]_output &lt;tensorrt.ITensor [shape=(454,), dtype=DataType.FLOAT]&gt;', '[ELEMENTWISE]-[aten_ops.add.Tensor]-[add]_output_add.Tensor &lt;tensorrt.ITensor [shape=(), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node slice_1 (kind: aten.slice.Tensor, args: ('[ELEMENTWISE]-[aten_ops.mul.Tensor]-[mul]_output_mul.Tensor &lt;tensorrt.ITensor [shape=(454,), dtype=DataType.FLOAT]&gt;', 0, 0, 9223372036854775807))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node unsqueeze (kind: aten.unsqueeze.default, args: ('[SLICE]-[unknown_ir_ops.slice.Tensor]-[slice_1]_output &lt;tensorrt.ITensor [shape=(454,), dtype=DataType.FLOAT]&gt;', 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node add_1 (kind: aten.add.Tensor, args: ('arg0_1 &lt;tensorrt.ITensor [shape=(454, 4), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.unsqueeze.default]-[unsqueeze]_output &lt;tensorrt.ITensor [shape=(454, 1), dtype=DataType.FLOAT]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Marking output output0 [shape=(454, 4), dtype=DataType.FLOAT]
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT INetwork construction elapsed time: 0:00:00.018059
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Build TRT engine elapsed time: 0:00:00.152821
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT Engine uses: 16060 bytes of Memory
DEBUG:torch_tensorrt.dynamo._DryRunTracker:
++++++++++++++++++++++++++++++++++++++++++++++++++ Dry-Run Results for Graph ++++++++++++++++++++++++++++++++++++++++++++++++++

The graph consists of 7 Total Operators, of which 7 operators are supported, 100.0% coverage

Compiled with: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

  Graph Structure:

   Inputs: List[Tensor: (454, 4)@float32, Tensor: (454,)@int64]
    ...
    TRT Engine #1 - Submodule name: _run_on_acc_0
     Engine Inputs: List[Tensor: (454, 4)@float32, Tensor: (454,)@int64]
     Number of Operators in Engine: 7
     Engine Outputs: Tensor: (454, 4)@float32
    ...
   Outputs: Tuple(Tensor: (454, 4)@float32)

  ------------------------- Aggregate Stats -------------------------

   Average Number of Operators per TRT Engine: 7.0
   Most Operators in a TRT Engine: 7

  ********** Recommendations **********

   - For minimal graph segmentation, select min_block_size=7 which would generate 1 TRT engine(s)
   - The current level of graph segmentation is equivalent to selecting min_block_size=7 which generates 1 TRT engine(s)
WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torch/_library/abstract_impl.py:127: UserWarning: create_unbacked_symint is deprecated, please use new_dynamic_size instead
  warnings.warn(

INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)
INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.
INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

DEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.lowering._repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.lowering._remove_sym_nodes:Removed SymInt placeholders:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 0 detach nodes:
graph():
    %_log_api_usage_once : [num_users=0] = call_function[target=torch._C._log_api_usage_once](args = (torchvision.ops.boxes.nms,), kwargs = {})
    return ()
DEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:
graph():
    return ()
DEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:
graph():
    return ()
DEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:
 graph():
    return ()
DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Supported Nodes:

DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
All Nodes Supported

WARNING:torch_tensorrt.dynamo._compiler:0 supported operations detected in subgraph containing 0 computational nodes. Skipping this subgraph, since min_block_size was detected to be 7
INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)
INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.
INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

DEBUG:torch_tensorrt.dynamo.backend.backends:Pre-AOT Autograd graph:
graph():
    %boxes : torch.Tensor [num_users=1] = placeholder[target=L_stack0_0_0_boxes_]
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (200,), kwargs = {dtype: torch.float32, device: cuda:0})
    %tensor_1 : [num_users=1] = call_function[target=torch.tensor](args = (800,), kwargs = {dtype: torch.float32, device: cuda:0})
    %ratio_height : [num_users=2] = call_function[target=operator.truediv](args = (%tensor, %tensor_1), kwargs = {})
    %tensor_2 : [num_users=1] = call_function[target=torch.tensor](args = (200,), kwargs = {dtype: torch.float32, device: cuda:0})
    %tensor_3 : [num_users=1] = call_function[target=torch.tensor](args = (800,), kwargs = {dtype: torch.float32, device: cuda:0})
    %ratio_width : [num_users=2] = call_function[target=operator.truediv](args = (%tensor_2, %tensor_3), kwargs = {})
    %unbind : [num_users=4] = call_method[target=unbind](args = (%boxes, 1), kwargs = {})
    %xmin : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 0), kwargs = {})
    %ymin : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 1), kwargs = {})
    %xmax : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 2), kwargs = {})
    %ymax : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 3), kwargs = {})
    %xmin_1 : [num_users=1] = call_function[target=operator.mul](args = (%xmin, %ratio_width), kwargs = {})
    %xmax_1 : [num_users=1] = call_function[target=operator.mul](args = (%xmax, %ratio_width), kwargs = {})
    %ymin_1 : [num_users=1] = call_function[target=operator.mul](args = (%ymin, %ratio_height), kwargs = {})
    %ymax_1 : [num_users=1] = call_function[target=operator.mul](args = (%ymax, %ratio_height), kwargs = {})
    %boxes_1 : [num_users=1] = call_function[target=torch.stack](args = ((%xmin_1, %ymin_1, %xmax_1, %ymax_1),), kwargs = {dim: 1})
    return (boxes_1,)
DEBUG:torch_tensorrt.dynamo.lowering._repair_input_aliasing:Inserted auxiliary clone nodes for placeholders:
graph():
    %boxes : torch.Tensor [num_users=1] = placeholder[target=L_stack0_0_0_boxes_]
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%boxes,), kwargs = {})
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (200,), kwargs = {dtype: torch.float32, device: cuda:0})
    %tensor_1 : [num_users=1] = call_function[target=torch.tensor](args = (800,), kwargs = {dtype: torch.float32, device: cuda:0})
    %ratio_height : [num_users=2] = call_function[target=operator.truediv](args = (%tensor, %tensor_1), kwargs = {})
    %tensor_2 : [num_users=1] = call_function[target=torch.tensor](args = (200,), kwargs = {dtype: torch.float32, device: cuda:0})
    %tensor_3 : [num_users=1] = call_function[target=torch.tensor](args = (800,), kwargs = {dtype: torch.float32, device: cuda:0})
    %ratio_width : [num_users=2] = call_function[target=operator.truediv](args = (%tensor_2, %tensor_3), kwargs = {})
    %unbind : [num_users=4] = call_method[target=unbind](args = (%clone_default, 1), kwargs = {})
    %xmin : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 0), kwargs = {})
    %ymin : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 1), kwargs = {})
    %xmax : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 2), kwargs = {})
    %ymax : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 3), kwargs = {})
    %xmin_1 : [num_users=1] = call_function[target=operator.mul](args = (%xmin, %ratio_width), kwargs = {})
    %xmax_1 : [num_users=1] = call_function[target=operator.mul](args = (%xmax, %ratio_width), kwargs = {})
    %ymin_1 : [num_users=1] = call_function[target=operator.mul](args = (%ymin, %ratio_height), kwargs = {})
    %ymax_1 : [num_users=1] = call_function[target=operator.mul](args = (%ymax, %ratio_height), kwargs = {})
    %boxes_1 : [num_users=1] = call_function[target=torch.stack](args = ((%xmin_1, %ymin_1, %xmax_1, %ymax_1),), kwargs = {dim: 1})
    return (boxes_1,)
DEBUG:torch_tensorrt.dynamo.lowering._remove_sym_nodes:Removed SymInt placeholders:
graph():
    %boxes : torch.Tensor [num_users=1] = placeholder[target=L_stack0_0_0_boxes_]
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%boxes,), kwargs = {})
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (200,), kwargs = {dtype: torch.float32, device: cuda:0})
    %tensor_1 : [num_users=1] = call_function[target=torch.tensor](args = (800,), kwargs = {dtype: torch.float32, device: cuda:0})
    %ratio_height : [num_users=2] = call_function[target=operator.truediv](args = (%tensor, %tensor_1), kwargs = {})
    %tensor_2 : [num_users=1] = call_function[target=torch.tensor](args = (200,), kwargs = {dtype: torch.float32, device: cuda:0})
    %tensor_3 : [num_users=1] = call_function[target=torch.tensor](args = (800,), kwargs = {dtype: torch.float32, device: cuda:0})
    %ratio_width : [num_users=2] = call_function[target=operator.truediv](args = (%tensor_2, %tensor_3), kwargs = {})
    %unbind : [num_users=4] = call_method[target=unbind](args = (%clone_default, 1), kwargs = {})
    %xmin : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 0), kwargs = {})
    %ymin : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 1), kwargs = {})
    %xmax : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 2), kwargs = {})
    %ymax : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 3), kwargs = {})
    %xmin_1 : [num_users=1] = call_function[target=operator.mul](args = (%xmin, %ratio_width), kwargs = {})
    %xmax_1 : [num_users=1] = call_function[target=operator.mul](args = (%xmax, %ratio_width), kwargs = {})
    %ymin_1 : [num_users=1] = call_function[target=operator.mul](args = (%ymin, %ratio_height), kwargs = {})
    %ymax_1 : [num_users=1] = call_function[target=operator.mul](args = (%ymax, %ratio_height), kwargs = {})
    %boxes_1 : [num_users=1] = call_function[target=torch.stack](args = ((%xmin_1, %ymin_1, %xmax_1, %ymax_1),), kwargs = {dim: 1})
    return (boxes_1,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_detach:Removed 0 detach nodes:
graph():
    %boxes : torch.Tensor [num_users=1] = placeholder[target=L_stack0_0_0_boxes_]
    %clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%boxes,), kwargs = {})
    %tensor : [num_users=1] = call_function[target=torch.tensor](args = (200,), kwargs = {dtype: torch.float32, device: cuda:0})
    %tensor_1 : [num_users=1] = call_function[target=torch.tensor](args = (800,), kwargs = {dtype: torch.float32, device: cuda:0})
    %ratio_height : [num_users=2] = call_function[target=operator.truediv](args = (%tensor, %tensor_1), kwargs = {})
    %tensor_2 : [num_users=1] = call_function[target=torch.tensor](args = (200,), kwargs = {dtype: torch.float32, device: cuda:0})
    %tensor_3 : [num_users=1] = call_function[target=torch.tensor](args = (800,), kwargs = {dtype: torch.float32, device: cuda:0})
    %ratio_width : [num_users=2] = call_function[target=operator.truediv](args = (%tensor_2, %tensor_3), kwargs = {})
    %unbind : [num_users=4] = call_method[target=unbind](args = (%clone_default, 1), kwargs = {})
    %xmin : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 0), kwargs = {})
    %ymin : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 1), kwargs = {})
    %xmax : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 2), kwargs = {})
    %ymax : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 3), kwargs = {})
    %xmin_1 : [num_users=1] = call_function[target=operator.mul](args = (%xmin, %ratio_width), kwargs = {})
    %xmax_1 : [num_users=1] = call_function[target=operator.mul](args = (%xmax, %ratio_width), kwargs = {})
    %ymin_1 : [num_users=1] = call_function[target=operator.mul](args = (%ymin, %ratio_height), kwargs = {})
    %ymax_1 : [num_users=1] = call_function[target=operator.mul](args = (%ymax, %ratio_height), kwargs = {})
    %boxes_1 : [num_users=1] = call_function[target=torch.stack](args = ((%xmin_1, %ymin_1, %xmax_1, %ymax_1),), kwargs = {dim: 1})
    return (boxes_1,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Post-AOT Autograd graph:
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %clone : [num_users=4] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {})
    %_tensor_constant0 : [num_users=1] = get_attr[target=_tensor_constant0]
    %lift_fresh_copy : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant0,), kwargs = {})
    %_tensor_constant1 : [num_users=1] = get_attr[target=_tensor_constant1]
    %lift_fresh_copy_1 : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant1,), kwargs = {})
    %div : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%lift_fresh_copy, %lift_fresh_copy_1), kwargs = {})
    %_tensor_constant2 : [num_users=1] = get_attr[target=_tensor_constant2]
    %lift_fresh_copy_2 : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant2,), kwargs = {})
    %_tensor_constant3 : [num_users=1] = get_attr[target=_tensor_constant3]
    %lift_fresh_copy_3 : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant3,), kwargs = {})
    %div_1 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%lift_fresh_copy_2, %lift_fresh_copy_3), kwargs = {})
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%clone, 1, 0, 1), kwargs = {})
    %slice_2 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%clone, 1, 1, 2), kwargs = {})
    %slice_3 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%clone, 1, 2, 3), kwargs = {})
    %slice_4 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%clone, 1, 3, 4), kwargs = {})
    %squeeze : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_1, 1), kwargs = {})
    %squeeze_1 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_2, 1), kwargs = {})
    %squeeze_2 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_3, 1), kwargs = {})
    %squeeze_3 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_4, 1), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze, %div_1), kwargs = {})
    %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_2, %div_1), kwargs = {})
    %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_1, %div), kwargs = {})
    %mul_3 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_3, %div), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul, 1), kwargs = {})
    %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_2, 1), kwargs = {})
    %unsqueeze_2 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1, 1), kwargs = {})
    %unsqueeze_3 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_3, 1), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze, %unsqueeze_1, %unsqueeze_2, %unsqueeze_3], 1), kwargs = {})
    return (cat,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removing node clone from graph, since it is a clone node which is the only user of placeholder arg0_1 and was inserted by the compiler.
DEBUG:torch_tensorrt.dynamo.lowering.passes.remove_input_alias_fixing_clones:Removed auxiliary clone nodes for placeholders:
graph():
    %arg0_1 : [num_users=4] = placeholder[target=arg0_1]
    %_tensor_constant0 : [num_users=1] = get_attr[target=_tensor_constant0]
    %lift_fresh_copy : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant0,), kwargs = {})
    %_tensor_constant1 : [num_users=1] = get_attr[target=_tensor_constant1]
    %lift_fresh_copy_1 : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant1,), kwargs = {})
    %div : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%lift_fresh_copy, %lift_fresh_copy_1), kwargs = {})
    %_tensor_constant2 : [num_users=1] = get_attr[target=_tensor_constant2]
    %lift_fresh_copy_2 : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant2,), kwargs = {})
    %_tensor_constant3 : [num_users=1] = get_attr[target=_tensor_constant3]
    %lift_fresh_copy_3 : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_tensor_constant3,), kwargs = {})
    %div_1 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%lift_fresh_copy_2, %lift_fresh_copy_3), kwargs = {})
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 0, 1), kwargs = {})
    %slice_2 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 1, 2), kwargs = {})
    %slice_3 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 2, 3), kwargs = {})
    %slice_4 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 3, 4), kwargs = {})
    %squeeze : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_1, 1), kwargs = {})
    %squeeze_1 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_2, 1), kwargs = {})
    %squeeze_2 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_3, 1), kwargs = {})
    %squeeze_3 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_4, 1), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze, %div_1), kwargs = {})
    %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_2, %div_1), kwargs = {})
    %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_1, %div), kwargs = {})
    %mul_3 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_3, %div), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul, 1), kwargs = {})
    %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_2, 1), kwargs = {})
    %unsqueeze_2 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1, 1), kwargs = {})
    %unsqueeze_3 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_3, 1), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze, %unsqueeze_1, %unsqueeze_2, %unsqueeze_3], 1), kwargs = {})
    return (cat,)
DEBUG:torch_tensorrt.dynamo.lowering.passes.constant_folding:Graph after constant folding:
graph():
    %arg0_1 : [num_users=4] = placeholder[target=arg0_1]
    %_frozen_param0 : [num_users=2] = get_attr[target=_frozen_param0]
    %_frozen_param1 : [num_users=2] = get_attr[target=_frozen_param1]
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 0, 1), kwargs = {})
    %slice_2 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 1, 2), kwargs = {})
    %slice_3 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 2, 3), kwargs = {})
    %slice_4 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 3, 4), kwargs = {})
    %squeeze : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_1, 1), kwargs = {})
    %squeeze_1 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_2, 1), kwargs = {})
    %squeeze_2 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_3, 1), kwargs = {})
    %squeeze_3 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_4, 1), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze, %_frozen_param1), kwargs = {})
    %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_2, %_frozen_param1), kwargs = {})
    %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_1, %_frozen_param0), kwargs = {})
    %mul_3 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_3, %_frozen_param0), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul, 1), kwargs = {})
    %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_2, 1), kwargs = {})
    %unsqueeze_2 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1, 1), kwargs = {})
    %unsqueeze_3 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_3, 1), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze, %unsqueeze_1, %unsqueeze_2, %unsqueeze_3], 1), kwargs = {})
    return (cat,)
DEBUG:torch_tensorrt.dynamo.backend.backends:Lowered Input graph:
 graph():
    %arg0_1 : [num_users=4] = placeholder[target=arg0_1]
    %_frozen_param0 : [num_users=2] = get_attr[target=_frozen_param0]
    %_frozen_param1 : [num_users=2] = get_attr[target=_frozen_param1]
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 0, 1), kwargs = {})
    %slice_2 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 1, 2), kwargs = {})
    %slice_3 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 2, 3), kwargs = {})
    %slice_4 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 3, 4), kwargs = {})
    %squeeze : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_1, 1), kwargs = {})
    %squeeze_1 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_2, 1), kwargs = {})
    %squeeze_2 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_3, 1), kwargs = {})
    %squeeze_3 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_4, 1), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze, %_frozen_param1), kwargs = {})
    %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_2, %_frozen_param1), kwargs = {})
    %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_1, %_frozen_param0), kwargs = {})
    %mul_3 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_3, %_frozen_param0), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul, 1), kwargs = {})
    %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_2, 1), kwargs = {})
    %unsqueeze_2 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1, 1), kwargs = {})
    %unsqueeze_3 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_3, 1), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze, %unsqueeze_1, %unsqueeze_2, %unsqueeze_3], 1), kwargs = {})
    return (cat,)
DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
Supported Nodes:
- torch.ops.aten.slice.Tensor + Operator Count: 4
- torch.ops.aten.squeeze.dim + Operator Count: 4
- torch.ops.aten.mul.Tensor + Operator Count: 4
- torch.ops.aten.unsqueeze.default + Operator Count: 4
- torch.ops.aten.cat.default + Operator Count: 1

DEBUG:torch_tensorrt.dynamo.partitioning._global_partitioner:
All Nodes Supported

DEBUG:torch_tensorrt.dynamo._compiler:Detected support for 17 operators out of 17 in subgraph.
DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
Number of TensorRT-Accelerated Engines Generated: 1
DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
Supported Nodes:
- torch.ops.aten.slice.Tensor + Operator Count: 4
- torch.ops.aten.squeeze.dim + Operator Count: 4
- torch.ops.aten.mul.Tensor + Operator Count: 4
- torch.ops.aten.unsqueeze.default + Operator Count: 4
- torch.ops.aten.cat.default + Operator Count: 1

DEBUG:torch_tensorrt.dynamo.partitioning._adjacency_partitioner:
All Nodes Supported

DEBUG:torch_tensorrt.dynamo._compiler:Submodule name: _run_on_acc_0
 Input shapes: [(100, 4)]
 graph():
    %arg0_1 : [num_users=4] = placeholder[target=arg0_1]
    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 0, 1), kwargs = {})
    %slice_2 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 1, 2), kwargs = {})
    %slice_3 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 2, 3), kwargs = {})
    %slice_4 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%arg0_1, 1, 3, 4), kwargs = {})
    %squeeze : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_1, 1), kwargs = {})
    %squeeze_1 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_2, 1), kwargs = {})
    %squeeze_2 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_3, 1), kwargs = {})
    %squeeze_3 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%slice_4, 1), kwargs = {})
    %_frozen_param1 : [num_users=2] = get_attr[target=_frozen_param1]
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze, %_frozen_param1), kwargs = {})
    %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_2, %_frozen_param1), kwargs = {})
    %_frozen_param0 : [num_users=2] = get_attr[target=_frozen_param0]
    %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_1, %_frozen_param0), kwargs = {})
    %mul_3 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%squeeze_3, %_frozen_param0), kwargs = {})
    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul, 1), kwargs = {})
    %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_2, 1), kwargs = {})
    %unsqueeze_2 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_1, 1), kwargs = {})
    %unsqueeze_3 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%mul_3, 1), kwargs = {})
    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze, %unsqueeze_1, %unsqueeze_2, %unsqueeze_3], 1), kwargs = {})
    return cat
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Adding input to in-progress INetwork: arg0_1 [shape=[100, 4], dtype=DataType.FLOAT]
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node slice_1 (kind: aten.slice.Tensor, args: ('arg0_1 &lt;tensorrt.ITensor [shape=(100, 4), dtype=DataType.FLOAT]&gt;', 1, 0, 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node slice_2 (kind: aten.slice.Tensor, args: ('arg0_1 &lt;tensorrt.ITensor [shape=(100, 4), dtype=DataType.FLOAT]&gt;', 1, 1, 2))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node slice_3 (kind: aten.slice.Tensor, args: ('arg0_1 &lt;tensorrt.ITensor [shape=(100, 4), dtype=DataType.FLOAT]&gt;', 1, 2, 3))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node slice_4 (kind: aten.slice.Tensor, args: ('arg0_1 &lt;tensorrt.ITensor [shape=(100, 4), dtype=DataType.FLOAT]&gt;', 1, 3, 4))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node squeeze (kind: aten.squeeze.dim, args: ('[SLICE]-[unknown_ir_ops.slice.Tensor]-[slice_1]_output &lt;tensorrt.ITensor [shape=(100, 1), dtype=DataType.FLOAT]&gt;', 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node squeeze_1 (kind: aten.squeeze.dim, args: ('[SLICE]-[unknown_ir_ops.slice.Tensor]-[slice_2]_output &lt;tensorrt.ITensor [shape=(100, 1), dtype=DataType.FLOAT]&gt;', 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node squeeze_2 (kind: aten.squeeze.dim, args: ('[SLICE]-[unknown_ir_ops.slice.Tensor]-[slice_3]_output &lt;tensorrt.ITensor [shape=(100, 1), dtype=DataType.FLOAT]&gt;', 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node squeeze_3 (kind: aten.squeeze.dim, args: ('[SLICE]-[unknown_ir_ops.slice.Tensor]-[slice_4]_output &lt;tensorrt.ITensor [shape=(100, 1), dtype=DataType.FLOAT]&gt;', 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node mul (kind: aten.mul.Tensor, args: ('[SHUFFLE]-[aten_ops.squeeze.dim]-[squeeze]_output &lt;tensorrt.ITensor [shape=(100,), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node mul_1 (kind: aten.mul.Tensor, args: ('[SHUFFLE]-[aten_ops.squeeze.dim]-[squeeze_2]_output &lt;tensorrt.ITensor [shape=(100,), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node mul_2 (kind: aten.mul.Tensor, args: ('[SHUFFLE]-[aten_ops.squeeze.dim]-[squeeze_1]_output &lt;tensorrt.ITensor [shape=(100,), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node mul_3 (kind: aten.mul.Tensor, args: ('[SHUFFLE]-[aten_ops.squeeze.dim]-[squeeze_3]_output &lt;tensorrt.ITensor [shape=(100,), dtype=DataType.FLOAT]&gt;', '&lt;torch.Tensor as np.ndarray [shape=(), dtype=float32]&gt;'))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node unsqueeze (kind: aten.unsqueeze.default, args: ('[ELEMENTWISE]-[aten_ops.mul.Tensor]-[mul]_output_mul.Tensor &lt;tensorrt.ITensor [shape=(100,), dtype=DataType.FLOAT]&gt;', 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node unsqueeze_1 (kind: aten.unsqueeze.default, args: ('[ELEMENTWISE]-[aten_ops.mul.Tensor]-[mul_2]_output_mul.Tensor &lt;tensorrt.ITensor [shape=(100,), dtype=DataType.FLOAT]&gt;', 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node unsqueeze_2 (kind: aten.unsqueeze.default, args: ('[ELEMENTWISE]-[aten_ops.mul.Tensor]-[mul_1]_output_mul.Tensor &lt;tensorrt.ITensor [shape=(100,), dtype=DataType.FLOAT]&gt;', 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node unsqueeze_3 (kind: aten.unsqueeze.default, args: ('[ELEMENTWISE]-[aten_ops.mul.Tensor]-[mul_3]_output_mul.Tensor &lt;tensorrt.ITensor [shape=(100,), dtype=DataType.FLOAT]&gt;', 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Converting node cat (kind: aten.cat.default, args: (['[SHUFFLE]-[aten_ops.unsqueeze.default]-[unsqueeze]_output &lt;tensorrt.ITensor [shape=(100, 1), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.unsqueeze.default]-[unsqueeze_1]_output &lt;tensorrt.ITensor [shape=(100, 1), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.unsqueeze.default]-[unsqueeze_2]_output &lt;tensorrt.ITensor [shape=(100, 1), dtype=DataType.FLOAT]&gt;', '[SHUFFLE]-[aten_ops.unsqueeze.default]-[unsqueeze_3]_output &lt;tensorrt.ITensor [shape=(100, 1), dtype=DataType.FLOAT]&gt;'], 1))
DEBUG:torch_tensorrt.dynamo.conversion._TRTInterpreter:Marking output output0 [shape=(100, 4), dtype=DataType.FLOAT]
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT INetwork construction elapsed time: 0:00:00.040412
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Build TRT engine elapsed time: 0:00:00.147493
INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT Engine uses: 35956 bytes of Memory
DEBUG:torch_tensorrt.dynamo._DryRunTracker:
++++++++++++++++++++++++++++++++++++++++++++++++++ Dry-Run Results for Graph ++++++++++++++++++++++++++++++++++++++++++++++++++

The graph consists of 17 Total Operators, of which 17 operators are supported, 100.0% coverage

Compiled with: CompilationSettings(enabled_precisions={&lt;dtype.f32: 7&gt;}, debug=True, workspace_size=21474836480, min_block_size=7, torch_executed_ops={}, pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=&lt;EngineCapability.STANDARD: 1&gt;, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)

  Graph Structure:

   Inputs: List[Tensor: (100, 4)@float32]
    ...
    TRT Engine #1 - Submodule name: _run_on_acc_0
     Engine Inputs: List[Tensor: (100, 4)@float32]
     Number of Operators in Engine: 17
     Engine Outputs: Tensor: (100, 4)@float32
    ...
   Outputs: Tuple(Tensor: (100, 4)@float32)

  ------------------------- Aggregate Stats -------------------------

   Average Number of Operators per TRT Engine: 17.0
   Most Operators in a TRT Engine: 17

  ********** Recommendations **********

   - For minimal graph segmentation, select min_block_size=17 which would generate 1 TRT engine(s)
   - The current level of graph segmentation is equivalent to selecting min_block_size=17 which generates 1 TRT engine(s)</code></pre>
</div>
</div>
<div class="cell" data-outputid="95cc8134-36e8-486e-8bb6-232dfb42117c" data-execution_count="115">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>trt_time <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>o trt_model(<span class="op">*</span>inputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>26.1 ms ± 207 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre>
</div>
</div>
<div class="cell" data-outputid="90f3c99e-36be-4f17-b4f5-1d8a49d32e76" data-execution_count="117">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>runtime <span class="op">=</span> [</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'cpu'</span>,</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quant'</span>,</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'onnx'</span>,</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'gpu'</span>,</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'gpu half'</span>,</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'tensorrt'</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>latency <span class="op">=</span> [</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    cpu_time.average,</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    quant_time.average,</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    onnx_time.average,</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>    gpu_time.average,</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>    gpu_half_time.average,</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>    trt_time.average</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>latency <span class="op">=</span> [<span class="bu">round</span>(n, <span class="dv">3</span>) <span class="cf">for</span> n <span class="kw">in</span> latency]</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>ax.bar(runtime, latency)</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'latency (ms)'</span>)</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>ax.set_yscale(<span class="st">'log'</span>)</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="onnx_tensorrt_files/figure-html/cell-24-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>… half precision on the GPU is nearly as fast as TensorRT.. with TensorRT can also use half-precision to improve latency even more …</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot latency for all methods (bar chart)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>