<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="me">
<meta name="dcterms.date" content="2023-11-15">

<title>cbhyphen.github.io - Eager Mode Quantization in PyTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">cbhyphen.github.io</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">about</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cbhyphen" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Eager Mode Quantization in PyTorch</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>me </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 15, 2023</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">November 22, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>At the time of writing this quantization in PyTorch was relatively new to me, and I wanted a deep dive on the topic for something non-trivial. After reading about different <a href="https://pytorch.org/docs/stable/quantization.html#quantization-api-summary">quantization modes</a> (eager vs graph fx) as well as quantization methods (dynamic, static post-training, static aware-training), I decide to try eager mode quantization aware training (QAT).</p>
<p>In my research for a project, I came across <a href="https://github.com/pytorch/pytorch/issues/31316">multiple discussions</a> online requesting either <a href="https://discuss.pytorch.org/t/slow-inference-time-on-quantized-faster-rcnn-model/115182">help</a> or a <a href="https://discuss.pytorch.org/t/tutorial-on-quantizing-object-detection-model/102604">tutorial</a> for quantizing the backbone of an object detection model (faster R-CNN in this case). As far as I could tell there was nothing available so this was the perfect excuse.</p>
<p>So in this post, I will go through that process of quantization-aware training and include some analysis on the benefits of quantization.</p>
<section id="resnet-and-feature-pyramid-network" class="level3">
<h3 class="anchored" data-anchor-id="resnet-and-feature-pyramid-network">ResNet and Feature Pyramid Network</h3>
<p>This assumes some familiarity with the R-CNN architecture, but to refresh, the feature extraction backbone consists of two components; the resnet and the feature pyramid network. The FPN combines output from consecutive layers of the resnet (via upsampling) which allows it to extract semantic information at higher resolutions. These two components of the backbone can be quantized while the rest of the network still uses floating point precision.</p>
<p>From an implementation standpoint, there is a utility class <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/_utils.py#L13">IntermediateLayerGetter</a> for extracting each layer output (no fully connected) from the resnet. And another convenience class for the <a href="https://github.com/pytorch/vision/blob/main/torchvision/ops/feature_pyramid_network.py#L36">FPN</a> which takes the layer ouputs as input. Combining these two is <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/detection/backbone_utils.py#L13">BackboneWithFPN</a> which is mostly just a thin wrapper around both.</p>
<p>As we are doing eager mode static quantization, we’ll need to <a href="https://pytorch.org/docs/stable/quantization.html#model-preparation-for-eager-mode-static-quantization">prepare the model</a> before we can train it and subsequently quantize it.</p>
</section>
<section id="model-preparation" class="level3">
<h3 class="anchored" data-anchor-id="model-preparation">Model Preparation</h3>
<p>The first step in preparing the network for quantization is to create a modified <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L108">bottleneck block</a>. This isn’t obvious until you try to quantize the ResNet without it. You will get an error <code>.. out += identity .. Could not run 'aten::add.out' ..</code> which means that PyTorch isn’t able to quantize the skip connection using the <code>+=</code> operator in eager mode. <a href="https://discuss.pytorch.org/t/quantizing-an-existing-object-detector-with-resnet-backbone/134627">This discussion on the pytorch forums</a> was helpful for describing the error as well as how to fix it. The modified bottleneck block just uses <a href="https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.FloatFunctional.html">FloatFunctional</a> which has a quantized addition operator. I’m using ResNet 101 here but for much smaller networks you would want to modify the basic block. Also, the original bottleneck class reuses the ReLU layer which won’t work when fusing. Finding <a href="https://leimao.github.io/blog/PyTorch-Static-Quantization/">this blog post</a> about quantizing ResNet was helpful for realizing and avoiding that pitfall.</p>
<div class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional, Callable</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> Tensor</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.ao.nn.quantized <span class="im">import</span> FloatFunctional</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.resnet <span class="im">import</span> conv1x1, conv3x3</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BottleneckQuantizeable(nn.Module):</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># while original implementation places the stride at the first 1x1 convolution(self.conv1)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># according to "Deep residual learning for image recognition" https://arxiv.org/abs/1512.03385.</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This variant is also known as ResNet V1.5 and improves accuracy according to</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    expansion: <span class="bu">int</span> <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        inplanes: <span class="bu">int</span>,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        planes: <span class="bu">int</span>,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        stride: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        downsample: Optional[nn.Module] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        groups: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        base_width: <span class="bu">int</span> <span class="op">=</span> <span class="dv">64</span>,</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        dilation: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        norm_layer: Optional[Callable[..., nn.Module]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> norm_layer <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>            norm_layer <span class="op">=</span> nn.BatchNorm2d</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        width <span class="op">=</span> <span class="bu">int</span>(planes <span class="op">*</span> (base_width <span class="op">/</span> <span class="fl">64.0</span>)) <span class="op">*</span> groups</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Both self.conv2 and self.downsample layers downsample the input when stride != 1</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> conv1x1(inplanes, width)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> norm_layer(width)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu1 <span class="op">=</span> nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> conv3x3(width, width, stride, groups, dilation)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> norm_layer(width)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu2 <span class="op">=</span> nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> conv1x1(width, planes <span class="op">*</span> <span class="va">self</span>.expansion)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn3 <span class="op">=</span> norm_layer(planes <span class="op">*</span> <span class="va">self</span>.expansion)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu3 <span class="op">=</span> nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.downsample <span class="op">=</span> downsample</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stride <span class="op">=</span> stride</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_add <span class="op">=</span> FloatFunctional()</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        identity <span class="op">=</span> x</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn1(out)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu1(out)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv2(out)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn2(out)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu2(out)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv3(out)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.bn3(out)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.downsample <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>            identity <span class="op">=</span> <span class="va">self</span>.downsample(x)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.ff_add.add(out, identity)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.relu3(out)</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we have a quantizeable bottleneck, we can simply reference it when generating the ResNet. Even though the float functional operator was added, we can still load pretrained imagenet weights since the trainable submodules didn’t change. Note that the number of classes for the ResNet don’t matter here because we will extract intermediate layers and ignore the final fully connected layer.</p>
<div class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.resnet <span class="im">import</span> ResNet, ResNet50_Weights, ResNet101_Weights</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> resnet_101():</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    resnet <span class="op">=</span> ResNet(block<span class="op">=</span>BottleneckQuantizeable, layers<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">23</span>, <span class="dv">3</span>])</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    resnet.load_state_dict(ResNet101_Weights.DEFAULT.get_state_dict(progress<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> resnet</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>resnet <span class="op">=</span> resnet_101()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The next step would be to pass the resnet to the <code>IntermediateLayerGetter</code>. In addition to the resnet we created, this class also requires a dictionary of the layer names (to know what to extract). It returns an <code>OrderedDict</code> of those layer outputs. Here’s an example using a toy image.</p>
<div class="cell" data-outputid="32d9a6a0-a267-4fe1-e191-a1c51b5c00bc" data-execution_count="86">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models._utils <span class="im">import</span> IntermediateLayerGetter</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>returned_layers <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]  <span class="co"># get all 4 layers</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>return_layers <span class="op">=</span> {<span class="ss">f"layer</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>: <span class="bu">str</span>(v) <span class="cf">for</span> v, k <span class="kw">in</span> <span class="bu">enumerate</span>(returned_layers)}  <span class="co"># {'layer1': 0, 'layer2': 1, ...}</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>resnet_layers <span class="op">=</span> IntermediateLayerGetter(resnet, return_layers<span class="op">=</span>return_layers)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> resnet_layers(torch.rand(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">200</span>, <span class="dv">200</span>))  <span class="co"># e.g. 200 x 200 image with 3 channels</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>[(k, v.shape) <span class="cf">for</span> k, v <span class="kw">in</span> out.items()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="86">
<pre><code>[('0', torch.Size([1, 256, 50, 50])),
 ('1', torch.Size([1, 512, 25, 25])),
 ('2', torch.Size([1, 1024, 13, 13])),
 ('3', torch.Size([1, 2048, 7, 7]))]</code></pre>
</div>
</div>
<p>As mentioned, the output of the resnet layers will be fed to the feature pyramid network. Before we can do this, the FPN also needs to be modified as it uses the addition operator <code>+</code>. There is also a functional <code>F.interpolate</code> that doesn’t need to be replaced, however it does need to be referenced differently as importing <code>torch.nn.functional as F</code> causes a namespace issue later with <code>torchvision</code> and QAT will fail.</p>
<div class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> OrderedDict</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Dict</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># importing as 'F' causes namespace collision with torchvision and QAT fails later</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># import torch.nn.functional as F</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.ops.misc <span class="im">import</span> Conv2dNormActivation</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.utils <span class="im">import</span> _log_api_usage_once</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.ops.feature_pyramid_network <span class="im">import</span> ExtraFPNBlock</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeaturePyramidNetworkQuantizeable(nn.Module):</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Module that adds a FPN from on top of a set of feature maps. This is based on</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co">    `"Feature Pyramid Network for Object Detection" &lt;https://arxiv.org/abs/1612.03144&gt;`_.</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co">    The feature maps are currently supposed to be in increasing depth</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co">    order.</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co">    The input to the model is expected to be an OrderedDict[Tensor], containing</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="co">    the feature maps on top of which the FPN will be added.</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="co">        in_channels_list (list[int]): number of channels for each feature map that</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="co">            is passed to the module</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="co">        out_channels (int): number of channels of the FPN representation</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="co">        extra_blocks (ExtraFPNBlock or None): if provided, extra operations will</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="co">            be performed. It is expected to take the fpn features, the original</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="co">            features and the names of the original features as input, and returns</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="co">            a new list of feature maps and their corresponding names</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a><span class="co">        norm_layer (callable, optional): Module specifying the normalization layer to use. Default: None</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="co">    Examples::</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; m = torchvision.ops.FeaturePyramidNetwork([10, 20, 30], 5)</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; # get some dummy data</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; x = OrderedDict()</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; x['feat0'] = torch.rand(1, 10, 64, 64)</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; x['feat2'] = torch.rand(1, 20, 16, 16)</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; x['feat3'] = torch.rand(1, 30, 8, 8)</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; # compute the FPN on top of x</span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; output = m(x)</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; print([(k, v.shape) for k, v in output.items()])</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; # returns</span></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt;   [('feat0', torch.Size([1, 5, 64, 64])),</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt;    ('feat2', torch.Size([1, 5, 16, 16])),</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt;    ('feat3', torch.Size([1, 5, 8, 8]))]</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>    _version <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>        in_channels_list: List[<span class="bu">int</span>],</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>        out_channels: <span class="bu">int</span>,</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>        extra_blocks: Optional[ExtraFPNBlock] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>        norm_layer: Optional[Callable[..., nn.Module]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>        _log_api_usage_once(<span class="va">self</span>)</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inner_blocks <span class="op">=</span> nn.ModuleList()</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_blocks <span class="op">=</span> nn.ModuleList()</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> in_channels <span class="kw">in</span> in_channels_list:</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> in_channels <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"in_channels=0 is currently not supported"</span>)</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>            inner_block_module <span class="op">=</span> Conv2dNormActivation(</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>                in_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>, norm_layer<span class="op">=</span>norm_layer, activation_layer<span class="op">=</span><span class="va">None</span></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>            layer_block_module <span class="op">=</span> Conv2dNormActivation(</span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>                out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, norm_layer<span class="op">=</span>norm_layer, activation_layer<span class="op">=</span><span class="va">None</span></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.inner_blocks.append(inner_block_module)</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.layer_blocks.append(layer_block_module)</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize parameters now to avoid modifying the initialization of top_blocks</span></span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> m <span class="kw">in</span> <span class="va">self</span>.modules():</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(m, nn.Conv2d):</span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>                nn.init.kaiming_uniform_(m.weight, a<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> m.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>                    nn.init.constant_(m.bias, <span class="dv">0</span>)</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> extra_blocks <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(extra_blocks, ExtraFPNBlock):</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">TypeError</span>(<span class="ss">f"extra_blocks should be of type ExtraFPNBlock not </span><span class="sc">{</span><span class="bu">type</span>(extra_blocks)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.extra_blocks <span class="op">=</span> extra_blocks</span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_add <span class="op">=</span> FloatFunctional()</span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _load_from_state_dict(</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>        state_dict,</span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>        prefix,</span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>        local_metadata,</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>        strict,</span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>        missing_keys,</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a>        unexpected_keys,</span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a>        error_msgs,</span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a>        version <span class="op">=</span> local_metadata.get(<span class="st">"version"</span>, <span class="va">None</span>)</span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> version <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> version <span class="op">&lt;</span> <span class="dv">2</span>:</span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a>            num_blocks <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.inner_blocks)</span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> block <span class="kw">in</span> [<span class="st">"inner_blocks"</span>, <span class="st">"layer_blocks"</span>]:</span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_blocks):</span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> <span class="bu">type</span> <span class="kw">in</span> [<span class="st">"weight"</span>, <span class="st">"bias"</span>]:</span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a>                        old_key <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>prefix<span class="sc">}{</span>block<span class="sc">}</span><span class="ss">.</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">.</span><span class="sc">{</span><span class="bu">type</span><span class="sc">}</span><span class="ss">"</span></span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a>                        new_key <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>prefix<span class="sc">}{</span>block<span class="sc">}</span><span class="ss">.</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">.0.</span><span class="sc">{</span><span class="bu">type</span><span class="sc">}</span><span class="ss">"</span></span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> old_key <span class="kw">in</span> state_dict:</span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a>                            state_dict[new_key] <span class="op">=</span> state_dict.pop(old_key)</span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>()._load_from_state_dict(</span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a>            state_dict,</span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a>            prefix,</span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a>            local_metadata,</span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a>            strict,</span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a>            missing_keys,</span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a>            unexpected_keys,</span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a>            error_msgs,</span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_result_from_inner_blocks(<span class="va">self</span>, x: Tensor, idx: <span class="bu">int</span>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a><span class="co">        This is equivalent to self.inner_blocks[idx](x),</span></span>
<span id="cb5-125"><a href="#cb5-125" aria-hidden="true" tabindex="-1"></a><span class="co">        but torchscript doesn't support this yet</span></span>
<span id="cb5-126"><a href="#cb5-126" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-127"><a href="#cb5-127" aria-hidden="true" tabindex="-1"></a>        num_blocks <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.inner_blocks)</span>
<span id="cb5-128"><a href="#cb5-128" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx <span class="op">&lt;</span> <span class="dv">0</span>:</span>
<span id="cb5-129"><a href="#cb5-129" aria-hidden="true" tabindex="-1"></a>            idx <span class="op">+=</span> num_blocks</span>
<span id="cb5-130"><a href="#cb5-130" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> x</span>
<span id="cb5-131"><a href="#cb5-131" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, module <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.inner_blocks):</span>
<span id="cb5-132"><a href="#cb5-132" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">==</span> idx:</span>
<span id="cb5-133"><a href="#cb5-133" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> module(x)</span>
<span id="cb5-134"><a href="#cb5-134" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb5-135"><a href="#cb5-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-136"><a href="#cb5-136" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_result_from_layer_blocks(<span class="va">self</span>, x: Tensor, idx: <span class="bu">int</span>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb5-137"><a href="#cb5-137" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-138"><a href="#cb5-138" aria-hidden="true" tabindex="-1"></a><span class="co">        This is equivalent to self.layer_blocks[idx](x),</span></span>
<span id="cb5-139"><a href="#cb5-139" aria-hidden="true" tabindex="-1"></a><span class="co">        but torchscript doesn't support this yet</span></span>
<span id="cb5-140"><a href="#cb5-140" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-141"><a href="#cb5-141" aria-hidden="true" tabindex="-1"></a>        num_blocks <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.layer_blocks)</span>
<span id="cb5-142"><a href="#cb5-142" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx <span class="op">&lt;</span> <span class="dv">0</span>:</span>
<span id="cb5-143"><a href="#cb5-143" aria-hidden="true" tabindex="-1"></a>            idx <span class="op">+=</span> num_blocks</span>
<span id="cb5-144"><a href="#cb5-144" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> x</span>
<span id="cb5-145"><a href="#cb5-145" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, module <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.layer_blocks):</span>
<span id="cb5-146"><a href="#cb5-146" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">==</span> idx:</span>
<span id="cb5-147"><a href="#cb5-147" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> module(x)</span>
<span id="cb5-148"><a href="#cb5-148" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb5-149"><a href="#cb5-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-150"><a href="#cb5-150" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Dict[<span class="bu">str</span>, Tensor]) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Tensor]:</span>
<span id="cb5-151"><a href="#cb5-151" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-152"><a href="#cb5-152" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the FPN for a set of feature maps.</span></span>
<span id="cb5-153"><a href="#cb5-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-154"><a href="#cb5-154" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb5-155"><a href="#cb5-155" aria-hidden="true" tabindex="-1"></a><span class="co">            x (OrderedDict[Tensor]): feature maps for each feature level.</span></span>
<span id="cb5-156"><a href="#cb5-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-157"><a href="#cb5-157" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb5-158"><a href="#cb5-158" aria-hidden="true" tabindex="-1"></a><span class="co">            results (OrderedDict[Tensor]): feature maps after FPN layers.</span></span>
<span id="cb5-159"><a href="#cb5-159" aria-hidden="true" tabindex="-1"></a><span class="co">                They are ordered from the highest resolution first.</span></span>
<span id="cb5-160"><a href="#cb5-160" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-161"><a href="#cb5-161" aria-hidden="true" tabindex="-1"></a>        <span class="co"># unpack OrderedDict into two lists for easier handling</span></span>
<span id="cb5-162"><a href="#cb5-162" aria-hidden="true" tabindex="-1"></a>        names <span class="op">=</span> <span class="bu">list</span>(x.keys())</span>
<span id="cb5-163"><a href="#cb5-163" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="bu">list</span>(x.values())</span>
<span id="cb5-164"><a href="#cb5-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-165"><a href="#cb5-165" aria-hidden="true" tabindex="-1"></a>        last_inner <span class="op">=</span> <span class="va">self</span>.get_result_from_inner_blocks(x[<span class="op">-</span><span class="dv">1</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-166"><a href="#cb5-166" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> []</span>
<span id="cb5-167"><a href="#cb5-167" aria-hidden="true" tabindex="-1"></a>        results.append(<span class="va">self</span>.get_result_from_layer_blocks(last_inner, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb5-168"><a href="#cb5-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-169"><a href="#cb5-169" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x) <span class="op">-</span> <span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb5-170"><a href="#cb5-170" aria-hidden="true" tabindex="-1"></a>            inner_lateral <span class="op">=</span> <span class="va">self</span>.get_result_from_inner_blocks(x[idx], idx)</span>
<span id="cb5-171"><a href="#cb5-171" aria-hidden="true" tabindex="-1"></a>            feat_shape <span class="op">=</span> inner_lateral.shape[<span class="op">-</span><span class="dv">2</span>:]</span>
<span id="cb5-172"><a href="#cb5-172" aria-hidden="true" tabindex="-1"></a>            inner_top_down <span class="op">=</span> torch.nn.functional.interpolate(last_inner, size<span class="op">=</span>feat_shape, mode<span class="op">=</span><span class="st">"nearest"</span>)</span>
<span id="cb5-173"><a href="#cb5-173" aria-hidden="true" tabindex="-1"></a>            <span class="co"># last_inner = inner_lateral + inner_top_down</span></span>
<span id="cb5-174"><a href="#cb5-174" aria-hidden="true" tabindex="-1"></a>            last_inner <span class="op">=</span> <span class="va">self</span>.ff_add.add(inner_lateral, inner_top_down)</span>
<span id="cb5-175"><a href="#cb5-175" aria-hidden="true" tabindex="-1"></a>            results.insert(<span class="dv">0</span>, <span class="va">self</span>.get_result_from_layer_blocks(last_inner, idx))</span>
<span id="cb5-176"><a href="#cb5-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-177"><a href="#cb5-177" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.extra_blocks <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-178"><a href="#cb5-178" aria-hidden="true" tabindex="-1"></a>            results, names <span class="op">=</span> <span class="va">self</span>.extra_blocks(results, x, names)</span>
<span id="cb5-179"><a href="#cb5-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-180"><a href="#cb5-180" aria-hidden="true" tabindex="-1"></a>        <span class="co"># make it back an OrderedDict</span></span>
<span id="cb5-181"><a href="#cb5-181" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> OrderedDict([(k, v) <span class="cf">for</span> k, v <span class="kw">in</span> <span class="bu">zip</span>(names, results)])</span>
<span id="cb5-182"><a href="#cb5-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-183"><a href="#cb5-183" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As you can see from the signature of the modified FPN, it also needs to know input dimensions of each layer from the resnet. There are several ways to get this but one way is to simply get the number of features in the final module of each layer.</p>
<div class="cell" data-outputid="c779fd2c-64ea-4bff-bf8e-92292c9c5084" data-execution_count="88">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># from backbone_utils.py</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># https://github.com/pytorch/vision/blob/main/torchvision/models/detection/backbone_utils.py#L145</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># in_channels_stage2 = res101_layers.inplanes // 8</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># in_channels_list = [in_channels_stage2 * 2 ** (i - 1) for i in returned_layers]</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>in_channels_list <span class="op">=</span> []</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k1, m1 <span class="kw">in</span> resnet.named_children():</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="st">'layer'</span> <span class="kw">in</span> k1:</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    in_channels_list.append((m1[<span class="op">-</span><span class="dv">1</span>].bn3.num_features))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>in_channels_list</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="88">
<pre><code>[256, 512, 1024, 2048]</code></pre>
</div>
</div>
<p>Next step is to create a modified <code>BackboneWithFPN</code> that uses our <code>FeaturePyramidNetworkQuantizeable</code>. Here we’ll also make sure that the inputs are quantized and the outputs subsequently dequantized so that they can be fed to the R-CNN.</p>
<p>One important note is that regular <code>BatchNorm2d</code> is the default and used here instead of <code>FrozenBatchNorm2d</code>. Frozen batch norm is the recommended layer because batches are generally <a href="https://github.com/facebookresearch/maskrcnn-benchmark/issues/267">too small for good estimates of mean and variance</a> statistics but it isn’t quantizeable. So using regular batch norm could be unstable and less performant if those layers aren’t frozen before training.</p>
<div class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.ops.feature_pyramid_network <span class="im">import</span> LastLevelMaxPool</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models._utils <span class="im">import</span> IntermediateLayerGetter</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.ao.quantization <span class="im">import</span> QuantStub, DeQuantStub</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BackboneWithFPNQuantizeable(nn.Module):</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        backbone: nn.Module,</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        return_layers: Dict[<span class="bu">str</span>, <span class="bu">str</span>],</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        in_channels_list: List[<span class="bu">int</span>],</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        out_channels: <span class="bu">int</span>,</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        extra_blocks: Optional[ExtraFPNBlock] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        norm_layer: Optional[Callable[..., nn.Module]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.quant <span class="op">=</span> QuantStub()</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dequant <span class="op">=</span> DeQuantStub()</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> extra_blocks <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>            extra_blocks <span class="op">=</span> LastLevelMaxPool()</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.body <span class="op">=</span> IntermediateLayerGetter(backbone, return_layers<span class="op">=</span>return_layers)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fpn <span class="op">=</span> FeaturePyramidNetworkQuantizeable(</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>            in_channels_list<span class="op">=</span>in_channels_list,</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>            out_channels<span class="op">=</span>out_channels,</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>            extra_blocks<span class="op">=</span>extra_blocks,</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>            norm_layer<span class="op">=</span>norm_layer</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_channels <span class="op">=</span> out_channels</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Tensor) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Tensor]:</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.quant(x)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.body(x)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fpn(x)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k, v <span class="kw">in</span> x.items():</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>            x[k] <span class="op">=</span> <span class="va">self</span>.dequant(v)</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can crate the modified backbone with FPN. Once created, there should be quant/dequant stubs visible in the network like so</p>
<pre><code>BackboneWithFPNQuantizeable(
  (quant): QuantStub()
  (dequant): DeQuantStub()
  (body): IntermediateLayerGetter(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BottleneckQuantizeable(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        ...</code></pre>
<div class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># resnet = resnet_101()</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># returned_layers = [1, 2, 3, 4]</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># return_layers = {f"layer{k}": str(v) for v, k in enumerate(returned_layers)}</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># in_channels_list = []</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># for k1, m1 in resnet.named_children():</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">#   if 'layer' in k1:</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">#     in_channels_list.append((m1[-1].bn3.num_features))</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>bbfpn <span class="op">=</span> BackboneWithFPNQuantizeable(</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    backbone<span class="op">=</span>resnet,</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    return_layers<span class="op">=</span>return_layers,</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    in_channels_list<span class="op">=</span>in_channels_list,</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    out_channels<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    extra_blocks<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    norm_layer<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co"># bbfpn</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now the backbone with FPN can be plugged into the <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py#L43">FasterRCNN</a>. The number of classes is set to 2 for either object or background which is specific to the dataset used here.</p>
<div class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.detection.faster_rcnn <span class="im">import</span> FasterRCNN</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>quant_rcnn <span class="op">=</span> FasterRCNN(bbfpn, num_classes<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="layer-fusion-and-quantization-config" class="level3">
<h3 class="anchored" data-anchor-id="layer-fusion-and-quantization-config">Layer Fusion and Quantization Config</h3>
<p>Before training and subsequently converting the model, we can fuse specific sequences of modules in the backbone. Fusing compresses the model <a href="https://pytorch.org/tutorials/recipes/fuse.html">making it smaller and run faster</a> by combining modules like <code>Conv2d-BatchNorm2d-ReLU</code> and <code>Conv2d-BatchNorm2d</code>. After fusing you should see new fused modules in the network like <code>ConvReLU2d</code> as well as <code>Identity</code> where previous modules were.</p>
<pre><code>FasterRCNN(
  (transform): GeneralizedRCNNTransform(
      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
      Resize(min_size=(800,), max_size=1333, mode='bilinear')
  )
  (backbone): BackboneWithFPNQuantizeable(
    (body): IntermediateLayerGetter(
      (conv1): ConvReLU2d(
        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
        (1): ReLU(inplace=True)
      )
      (bn1): Identity()
      (relu): Identity()
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): BottleneckQuantizeable(
          (conv1): ConvReLU2d(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU(inplace=True)
          )
          (bn1): Identity()
          (relu1): Identity()
          ...</code></pre>
<div class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.ao.quantization <span class="im">import</span> fuse_modules</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>quant_rcnn.<span class="bu">eval</span>()</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># fuse stem</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>fuse_modules(quant_rcnn.backbone.body, [[<span class="st">'conv1'</span>, <span class="st">'bn1'</span>, <span class="st">'relu'</span>]], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># fuse blocks</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k1, m1 <span class="kw">in</span> quant_rcnn.backbone.body.named_children():</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"layer"</span> <span class="kw">in</span> k1:  <span class="co"># in sequential layer with blocks</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k2, m2 <span class="kw">in</span> m1.named_children():</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>            fuse_modules(m2, [[<span class="st">"conv1"</span>, <span class="st">"bn1"</span>, <span class="st">"relu1"</span>], [<span class="st">"conv2"</span>, <span class="st">"bn2"</span>, <span class="st">"relu2"</span>], [<span class="st">"conv3"</span>, <span class="st">"bn3"</span>]], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k3, m3 <span class="kw">in</span> m2.named_children():</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="st">"downsample"</span> <span class="kw">in</span> k3:  <span class="co"># fuse downsample</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>                    fuse_modules(m3, [[<span class="st">"0"</span>, <span class="st">"1"</span>]], inplace<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Before training the quantization config needs to be set (on the backbone only). And again, because the batches are so small, <a href="https://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.qat.freeze_bn_stats.html">batch norm gets frozen</a> (see <a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#quantization-aware-training">this pytorch tutorial</a> for another example). Last, I’ll freeze the stem and the first layer in the backbone since the pretrained imagenet weights were loaded. After preparation you should be able to see the observers in the network.</p>
<pre><code>FasterRCNN(
  (transform): GeneralizedRCNNTransform(
      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
      Resize(min_size=(800,), max_size=1333, mode='bilinear')
  )
  (backbone): BackboneWithFPNQuantizeable(
    (body): IntermediateLayerGetter(
      (conv1): ConvReLU2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)
        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(
          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))
        )
        (activation_post_process): FusedMovingAvgObsFakeQuantize(
          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
      (bn1): Identity()
      ...</code></pre>
<div class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.ao.quantization <span class="im">import</span> get_default_qat_qconfig, prepare_qat</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>quant_rcnn.train()</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>quant_rcnn.backbone.qconfig <span class="op">=</span> get_default_qat_qconfig(<span class="st">'fbgemm'</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>quant_rcnn_prepared <span class="op">=</span> prepare_qat(quant_rcnn, inplace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>quant_rcnn_prepared <span class="op">=</span> quant_rcnn_prepared.<span class="bu">apply</span>(torch.nn.intrinsic.qat.freeze_bn_stats)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>quant_rcnn_prepared.backbone.body.conv1.weight.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, parameter <span class="kw">in</span> quant_rcnn_prepared.backbone.named_parameters():</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> re.search(<span class="vs">r".layer1"</span>, name):</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        parameter.requires_grad <span class="op">=</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="dataset-training-and-conversion" class="level3">
<h3 class="anchored" data-anchor-id="dataset-training-and-conversion">Dataset, Training, and Conversion</h3>
<p>I’ll be using the <a href="https://www.cis.upenn.edu/~jshi/ped_html/">PennFudan dataset</a> from the <a href="https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html">Torchvision object detection finetuning tutorial</a> for QAT. Most of the code below is borrowed from that tutorial with slight modifications and no segmentation.</p>
<div class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.io <span class="im">import</span> read_image</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.ops.boxes <span class="im">import</span> masks_to_boxes</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> tv_tensors</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms.v2 <span class="im">import</span> functional <span class="im">as</span> F  <span class="co"># careful namespace 'F'</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> v2 <span class="im">as</span> T</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PennFudanDataset(torch.utils.data.Dataset):</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, root, transforms):</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.root <span class="op">=</span> root</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transforms <span class="op">=</span> transforms</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># load all image files, sorting them to</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ensure that they are aligned</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.imgs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">sorted</span>(os.listdir(os.path.join(root, <span class="st">"PNGImages"</span>))))</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.masks <span class="op">=</span> <span class="bu">list</span>(<span class="bu">sorted</span>(os.listdir(os.path.join(root, <span class="st">"PedMasks"</span>))))</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># load images and masks</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        img_path <span class="op">=</span> os.path.join(<span class="va">self</span>.root, <span class="st">"PNGImages"</span>, <span class="va">self</span>.imgs[idx])</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        mask_path <span class="op">=</span> os.path.join(<span class="va">self</span>.root, <span class="st">"PedMasks"</span>, <span class="va">self</span>.masks[idx])</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> read_image(img_path)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> read_image(mask_path)</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># instances are encoded as different colors</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>        obj_ids <span class="op">=</span> torch.unique(mask)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># first id is the background, so remove it</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>        obj_ids <span class="op">=</span> obj_ids[<span class="dv">1</span>:]</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>        num_objs <span class="op">=</span> <span class="bu">len</span>(obj_ids)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># split the color-encoded mask into a set</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># of binary masks</span></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>        masks <span class="op">=</span> (mask <span class="op">==</span> obj_ids[:, <span class="va">None</span>, <span class="va">None</span>]).to(dtype<span class="op">=</span>torch.uint8)</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get bounding box coordinates for each mask</span></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>        boxes <span class="op">=</span> masks_to_boxes(masks)</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># there is only one class</span></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> torch.ones((num_objs,), dtype<span class="op">=</span>torch.int64)</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>        image_id <span class="op">=</span> idx</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>        area <span class="op">=</span> (boxes[:, <span class="dv">3</span>] <span class="op">-</span> boxes[:, <span class="dv">1</span>]) <span class="op">*</span> (boxes[:, <span class="dv">2</span>] <span class="op">-</span> boxes[:, <span class="dv">0</span>])</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># suppose all instances are not crowd</span></span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>        iscrowd <span class="op">=</span> torch.zeros((num_objs,), dtype<span class="op">=</span>torch.int64)</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Wrap sample and targets into torchvision tv_tensors:</span></span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> tv_tensors.Image(img)</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> {}</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>        target[<span class="st">"boxes"</span>] <span class="op">=</span> tv_tensors.BoundingBoxes(boxes, <span class="bu">format</span><span class="op">=</span><span class="st">"XYXY"</span>, canvas_size<span class="op">=</span>F.get_size(img))</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>        target[<span class="st">"masks"</span>] <span class="op">=</span> tv_tensors.Mask(masks)</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>        target[<span class="st">"labels"</span>] <span class="op">=</span> labels</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>        target[<span class="st">"image_id"</span>] <span class="op">=</span> image_id</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>        target[<span class="st">"area"</span>] <span class="op">=</span> area</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>        target[<span class="st">"iscrowd"</span>] <span class="op">=</span> iscrowd</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.transforms <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>            img, target <span class="op">=</span> <span class="va">self</span>.transforms(img, target)</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> img, target</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.imgs)</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_transform(train):</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a>    transforms <span class="op">=</span> []</span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> train:</span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a>        transforms.append(T.RandomHorizontalFlip(<span class="fl">0.5</span>))</span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a>    transforms.append(T.ToDtype(torch.<span class="bu">float</span>, scale<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a>    transforms.append(T.ToPureTensor())</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> T.Compose(transforms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>os.system(<span class="st">"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py"</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>os.system(<span class="st">"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py"</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>os.system(<span class="st">"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>os.system(<span class="st">"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py"</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>os.system(<span class="st">"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py"</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget https:<span class="op">//</span>www.cis.upenn.edu<span class="op">/~</span>jshi<span class="op">/</span>ped_html<span class="op">/</span>PennFudanPed.<span class="bu">zip</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>unzip PennFudanPed.<span class="bu">zip</span> <span class="op">-</span>d .<span class="op">/</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="95">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> utils</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> engine <span class="im">import</span> train_one_epoch, evaluate</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># train on the GPU or on the CPU, if a GPU is not available</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span>) <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> torch.device(<span class="st">'cpu'</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co"># our dataset has two classes only - background and person</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co"># use our dataset and defined transformations</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> PennFudanDataset(<span class="st">'PennFudanPed'</span>, get_transform(train<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>dataset_test <span class="op">=</span> PennFudanDataset(<span class="st">'PennFudanPed'</span>, get_transform(train<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co"># split the dataset in train and test set</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> torch.randperm(<span class="bu">len</span>(dataset)).tolist()</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> torch.utils.data.Subset(dataset, indices[:<span class="op">-</span><span class="dv">50</span>])</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>dataset_test <span class="op">=</span> torch.utils.data.Subset(dataset_test, indices[<span class="op">-</span><span class="dv">50</span>:])</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="co"># define training and validation data loaders</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    dataset,</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>    collate_fn<span class="op">=</span>utils.collate_fn</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>data_loader_test <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    dataset_test,</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>    collate_fn<span class="op">=</span>utils.collate_fn</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="f60201e3-1f86-43f1-a57d-238bd4d454cc" data-execution_count="96">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># move model to the right device</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>quant_rcnn_prepared.to(device)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># construct an optimizer</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> [p <span class="cf">for</span> p <span class="kw">in</span> quant_rcnn_prepared.parameters() <span class="cf">if</span> p.requires_grad]</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    params,</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span><span class="fl">0.005</span>,</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    momentum<span class="op">=</span><span class="fl">0.9</span>,</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.0005</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co"># and a learning rate scheduler</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>lr_scheduler <span class="op">=</span> torch.optim.lr_scheduler.StepLR(</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    optimizer,</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    step_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    gamma<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="co"># let's train it for 10 epochs</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># train for one epoch, printing every 20 iterations</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    train_one_epoch(quant_rcnn_prepared, optimizer, data_loader, device, epoch, print_freq<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># update the learning rate</span></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>    lr_scheduler.step()</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluate on the test dataset</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>    evaluate(quant_rcnn_prepared, data_loader_test, device<span class="op">=</span>device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: [0]  [ 0/60]  eta: 0:00:49  lr: 0.000090  loss: 1.5364 (1.5364)  loss_classifier: 0.8895 (0.8895)  loss_box_reg: 0.0007 (0.0007)  loss_objectness: 0.6395 (0.6395)  loss_rpn_box_reg: 0.0069 (0.0069)  time: 0.8299  data: 0.1936  max mem: 6367
Epoch: [0]  [20/60]  eta: 0:00:25  lr: 0.001783  loss: 0.6043 (0.7210)  loss_classifier: 0.1105 (0.2343)  loss_box_reg: 0.0665 (0.0648)  loss_objectness: 0.3679 (0.3956)  loss_rpn_box_reg: 0.0270 (0.0263)  time: 0.6163  data: 0.0064  max mem: 6367
Epoch: [0]  [40/60]  eta: 0:00:12  lr: 0.003476  loss: 0.3219 (0.5298)  loss_classifier: 0.0957 (0.1841)  loss_box_reg: 0.1185 (0.0904)  loss_objectness: 0.0582 (0.2321)  loss_rpn_box_reg: 0.0194 (0.0233)  time: 0.6057  data: 0.0056  max mem: 6367
Epoch: [0]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2767 (0.4805)  loss_classifier: 0.0943 (0.1721)  loss_box_reg: 0.1098 (0.0993)  loss_objectness: 0.0459 (0.1849)  loss_rpn_box_reg: 0.0160 (0.0242)  time: 0.6120  data: 0.0057  max mem: 6367
Epoch: [0] Total time: 0:00:37 (0.6183 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:23  model_time: 0.2870 (0.2870)  evaluator_time: 0.0120 (0.0120)  time: 0.4753  data: 0.1745  max mem: 6367
Test:  [49/50]  eta: 0:00:00  model_time: 0.2425 (0.2465)  evaluator_time: 0.0037 (0.0049)  time: 0.2517  data: 0.0030  max mem: 6367
Test: Total time: 0:00:13 (0.2624 s / it)
Averaged stats: model_time: 0.2425 (0.2465)  evaluator_time: 0.0037 (0.0049)
Accumulating evaluation results...
DONE (t=0.04s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.111
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.432
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.013
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.119
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.074
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.299
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.013
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.322
Epoch: [1]  [ 0/60]  eta: 0:00:50  lr: 0.005000  loss: 0.2406 (0.2406)  loss_classifier: 0.0861 (0.0861)  loss_box_reg: 0.1205 (0.1205)  loss_objectness: 0.0287 (0.0287)  loss_rpn_box_reg: 0.0053 (0.0053)  time: 0.8447  data: 0.1997  max mem: 6367
Epoch: [1]  [20/60]  eta: 0:00:25  lr: 0.005000  loss: 0.2323 (0.2870)  loss_classifier: 0.0756 (0.1012)  loss_box_reg: 0.1083 (0.1261)  loss_objectness: 0.0416 (0.0443)  loss_rpn_box_reg: 0.0130 (0.0154)  time: 0.6140  data: 0.0056  max mem: 6367
Epoch: [1]  [40/60]  eta: 0:00:12  lr: 0.005000  loss: 0.3195 (0.3082)  loss_classifier: 0.1031 (0.1040)  loss_box_reg: 0.1658 (0.1469)  loss_objectness: 0.0288 (0.0396)  loss_rpn_box_reg: 0.0207 (0.0177)  time: 0.6341  data: 0.0056  max mem: 6983
Epoch: [1]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2555 (0.2919)  loss_classifier: 0.0822 (0.0958)  loss_box_reg: 0.1283 (0.1443)  loss_objectness: 0.0088 (0.0319)  loss_rpn_box_reg: 0.0179 (0.0199)  time: 0.6182  data: 0.0055  max mem: 6983
Epoch: [1] Total time: 0:00:37 (0.6288 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:24  model_time: 0.3046 (0.3046)  evaluator_time: 0.0079 (0.0079)  time: 0.4994  data: 0.1854  max mem: 6983
Test:  [49/50]  eta: 0:00:00  model_time: 0.2479 (0.2522)  evaluator_time: 0.0022 (0.0028)  time: 0.2579  data: 0.0033  max mem: 6983
Test: Total time: 0:00:13 (0.2663 s / it)
Averaged stats: model_time: 0.2479 (0.2522)  evaluator_time: 0.0022 (0.0028)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.831
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.139
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.172
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.190
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.413
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.437
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.300
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.448
Epoch: [2]  [ 0/60]  eta: 0:00:51  lr: 0.005000  loss: 0.2004 (0.2004)  loss_classifier: 0.0508 (0.0508)  loss_box_reg: 0.1245 (0.1245)  loss_objectness: 0.0078 (0.0078)  loss_rpn_box_reg: 0.0173 (0.0173)  time: 0.8501  data: 0.1844  max mem: 6983
Epoch: [2]  [20/60]  eta: 0:00:26  lr: 0.005000  loss: 0.2482 (0.2507)  loss_classifier: 0.0578 (0.0676)  loss_box_reg: 0.1521 (0.1533)  loss_objectness: 0.0069 (0.0085)  loss_rpn_box_reg: 0.0191 (0.0213)  time: 0.6400  data: 0.0056  max mem: 6983
Epoch: [2]  [40/60]  eta: 0:00:12  lr: 0.005000  loss: 0.1892 (0.2265)  loss_classifier: 0.0588 (0.0633)  loss_box_reg: 0.1038 (0.1351)  loss_objectness: 0.0061 (0.0092)  loss_rpn_box_reg: 0.0143 (0.0189)  time: 0.6334  data: 0.0054  max mem: 6983
Epoch: [2]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.1695 (0.2197)  loss_classifier: 0.0545 (0.0631)  loss_box_reg: 0.0872 (0.1279)  loss_objectness: 0.0054 (0.0102)  loss_rpn_box_reg: 0.0138 (0.0186)  time: 0.6422  data: 0.0060  max mem: 6983
Epoch: [2] Total time: 0:00:38 (0.6448 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:23  model_time: 0.2871 (0.2871)  evaluator_time: 0.0039 (0.0039)  time: 0.4719  data: 0.1796  max mem: 6983
Test:  [49/50]  eta: 0:00:00  model_time: 0.2536 (0.2540)  evaluator_time: 0.0017 (0.0021)  time: 0.2615  data: 0.0031  max mem: 6983
Test: Total time: 0:00:13 (0.2672 s / it)
Averaged stats: model_time: 0.2536 (0.2540)  evaluator_time: 0.0017 (0.0021)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.949
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.386
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.324
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.496
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.564
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.572
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.412
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.584
Epoch: [3]  [ 0/60]  eta: 0:00:53  lr: 0.000500  loss: 0.1430 (0.1430)  loss_classifier: 0.0370 (0.0370)  loss_box_reg: 0.0875 (0.0875)  loss_objectness: 0.0089 (0.0089)  loss_rpn_box_reg: 0.0096 (0.0096)  time: 0.8995  data: 0.2071  max mem: 6983
Epoch: [3]  [20/60]  eta: 0:00:26  lr: 0.000500  loss: 0.1757 (0.1837)  loss_classifier: 0.0432 (0.0492)  loss_box_reg: 0.1194 (0.1130)  loss_objectness: 0.0066 (0.0068)  loss_rpn_box_reg: 0.0141 (0.0147)  time: 0.6443  data: 0.0057  max mem: 6983
Epoch: [3]  [40/60]  eta: 0:00:13  lr: 0.000500  loss: 0.1488 (0.1730)  loss_classifier: 0.0427 (0.0477)  loss_box_reg: 0.0932 (0.1054)  loss_objectness: 0.0046 (0.0062)  loss_rpn_box_reg: 0.0106 (0.0136)  time: 0.6576  data: 0.0057  max mem: 6983
Epoch: [3]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1673 (0.1781)  loss_classifier: 0.0469 (0.0508)  loss_box_reg: 0.0972 (0.1080)  loss_objectness: 0.0045 (0.0059)  loss_rpn_box_reg: 0.0096 (0.0134)  time: 0.6530  data: 0.0053  max mem: 6983
Epoch: [3] Total time: 0:00:39 (0.6593 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:23  model_time: 0.2835 (0.2835)  evaluator_time: 0.0039 (0.0039)  time: 0.4670  data: 0.1781  max mem: 6983
Test:  [49/50]  eta: 0:00:00  model_time: 0.2530 (0.2555)  evaluator_time: 0.0013 (0.0018)  time: 0.2578  data: 0.0032  max mem: 6983
Test: Total time: 0:00:13 (0.2685 s / it)
Averaged stats: model_time: 0.2530 (0.2555)  evaluator_time: 0.0013 (0.0018)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.493
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.942
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.419
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.362
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.285
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.571
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.574
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.585
Epoch: [4]  [ 0/60]  eta: 0:00:54  lr: 0.000500  loss: 0.1610 (0.1610)  loss_classifier: 0.0388 (0.0388)  loss_box_reg: 0.1063 (0.1063)  loss_objectness: 0.0052 (0.0052)  loss_rpn_box_reg: 0.0107 (0.0107)  time: 0.9067  data: 0.2171  max mem: 6983
Epoch: [4]  [20/60]  eta: 0:00:27  lr: 0.000500  loss: 0.1496 (0.1722)  loss_classifier: 0.0469 (0.0516)  loss_box_reg: 0.0883 (0.1041)  loss_objectness: 0.0025 (0.0046)  loss_rpn_box_reg: 0.0117 (0.0120)  time: 0.6716  data: 0.0062  max mem: 6983
Epoch: [4]  [40/60]  eta: 0:00:13  lr: 0.000500  loss: 0.1522 (0.1744)  loss_classifier: 0.0459 (0.0516)  loss_box_reg: 0.1082 (0.1082)  loss_objectness: 0.0038 (0.0043)  loss_rpn_box_reg: 0.0068 (0.0103)  time: 0.6789  data: 0.0058  max mem: 6983
Epoch: [4]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1792 (0.1787)  loss_classifier: 0.0513 (0.0527)  loss_box_reg: 0.1076 (0.1115)  loss_objectness: 0.0029 (0.0041)  loss_rpn_box_reg: 0.0100 (0.0105)  time: 0.6734  data: 0.0055  max mem: 6983
Epoch: [4] Total time: 0:00:40 (0.6817 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:23  model_time: 0.2894 (0.2894)  evaluator_time: 0.0040 (0.0040)  time: 0.4672  data: 0.1722  max mem: 6983
Test:  [49/50]  eta: 0:00:00  model_time: 0.2520 (0.2579)  evaluator_time: 0.0014 (0.0018)  time: 0.2610  data: 0.0030  max mem: 6983
Test: Total time: 0:00:13 (0.2706 s / it)
Averaged stats: model_time: 0.2520 (0.2579)  evaluator_time: 0.0014 (0.0018)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.528
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.952
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.546
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.338
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.542
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.293
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.605
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.615
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.463
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.626
Epoch: [5]  [ 0/60]  eta: 0:00:52  lr: 0.000500  loss: 0.1348 (0.1348)  loss_classifier: 0.0358 (0.0358)  loss_box_reg: 0.0862 (0.0862)  loss_objectness: 0.0018 (0.0018)  loss_rpn_box_reg: 0.0110 (0.0110)  time: 0.8712  data: 0.1820  max mem: 6983
Epoch: [5]  [20/60]  eta: 0:00:27  lr: 0.000500  loss: 0.1626 (0.1758)  loss_classifier: 0.0444 (0.0485)  loss_box_reg: 0.0996 (0.1133)  loss_objectness: 0.0029 (0.0039)  loss_rpn_box_reg: 0.0098 (0.0101)  time: 0.6826  data: 0.0058  max mem: 6983
Epoch: [5]  [40/60]  eta: 0:00:13  lr: 0.000500  loss: 0.1343 (0.1651)  loss_classifier: 0.0395 (0.0468)  loss_box_reg: 0.0837 (0.1052)  loss_objectness: 0.0023 (0.0037)  loss_rpn_box_reg: 0.0089 (0.0095)  time: 0.6910  data: 0.0059  max mem: 7287
Epoch: [5]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1644 (0.1699)  loss_classifier: 0.0499 (0.0484)  loss_box_reg: 0.0954 (0.1076)  loss_objectness: 0.0026 (0.0038)  loss_rpn_box_reg: 0.0098 (0.0100)  time: 0.6861  data: 0.0062  max mem: 7287
Epoch: [5] Total time: 0:00:41 (0.6929 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:24  model_time: 0.3069 (0.3069)  evaluator_time: 0.0041 (0.0041)  time: 0.4943  data: 0.1818  max mem: 7287
Test:  [49/50]  eta: 0:00:00  model_time: 0.2673 (0.2650)  evaluator_time: 0.0014 (0.0018)  time: 0.2721  data: 0.0030  max mem: 7287
Test: Total time: 0:00:13 (0.2780 s / it)
Averaged stats: model_time: 0.2673 (0.2650)  evaluator_time: 0.0014 (0.0018)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.560
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.957
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.602
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.451
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.570
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.311
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.644
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.647
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.550
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.655
Epoch: [6]  [ 0/60]  eta: 0:00:55  lr: 0.000050  loss: 0.1775 (0.1775)  loss_classifier: 0.0405 (0.0405)  loss_box_reg: 0.1216 (0.1216)  loss_objectness: 0.0029 (0.0029)  loss_rpn_box_reg: 0.0125 (0.0125)  time: 0.9273  data: 0.2030  max mem: 7287
Epoch: [6]  [20/60]  eta: 0:00:28  lr: 0.000050  loss: 0.1606 (0.1758)  loss_classifier: 0.0500 (0.0530)  loss_box_reg: 0.0945 (0.1087)  loss_objectness: 0.0032 (0.0039)  loss_rpn_box_reg: 0.0090 (0.0102)  time: 0.6964  data: 0.0055  max mem: 7287
Epoch: [6]  [40/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1137 (0.1597)  loss_classifier: 0.0357 (0.0480)  loss_box_reg: 0.0660 (0.0997)  loss_objectness: 0.0018 (0.0034)  loss_rpn_box_reg: 0.0057 (0.0086)  time: 0.6934  data: 0.0057  max mem: 7287
Epoch: [6]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1633 (0.1669)  loss_classifier: 0.0410 (0.0493)  loss_box_reg: 0.0954 (0.1050)  loss_objectness: 0.0024 (0.0035)  loss_rpn_box_reg: 0.0081 (0.0091)  time: 0.6901  data: 0.0058  max mem: 7287
Epoch: [6] Total time: 0:00:41 (0.6998 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:24  model_time: 0.3050 (0.3050)  evaluator_time: 0.0035 (0.0035)  time: 0.4857  data: 0.1758  max mem: 7287
Test:  [49/50]  eta: 0:00:00  model_time: 0.2615 (0.2624)  evaluator_time: 0.0013 (0.0018)  time: 0.2673  data: 0.0031  max mem: 7287
Test: Total time: 0:00:13 (0.2757 s / it)
Averaged stats: model_time: 0.2615 (0.2624)  evaluator_time: 0.0013 (0.0018)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.537
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.948
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.534
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.547
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.294
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.628
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.642
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.550
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.649
Epoch: [7]  [ 0/60]  eta: 0:00:55  lr: 0.000050  loss: 0.0931 (0.0931)  loss_classifier: 0.0252 (0.0252)  loss_box_reg: 0.0566 (0.0566)  loss_objectness: 0.0020 (0.0020)  loss_rpn_box_reg: 0.0093 (0.0093)  time: 0.9269  data: 0.2053  max mem: 7287
Epoch: [7]  [20/60]  eta: 0:00:28  lr: 0.000050  loss: 0.1182 (0.1405)  loss_classifier: 0.0339 (0.0406)  loss_box_reg: 0.0713 (0.0877)  loss_objectness: 0.0017 (0.0043)  loss_rpn_box_reg: 0.0049 (0.0079)  time: 0.6926  data: 0.0066  max mem: 7462
Epoch: [7]  [40/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1324 (0.1550)  loss_classifier: 0.0367 (0.0454)  loss_box_reg: 0.0773 (0.0970)  loss_objectness: 0.0020 (0.0038)  loss_rpn_box_reg: 0.0087 (0.0088)  time: 0.6966  data: 0.0055  max mem: 7462
Epoch: [7]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1809 (0.1661)  loss_classifier: 0.0493 (0.0488)  loss_box_reg: 0.1123 (0.1040)  loss_objectness: 0.0038 (0.0042)  loss_rpn_box_reg: 0.0103 (0.0092)  time: 0.6901  data: 0.0055  max mem: 7462
Epoch: [7] Total time: 0:00:42 (0.7001 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:24  model_time: 0.2994 (0.2994)  evaluator_time: 0.0039 (0.0039)  time: 0.4838  data: 0.1787  max mem: 7462
Test:  [49/50]  eta: 0:00:00  model_time: 0.2554 (0.2593)  evaluator_time: 0.0013 (0.0017)  time: 0.2629  data: 0.0029  max mem: 7462
Test: Total time: 0:00:13 (0.2721 s / it)
Averaged stats: model_time: 0.2554 (0.2593)  evaluator_time: 0.0013 (0.0017)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.530
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.951
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.491
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.451
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.540
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.283
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.616
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.618
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.562
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.623
Epoch: [8]  [ 0/60]  eta: 0:00:58  lr: 0.000050  loss: 0.3004 (0.3004)  loss_classifier: 0.0767 (0.0767)  loss_box_reg: 0.2012 (0.2012)  loss_objectness: 0.0051 (0.0051)  loss_rpn_box_reg: 0.0173 (0.0173)  time: 0.9718  data: 0.2255  max mem: 7462
Epoch: [8]  [20/60]  eta: 0:00:28  lr: 0.000050  loss: 0.1537 (0.1833)  loss_classifier: 0.0469 (0.0556)  loss_box_reg: 0.0927 (0.1127)  loss_objectness: 0.0018 (0.0034)  loss_rpn_box_reg: 0.0098 (0.0116)  time: 0.7003  data: 0.0058  max mem: 7462
Epoch: [8]  [40/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1291 (0.1668)  loss_classifier: 0.0407 (0.0491)  loss_box_reg: 0.0915 (0.1050)  loss_objectness: 0.0029 (0.0032)  loss_rpn_box_reg: 0.0055 (0.0094)  time: 0.6929  data: 0.0056  max mem: 7462
Epoch: [8]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1634 (0.1668)  loss_classifier: 0.0514 (0.0498)  loss_box_reg: 0.1034 (0.1045)  loss_objectness: 0.0024 (0.0032)  loss_rpn_box_reg: 0.0083 (0.0094)  time: 0.6999  data: 0.0056  max mem: 7462
Epoch: [8] Total time: 0:00:42 (0.7050 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:24  model_time: 0.3030 (0.3030)  evaluator_time: 0.0037 (0.0037)  time: 0.4906  data: 0.1825  max mem: 7462
Test:  [49/50]  eta: 0:00:00  model_time: 0.2565 (0.2625)  evaluator_time: 0.0012 (0.0017)  time: 0.2623  data: 0.0030  max mem: 7462
Test: Total time: 0:00:13 (0.2753 s / it)
Averaged stats: model_time: 0.2565 (0.2625)  evaluator_time: 0.0012 (0.0017)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.534
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.951
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.526
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.293
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.550
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.289
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.618
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.619
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.450
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.632
Epoch: [9]  [ 0/60]  eta: 0:00:57  lr: 0.000005  loss: 0.2058 (0.2058)  loss_classifier: 0.0578 (0.0578)  loss_box_reg: 0.1311 (0.1311)  loss_objectness: 0.0029 (0.0029)  loss_rpn_box_reg: 0.0140 (0.0140)  time: 0.9631  data: 0.2252  max mem: 7462
Epoch: [9]  [20/60]  eta: 0:00:28  lr: 0.000005  loss: 0.1616 (0.1713)  loss_classifier: 0.0448 (0.0518)  loss_box_reg: 0.0901 (0.1049)  loss_objectness: 0.0042 (0.0042)  loss_rpn_box_reg: 0.0101 (0.0105)  time: 0.7072  data: 0.0055  max mem: 7462
Epoch: [9]  [40/60]  eta: 0:00:14  lr: 0.000005  loss: 0.1449 (0.1669)  loss_classifier: 0.0415 (0.0500)  loss_box_reg: 0.1006 (0.1033)  loss_objectness: 0.0021 (0.0038)  loss_rpn_box_reg: 0.0069 (0.0098)  time: 0.7147  data: 0.0056  max mem: 7462
Epoch: [9]  [59/60]  eta: 0:00:00  lr: 0.000005  loss: 0.1712 (0.1657)  loss_classifier: 0.0517 (0.0498)  loss_box_reg: 0.0998 (0.1030)  loss_objectness: 0.0019 (0.0034)  loss_rpn_box_reg: 0.0084 (0.0094)  time: 0.7149  data: 0.0065  max mem: 7462
Epoch: [9] Total time: 0:00:43 (0.7191 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:24  model_time: 0.3057 (0.3057)  evaluator_time: 0.0038 (0.0038)  time: 0.4838  data: 0.1729  max mem: 7462
Test:  [49/50]  eta: 0:00:00  model_time: 0.2585 (0.2639)  evaluator_time: 0.0013 (0.0018)  time: 0.2670  data: 0.0030  max mem: 7462
Test: Total time: 0:00:13 (0.2766 s / it)
Averaged stats: model_time: 0.2585 (0.2639)  evaluator_time: 0.0013 (0.0018)
Accumulating evaluation results...
DONE (t=0.02s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.535
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.950
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.550
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.550
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.297
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.624
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.634
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.475
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.646</code></pre>
</div>
</div>
<p>Now to convert and save the model. Make sure to put the model on CPU before conversion or you will get an error. After conversion you should see quantized modules like <code>QuantizedConvReLU2d</code>.</p>
<pre><code>FasterRCNN(
  (transform): GeneralizedRCNNTransform(
      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
      Resize(min_size=(800,), max_size=1333, mode='bilinear')
  )
  (backbone): BackboneWithFPNQuantizeable(
    (quant): Quantize(scale=tensor([0.0375]), zero_point=tensor([57]), dtype=torch.quint8)
    (dequant): DeQuantize()
    (body): IntermediateLayerGetter(
      (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.2553767263889313, zero_point=0, padding=(3, 3))
      (bn1): Identity()
      (relu): Identity()
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): BottleneckQuantizeable(
          (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.12423195689916611, zero_point=0)
          (bn1): Identity()
          (relu1): Identity()
          ...</code></pre>
<div class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.ao.quantization <span class="im">import</span> convert</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>quant_rcnn_prepared.<span class="bu">eval</span>()</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>quant_rcnn_prepared.to(torch.device(<span class="st">'cpu'</span>))</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>quant_rcnn_converted <span class="op">=</span> convert(quant_rcnn_prepared, inplace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>quant_model_path <span class="op">=</span> <span class="st">"/content/quant_model.pth"</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>torch.save(quant_rcnn_converted.state_dict(), quant_model_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For comparison I’ll generate the same network without any modifications made for quantization (including fusion). Then we can compare model sizes and latency. Note that this is just comparing latency on the CPU, if the float model was on GPU it could be significantly faster depending upon the hardware.</p>
<div class="cell" data-execution_count="98">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.detection.backbone_utils <span class="im">import</span> BackboneWithFPN</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.resnet <span class="im">import</span> Bottleneck</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>resnet_bb <span class="op">=</span> resnet_101()</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>rcnn <span class="op">=</span> FasterRCNN(</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    BackboneWithFPN(</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        backbone<span class="op">=</span>resnet_bb,</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        return_layers<span class="op">=</span>return_layers,</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        in_channels_list<span class="op">=</span>in_channels_list,</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        out_channels<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>        extra_blocks<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>        norm_layer<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>    num_classes<span class="op">=</span><span class="dv">2</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>rcnn.<span class="bu">eval</span>()</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>rcnn.to(torch.device(<span class="st">'cpu'</span>))</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">"/content/float_model.pth"</span></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>torch.save(rcnn.state_dict(), model_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="ff6296c4-b1a5-4d10-fc30-a84ddef1d692" data-execution_count="99">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'size of quantized model: </span><span class="sc">{</span><span class="bu">round</span>(os.path.getsize(<span class="st">"/content/quant_model.pth"</span>) <span class="op">/</span> <span class="fl">1e6</span>)<span class="sc">}</span><span class="ss"> MB'</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'size of float model: </span><span class="sc">{</span><span class="bu">round</span>(os.path.getsize(<span class="st">"/content/float_model.pth"</span>) <span class="op">/</span> <span class="fl">1e6</span>)<span class="sc">}</span><span class="ss"> MB'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>size of quantized model: 105 MB
size of float model: 242 MB</code></pre>
</div>
</div>
<div class="cell" data-outputid="3aca0f62-0e83-42b8-9f0a-9fefc307986b" data-execution_count="100">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> time <span class="im">import</span> perf_counter</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>quant_rcnn_converted.to(torch.device(<span class="st">'cpu'</span>))</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co"># just grab one test image/batch</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>images, targets <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(data_loader_test))</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>images <span class="op">=</span> <span class="bu">list</span>(img.to(torch.device(<span class="st">'cpu'</span>)) <span class="cf">for</span> img <span class="kw">in</span> images)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> perf_counter()</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    __ <span class="op">=</span> quant_rcnn_converted(images)</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"quant model avg time: </span><span class="sc">{</span>(perf_counter() <span class="op">-</span> start) <span class="op">/</span> n<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> perf_counter()</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>    __ <span class="op">=</span> rcnn(images)</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"float model avg time (cpu): </span><span class="sc">{</span>(perf_counter() <span class="op">-</span> start) <span class="op">/</span> n<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>quant model avg time: 1.42
float model avg time (cpu): 2.20</code></pre>
</div>
</div>
<p>I believe a fully quantized model would be even smaller and faster by comparison. In this case, while we did quantize the backbone for the RCNN, it only accounted for roughly 75% of the model parameters. So a significant number of float operations still occur after the quantized backbone.</p>
<div class="cell" data-outputid="ff10f6c1-1872-483a-de80-6db4d2bc7865" data-execution_count="101">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>num_model_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> rcnn.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>num_backbone_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> rcnn.backbone.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"total number of parameters in model: </span><span class="sc">{</span>num_model_params<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"total number of parameters in backbone: </span><span class="sc">{</span>num_backbone_params<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ratio of quantized parameters: </span><span class="sc">{</span>num_backbone_params <span class="op">/</span> num_model_params<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>total number of parameters in model: 60344409
total number of parameters in backbone: 45844544
ratio of quantized parameters: 0.76</code></pre>
</div>
</div>
<p>We can also profile each model to see where each spends the most time during a forward pass.</p>
<div class="cell" data-outputid="4b536781-a82a-4ba5-a423-b656abdc5297" data-execution_count="102">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.profiler <span class="im">import</span> profile, record_function, ProfilerActivity</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> profile(activities<span class="op">=</span>[ProfilerActivity.CPU], record_shapes<span class="op">=</span><span class="va">False</span>) <span class="im">as</span> prof:</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> record_function(<span class="st">"model_inference"</span>):</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>        quant_rcnn_converted(images)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prof.key_averages().table(sort_by<span class="op">=</span><span class="st">"cpu_time_total"</span>, row_limit<span class="op">=</span><span class="dv">10</span>))</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> profile(activities<span class="op">=</span>[ProfilerActivity.CPU], record_shapes<span class="op">=</span><span class="va">False</span>) <span class="im">as</span> prof:</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> record_function(<span class="st">"model_inference"</span>):</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>        rcnn(images)</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prof.key_averages().table(sort_by<span class="op">=</span><span class="st">"cpu_time_total"</span>, row_limit<span class="op">=</span><span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                  model_inference         2.15%      29.904ms       100.00%        1.388s        1.388s             1  
           quantized::conv2d_relu        26.61%     369.449ms        26.96%     374.209ms       5.585ms            67  
                quantized::conv2d        23.33%     323.889ms        23.44%     325.335ms       7.230ms            45  
           torchvision::roi_align        17.54%     243.538ms        20.78%     288.475ms      72.119ms             4  
                     aten::conv2d         0.00%      64.000us        13.45%     186.652ms      12.443ms            15  
                aten::convolution         0.03%     400.000us        13.44%     186.588ms      12.439ms            15  
               aten::_convolution         0.02%     280.000us        13.41%     186.188ms      12.413ms            15  
         aten::mkldnn_convolution        13.25%     183.875ms        13.29%     184.541ms      12.303ms            15  
                     aten::linear         0.01%     115.000us         5.54%      76.905ms      19.226ms             4  
                      aten::addmm         5.50%      76.300ms         5.52%      76.641ms      19.160ms             4  
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 1.388s

---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                  model_inference         3.80%      79.209ms       100.00%        2.086s        2.086s             1  
                     aten::conv2d         0.04%     759.000us        66.00%        1.377s      10.841ms           127  
                aten::convolution         0.14%       3.015ms        65.96%        1.376s      10.835ms           127  
               aten::_convolution         0.11%       2.234ms        65.82%        1.373s      10.811ms           127  
         aten::mkldnn_convolution        65.44%        1.365s        65.71%        1.371s      10.793ms           127  
           torchvision::roi_align        14.10%     294.166ms        14.37%     299.835ms      59.967ms             5  
                 aten::batch_norm         0.02%     462.000us         3.95%      82.490ms     793.173us           104  
     aten::_batch_norm_impl_index         0.05%     948.000us         3.93%      82.028ms     788.731us           104  
          aten::native_batch_norm         3.73%      77.820ms         3.87%      80.832ms     777.231us           104  
                     aten::linear         0.00%      41.000us         3.54%      73.933ms      18.483ms             4  
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.086s
</code></pre>
</div>
</div>
<p>The following loads the saved quantized model. It’s important that the same process of <a href="https://discuss.pytorch.org/t/how-to-load-quantized-model-for-inference/140283">fusing, preparing, and converting</a> be done before loading weights since quantization significantly alters the network. For sake of completeness, we can look at a prediction from the partially quantized RCNN.</p>
<div class="cell" data-execution_count="103">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>quant_model_loaded <span class="op">=</span> FasterRCNN(</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    BackboneWithFPNQuantizeable(</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>        backbone<span class="op">=</span>resnet_101(),</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>        return_layers<span class="op">=</span>return_layers,</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>        in_channels_list<span class="op">=</span>in_channels_list,</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>        out_channels<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        extra_blocks<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        norm_layer<span class="op">=</span><span class="va">None</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    num_classes<span class="op">=</span><span class="dv">2</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>quant_model_loaded.<span class="bu">eval</span>()</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>fuse_modules(quant_model_loaded.backbone.body, [[<span class="st">'conv1'</span>, <span class="st">'bn1'</span>, <span class="st">'relu'</span>]], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k1, m1 <span class="kw">in</span> quant_model_loaded.backbone.body.named_children():</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"layer"</span> <span class="kw">in</span> k1:  <span class="co"># in sequential layer with blocks</span></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k2, m2 <span class="kw">in</span> m1.named_children():</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>            fuse_modules(m2, [[<span class="st">"conv1"</span>, <span class="st">"bn1"</span>, <span class="st">"relu1"</span>], [<span class="st">"conv2"</span>, <span class="st">"bn2"</span>, <span class="st">"relu2"</span>], [<span class="st">"conv3"</span>, <span class="st">"bn3"</span>]], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k3, m3 <span class="kw">in</span> m2.named_children():</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="st">"downsample"</span> <span class="kw">in</span> k3:  <span class="co"># fuse downsample</span></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>                    fuse_modules(m3, [[<span class="st">"0"</span>, <span class="st">"1"</span>]], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>quant_model_loaded.train()</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>quant_model_loaded.backbone.qconfig <span class="op">=</span> torch.quantization.get_default_qconfig(<span class="st">'fbgemm'</span>)</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>torch.quantization.prepare_qat(quant_model_loaded, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>torch.quantization.convert(quant_model_loaded, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>quant_model_loaded.<span class="bu">eval</span>()</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>quant_model_loaded.load_state_dict(torch.load(quant_model_path, map_location<span class="op">=</span>torch.device(<span class="st">'cpu'</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="4d133102-bb02-4356-a243-e2b089096f9b" data-execution_count="106">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.utils <span class="im">import</span> draw_bounding_boxes, draw_segmentation_masks</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> read_image(<span class="st">"PennFudanPed/PNGImages/FudanPed00022.png"</span>)  <span class="co"># 7, 22</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>eval_transform <span class="op">=</span> get_transform(train<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> eval_transform(image)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convert RGBA -&gt; RGB and move to device</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x[:<span class="dv">3</span>, ...].to(torch.device(<span class="st">'cpu'</span>))</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> quant_model_loaded([x, ])</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> predictions[<span class="dv">0</span>]</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>threshold <span class="op">=</span> <span class="fl">0.50</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> (<span class="fl">255.0</span> <span class="op">*</span> (image <span class="op">-</span> image.<span class="bu">min</span>()) <span class="op">/</span> (image.<span class="bu">max</span>() <span class="op">-</span> image.<span class="bu">min</span>())).to(torch.uint8)</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> image[:<span class="dv">3</span>, ...]</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>pred_labels <span class="op">=</span> [<span class="ss">f"pedestrian: </span><span class="sc">{</span>score<span class="sc">:.3f}</span><span class="ss">"</span> <span class="cf">for</span> label, score <span class="kw">in</span> <span class="bu">zip</span>(pred[<span class="st">"labels"</span>], pred[<span class="st">"scores"</span>]) <span class="cf">if</span> score <span class="op">&gt;</span> threshold]</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>pred_boxes <span class="op">=</span> pred[<span class="st">"boxes"</span>].<span class="bu">long</span>()[pred[<span class="st">"scores"</span>] <span class="op">&gt;</span> threshold]</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>output_image <span class="op">=</span> draw_bounding_boxes(image, pred_boxes, pred_labels, colors<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a><span class="co"># masks = (pred["masks"] &gt; 0.7).squeeze(1)</span></span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a><span class="co"># output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors="blue")</span></span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">12</span>))</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>plt.imshow(output_image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="106">
<pre><code>&lt;matplotlib.image.AxesImage at 0x781f0969da80&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="pytorch_eager_qat_files/figure-html/cell-23-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>It’s worth mentioning that I ran into all sorts of issues on my early attempts. This post is polished and makes the whole process look linear but it wasn’t. There were many attempts, breaks to research bugs, figuring out how to do something, reverting to simpler models, etc. Things failed and failed until finally they didn’t.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>