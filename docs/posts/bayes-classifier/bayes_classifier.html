<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>cbhyphen.github.io - maximum a posteriori</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">cbhyphen.github.io</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">about</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cbhyphen" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">maximum a posteriori</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Even though I’ve studied (and revisited, and revisited..) Bayesian statistics several times over the years, I always felt that, over time, my understanding would lose it’s sharpness. In my opinion, the Bayesian paradigm isn’t very intuitive. So I created this post as future reference to myself, but also as a way to dive deeper into things like the naive assumption, maximum a posterior vs maximum likelihood, and decision boundaries.</p>
<p>Let’s assume there is a random variable <span class="math inline">\(X\)</span> that follows a Gaussian distribution</p>
<p><span class="math display">\[
X \sim N(\mu, \sigma)
\]</span></p>
<p>and a variable <span class="math inline">\(Y\)</span> which is discrete</p>
<p><span class="math display">\[
Y\in\{0, 1\}
\]</span></p>
<p>Suppose we know that the value of <span class="math inline">\(Y\)</span> is dependent upon <span class="math inline">\(X\)</span>, but that the relationship is not deterministic. We can model this relationship using conditional probability</p>
<p><span class="math display">\[
P(Y=y|X=x)
\]</span></p>
<p>But say we want to assign <span class="math inline">\(Y\)</span> a definitive value (i.e., classify). In that case we can simply select the value of <span class="math inline">\(Y\)</span> with the highest probability</p>
<p><span class="math display">\[
\arg\max_ y P(Y|X)
\]</span></p>
<p>And because we are selecting a value for <span class="math inline">\(Y\)</span> when there is uncertainty, this means we are making an estimate. The above is known as the maximum a posteriori (MAP) estimate of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, and <span class="math inline">\(P(Y|X)\)</span> is commonly referred to as the posterior.</p>
<p>Most likely we won’t have knowledge of the posterior (<span class="math inline">\(Y\)</span> is unknown afterall), so we use Bayes theorem to derive an equivalence</p>
<p><span class="math display">\[
P(Y|X) = {P(Y \cap X) \over P(X)} = {P(X|Y) \cdot P(Y) \over P(X)}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(P(X|Y)\)</span> is the likelhood (i.e., probability of the data given the class)</li>
<li><span class="math inline">\(P(Y)\)</span> is the prior (i.e., probability of the class)</li>
<li><span class="math inline">\(P(X)\)</span> is the marginal (i.e., probability of the data)</li>
</ul>
<p>When performing the MAP estimate, we are given some value of <span class="math inline">\(X\)</span> and then calculate the posterior probability for each possible value of <span class="math inline">\(Y\)</span>. This means that the marginal is the same for all values of <span class="math inline">\(Y\)</span> and is just a constant that can be factored out</p>
<p><span class="math display">\[
P(Y|X) \propto {P(X|Y) \cdot P(Y)}
\]</span></p>
<p>which simplifies the MAP classifier to</p>
<p><span class="math display">\[
\arg\max_y {P(X|Y)  \cdot P(Y)}
\]</span></p>
<p>As far as the likelihood function, we made an assumption on the distribution of <span class="math inline">\(X\)</span> so we can use the Gaussian probability density function</p>
<p><span class="math display">\[
p(x|y) = \frac{1}{\sigma_y\sqrt2\pi} e ^ {- \frac{1}{2} ( \frac{x - \mu_y}{\sigma_y} ) ^2}
\]</span></p>
<p>If we don’t know the Gaussian parameters above, we just estimate them using the empirical mean and variance of the training data for each class which is a maximum likelihood estimate.</p>
<p><span class="math display">\[
\mu_y = \frac{1}{n}\sum_{i}^{n}x_i
\]</span></p>
<p><span class="math display">\[
\sigma_y^2 = \frac{1}{n}\sum_{i}^{n}(x_i - \mu_y)^2
\]</span></p>
<p>We don’t know the distribution of the prior, so we have to estimate it. In practice, we simply use the prevalence of each class in the training data which is again a maximum likelihood estimate.</p>
<p><span class="math display">\[
p(y) = \frac{1}{n}\sum_{i}^{n} \mathbb{1}(y_i = y)
\]</span></p>
<p>It’s worth noting that there is also a maximum likelihood estimate (MLE) that could be used for the classifier. As the name suggest we would just use the likelihood term and remove the prior</p>
<p><span class="math display">\[
\arg\max_y {P(X|Y)}
\]</span></p>
<p>but this ignores the prior distribution if we have that information.</p>
<p>With some basic theory out of the way, let’s build a classifer.</p>
<section id="univariate-classifier" class="level2">
<h2 class="anchored" data-anchor-id="univariate-classifier">univariate classifier</h2>
<p>Let’s simulate univariate Gaussian data for the two classes. For simplicity, the data will have different means but the same variance.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>mu_1 <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>mu_2 <span class="op">=</span> <span class="dv">80</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>x_1 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu_1, scale<span class="op">=</span>std, size<span class="op">=</span>n)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>x_2 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu_2, scale<span class="op">=</span>std, size<span class="op">=</span>n)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">'x'</span>: np.concatenate([x_1, x_2]), <span class="st">'y'</span>: [<span class="dv">1</span>] <span class="op">*</span> n <span class="op">+</span> [<span class="dv">2</span>] <span class="op">*</span> n})</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>sns.displot(df, kind<span class="op">=</span><span class="st">'kde'</span>, x<span class="op">=</span><span class="st">'x'</span>, hue<span class="op">=</span><span class="st">'y'</span>,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>            fill<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">0</span>, palette<span class="op">=</span><span class="st">'dark'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="bayes_classifier_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Time to estimate priors, means, and standard deviations. This is trivial since we generated the data but let’s pretend that we didn’t :)</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>priors <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].size <span class="op">/</span> df.size <span class="cf">for</span> k <span class="kw">in</span> df.y.unique()}</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>priors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>{1: 0.5, 2: 0.5}</code></pre>
</div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].x.mean() <span class="cf">for</span> k <span class="kw">in</span> df.y.unique()}</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>means</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>{1: 39.74917237733038, 2: 80.16187136171098}</code></pre>
</div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>stdevs <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].x.std() <span class="cf">for</span> k <span class="kw">in</span> df.y.unique()}</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># .std(ddof=0) if not sample</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>stdevs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>{1: 10.004638113965834, 2: 10.265729149219876}</code></pre>
</div>
</div>
<p>Now that the data is fit, we can build a classifier and predict new instances.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># scipy actually has gaussian pdf: from scipy.stats import norm</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> uni_gaussian_pdf(x, mean, stdev):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    scalar <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (stdev <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    exponential <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">-</span> mean) <span class="op">/</span> stdev) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scalar <span class="op">*</span> exponential</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>classes <span class="op">=</span> df.y.unique().tolist()  </span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gbayes_uni_classify(x):</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    probas <span class="op">=</span> []</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> classes:</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        likelihood <span class="op">=</span> uni_gaussian_pdf(x, means[c], stdevs[c])</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># likelihood = norm.pdf(x, means[c], stdevs[c])</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        probas.append(likelihood <span class="op">*</span> priors[c])</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> classes[np.argmax(probas)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It’s important to mention here that the priors are the same since we generated equal amounts of data for both classes. Mathematically this means that the prior is a constant and can be factored out in the original MAP equation (for this case) giving</p>
<p><span class="math display">\[
\arg\max_y {P(X|Y)}
\]</span></p>
<p>So in the case where priors are the same, the MAP is equivalent to the MLE.</p>
<p>And now to visualize the decision boundary.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>sim_data <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">150</span>, <span class="dv">1</span>)  <span class="co"># uniform sequence</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>sim_class_preds <span class="op">=</span> [gbayes_uni_classify(x) <span class="cf">for</span> x <span class="kw">in</span> sim_data]</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>decision_boundary <span class="op">=</span> np.where(np.array(sim_class_preds[:<span class="op">-</span><span class="dv">1</span>]) <span class="op">-</span> np.array(sim_class_preds[<span class="dv">1</span>:]) <span class="op">!=</span> <span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(decision_boundary)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>sns.displot(df, kind<span class="op">=</span><span class="st">'kde'</span>, x<span class="op">=</span><span class="st">'x'</span>, hue<span class="op">=</span><span class="st">'y'</span>, fill<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">0</span>, palette<span class="op">=</span><span class="st">'dark'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> v <span class="kw">in</span> sim_data[decision_boundary]:</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    plt.axvline(v, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[59]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayes_classifier_files/figure-html/cell-8-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>The decision boundary is roughly halfway between means, as expected. This isn’t super interesting but what if the variances are different?</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>mu_1 <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>mu_2 <span class="op">=</span> <span class="dv">80</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>std_1 <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>std_2 <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>x_1 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu_1, scale<span class="op">=</span>std_1, size<span class="op">=</span>n)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>x_2 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu_2, scale<span class="op">=</span>std_2, size<span class="op">=</span>n)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">'x'</span>: np.concatenate([x_1, x_2]), <span class="st">'y'</span>: [<span class="dv">1</span>] <span class="op">*</span> n <span class="op">+</span> [<span class="dv">2</span>] <span class="op">*</span> n})</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>sns.displot(df, kind<span class="op">=</span><span class="st">'kde'</span>, x<span class="op">=</span><span class="st">'x'</span>, hue<span class="op">=</span><span class="st">'y'</span>, fill<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">0</span>, palette<span class="op">=</span><span class="st">'dark'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="bayes_classifier_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># class so we don't repeat same spiel</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GBUniClf:</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classes <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.priors <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.means <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stdevs <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, df):</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classes <span class="op">=</span> df.y.unique().tolist()</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.priors <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].size <span class="op">/</span> df.size <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.means <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].x.mean() <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stdevs <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].x.std() <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> likelihood(<span class="va">self</span>, x, mean, stdev):</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        scalar <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (stdev <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        exponential <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">-</span> mean) <span class="op">/</span> stdev) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> scalar <span class="op">*</span> exponential</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        probas <span class="op">=</span> []</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> <span class="va">self</span>.classes:</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>            likelihood <span class="op">=</span> <span class="va">self</span>.likelihood(x, <span class="va">self</span>.means[c], <span class="va">self</span>.stdevs[c])</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>            probas.append(likelihood <span class="op">*</span> <span class="va">self</span>.priors[c])</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.classes[np.argmax(probas)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GBUniClf()</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>sim_data <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">50</span>, <span class="dv">200</span>, <span class="dv">1</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>sim_class_preds <span class="op">=</span> [clf.predict(x) <span class="cf">for</span> x <span class="kw">in</span> sim_data]</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>decision_boundary <span class="op">=</span> np.where(np.array(sim_class_preds[:<span class="op">-</span><span class="dv">1</span>]) <span class="op">-</span> np.array(sim_class_preds[<span class="dv">1</span>:]) <span class="op">!=</span> <span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(decision_boundary)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>df_preds <span class="op">=</span> pd.DataFrame({<span class="st">'x'</span>: sim_data, <span class="st">'y'</span>: sim_class_preds})</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>sns.displot(df, kind<span class="op">=</span><span class="st">'kde'</span>, x<span class="op">=</span><span class="st">'x'</span>, hue<span class="op">=</span><span class="st">'y'</span>, fill<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">0</span>, palette<span class="op">=</span><span class="st">'dark'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>rug_plot <span class="op">=</span> sns.rugplot(df_preds, x<span class="op">=</span><span class="st">"x"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>rug_plot.get_legend().remove()</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> v <span class="kw">in</span> sim_data[decision_boundary]:</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    plt.axvline(v, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[113 174]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayes_classifier_files/figure-html/cell-11-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Because class 1 has a larger variance, there is now a second decision boundary. Instances with high values of <span class="math inline">\(x\)</span> (far right) are less likely to belong to class 2 even though they are closer to its’ mean. Instead they get classified as 1.</p>
<p>What if the priors are different?</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>n_1 <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>n_2 <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>mu_1 <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>mu_2 <span class="op">=</span> <span class="dv">80</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>std_1 <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>std_2 <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>x_1 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu_1, scale<span class="op">=</span>std_1, size<span class="op">=</span>n_1)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>x_2 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu_2, scale<span class="op">=</span>std_2, size<span class="op">=</span>n_2)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">'x'</span>: np.concatenate([x_1, x_2]), <span class="st">'y'</span>: [<span class="dv">1</span>] <span class="op">*</span> n_1 <span class="op">+</span> [<span class="dv">2</span>] <span class="op">*</span> n_2})</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GBUniClf()</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>sim_data <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">50</span>, <span class="dv">200</span>, <span class="dv">1</span>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>sim_class_preds <span class="op">=</span> [clf.predict(x) <span class="cf">for</span> x <span class="kw">in</span> sim_data]</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>decision_boundary <span class="op">=</span> np.where(np.array(sim_class_preds[:<span class="op">-</span><span class="dv">1</span>]) <span class="op">-</span> np.array(sim_class_preds[<span class="dv">1</span>:]) <span class="op">!=</span> <span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(decision_boundary)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>df_preds <span class="op">=</span> pd.DataFrame({<span class="st">'x'</span>: sim_data, <span class="st">'y'</span>: sim_class_preds})</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>sns.displot(df, kind<span class="op">=</span><span class="st">'kde'</span>, x<span class="op">=</span><span class="st">'x'</span>, hue<span class="op">=</span><span class="st">'y'</span>, fill<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">0</span>, palette<span class="op">=</span><span class="st">'dark'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>rug_plot <span class="op">=</span> sns.rugplot(df_preds, x<span class="op">=</span><span class="st">"x"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>rug_plot.get_legend().remove()</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> v <span class="kw">in</span> sim_data[decision_boundary]:</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>    plt.axvline(v, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[120 164]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayes_classifier_files/figure-html/cell-12-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>It simply makes the more prevalent class more likely as expected.</p>
<section id="multivariate" class="level3">
<h3 class="anchored" data-anchor-id="multivariate">multivariate</h3>
<p>Now we can look at bivariate data where covariance between features come into play. The naive assumption ignores covariance so we can compare classifiers that do and do not make that assumption.</p>
<p>Mathematically, the posterior is now conditioned on multiple features</p>
<p><span class="math display">\[
P(Y|X) = P(Y|x_1, x_2, \ldots , x_i)
\]</span></p>
<p>and the MAP classifier in the multivariate case is</p>
<p><span class="math display">\[
\arg\max_y {P(Y) \cdot P(x_1, x_2, \ldots , x_i|Y)}
\]</span></p>
<p>Therefore we use the multivariate likelihood function which makes use of covariance</p>
<p><span class="math display">\[
p(x|y) = \frac{1}{\sqrt{(2\pi)^n |\Sigma_y|}} e^{ - \frac{1}{2} (x - \mu_y)^T \Sigma_y^{-1} (x - \mu_y)}
\]</span></p>
<p>This is a drop-in replacement though, and the rest of the classifier is the same.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>mu_1 <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>mu_2 <span class="op">=</span> <span class="dv">80</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>x_1 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu_1, scale<span class="op">=</span>std, size<span class="op">=</span>(n, <span class="dv">2</span>))</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>x_2 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu_2, scale<span class="op">=</span>std, size<span class="op">=</span>(n, <span class="dv">2</span>))</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.concatenate([x_1, x_2])</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">'x1'</span>: data[:, <span class="dv">0</span>], <span class="st">'x2'</span>: data[:, <span class="dv">1</span>], <span class="st">'y'</span>: [<span class="dv">1</span>] <span class="op">*</span> n <span class="op">+</span> [<span class="dv">2</span>] <span class="op">*</span> n})</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="co"># s = sns.scatterplot(df, x='x1', y='x2', hue='y', hue_order=classes, palette='dark', alpha=0.25)</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> sns.kdeplot(df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, fill<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>sns.move_legend(s, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="bayes_classifier_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GBBiClf:</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classes <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.priors <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.means <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.covars <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.covar_dets <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.covar_invs <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, df):</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classes <span class="op">=</span> df.y.unique().tolist()</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.priors <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].shape[<span class="dv">0</span>] <span class="op">/</span> df.shape[<span class="dv">0</span>] <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.means <span class="op">=</span> {k: df[[<span class="st">'x1'</span>, <span class="st">'x2'</span>]][df.y <span class="op">==</span> k].mean(axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.covars <span class="op">=</span> {k: np.cov(df[[<span class="st">'x1'</span>, <span class="st">'x2'</span>]][df.y <span class="op">==</span> k], rowvar<span class="op">=</span><span class="va">False</span>, bias<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.covar_dets <span class="op">=</span> {k: np.linalg.det(<span class="va">self</span>.covars[k]) <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.covar_invs <span class="op">=</span> {k: np.linalg.inv(<span class="va">self</span>.covars[k]) <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> likelihood(<span class="va">self</span>, x, c):</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        dims <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        scalar <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> np.sqrt(((<span class="dv">2</span> <span class="op">*</span> np.pi) <span class="op">**</span> dims) <span class="op">*</span> <span class="va">self</span>.covar_dets[c])</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        exponential <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> (x <span class="op">-</span> <span class="va">self</span>.means[c]).T <span class="op">@</span> <span class="va">self</span>.covar_invs[c] <span class="op">@</span> (x <span class="op">-</span> <span class="va">self</span>.means[c]))</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> scalar <span class="op">*</span> exponential</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        probas <span class="op">=</span> []</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> <span class="va">self</span>.classes:</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>            likelihood <span class="op">=</span> <span class="va">self</span>.likelihood(x, c)</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>            probas.append(likelihood <span class="op">*</span> <span class="va">self</span>.priors[c])</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.classes[np.argmax(probas)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GBBiClf()</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>sim_data_range <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">140</span>, <span class="dv">1</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>sim_data <span class="op">=</span> np.array([np.array([x1, x2]) <span class="cf">for</span> x1 <span class="kw">in</span> sim_data_range <span class="cf">for</span> x2 <span class="kw">in</span> sim_data_range])</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>sim_classes <span class="op">=</span> [clf.predict(x) <span class="cf">for</span> x <span class="kw">in</span> sim_data]</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>plot_df <span class="op">=</span> pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)]), columns<span class="op">=</span>[<span class="st">'x1'</span>, <span class="st">'x2'</span>, <span class="st">'y'</span>])</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co"># sns.scatterplot(plot_df, x='x1', y="x2", hue="y", hue_order=classes, palette='dark', marker=".", alpha=0.15)</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>plt_points <span class="op">=</span> sns.relplot(plot_df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'y'</span>, hue_order<span class="op">=</span>clf.classes, palette<span class="op">=</span><span class="st">'dark'</span>, marker<span class="op">=</span><span class="st">"."</span>, alpha<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>plt_points._legend.remove()</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> sns.kdeplot(df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, fill<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>sns.move_legend(s, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="bayes_classifier_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The variance was same for both distributions and the features were sampled independently, so the decision boundary isn’t complex. Slight curvature is due to the estimate of covariance which is different from the true value.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>clf.covars</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>{1: array([[97.13589025,  3.99602431],
        [ 3.99602431, 99.21513485]]),
 2: array([[104.87159824,  -2.09309889],
        [ -2.09309889, 102.99262222]])}</code></pre>
</div>
</div>
<p>Even though this data is uninteresting, let’s compare the decision boundary of a <strong>naive</strong> classifier. The naive assumption is that all features are independent, so we can use the chain rule of probability for a simpler calculation of the likelihood.</p>
<p><span class="math display">\[
P(X|Y) = P(x_1, x_2, \ldots , x_i|Y) = \prod\limits_{i}P(x_i|Y)
\]</span></p>
<p>The MAP classifier under the naive assumption then becomes</p>
<p><span class="math display">\[
\arg\max_y {P(Y) \cdot P(x_1|Y) \cdot P(x_2|Y) \cdot \ldots \cdot P(x_m|Y)}
\]</span></p>
<p>For this case though, since the features were generated independently, the decision boundary should be roughly the same.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GNBBiClf:</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classes <span class="op">=</span> <span class="va">None</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.priors <span class="op">=</span> <span class="va">None</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.means <span class="op">=</span> <span class="va">None</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, df):</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classes <span class="op">=</span> df.y.unique().tolist()</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.priors <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].shape[<span class="dv">0</span>] <span class="op">/</span> df.shape[<span class="dv">0</span>] <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.means <span class="op">=</span> {k: df[[<span class="st">'x1'</span>, <span class="st">'x2'</span>]][df.y <span class="op">==</span> k].mean(axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stdevs <span class="op">=</span> {k: np.std(df[[<span class="st">'x1'</span>, <span class="st">'x2'</span>]][df.y <span class="op">==</span> k], axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> likelihood(<span class="va">self</span>, x, mean, stdev):</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        scalar <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (stdev <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        exponential <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">-</span> mean) <span class="op">/</span> stdev) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> scalar <span class="op">*</span> exponential</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>        probas <span class="op">=</span> []</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> <span class="va">self</span>.classes:</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>            joint_likelihood <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i, v <span class="kw">in</span> <span class="bu">enumerate</span>(x):</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>                likelihood <span class="op">=</span> <span class="va">self</span>.likelihood(v, <span class="va">self</span>.means[c][i], <span class="va">self</span>.stdevs[c][i])</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>                joint_likelihood <span class="op">*=</span> likelihood</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>            probas.append(joint_likelihood <span class="op">*</span> <span class="va">self</span>.priors[c])</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.classes[np.argmax(probas)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GNBBiClf()</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>sim_data_range <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">140</span>, <span class="dv">1</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>sim_data <span class="op">=</span> np.array([np.array([x1, x2]) <span class="cf">for</span> x1 <span class="kw">in</span> sim_data_range <span class="cf">for</span> x2 <span class="kw">in</span> sim_data_range])</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>sim_classes <span class="op">=</span> [clf.predict(x) <span class="cf">for</span> x <span class="kw">in</span> sim_data]</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>plot_df <span class="op">=</span> pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)]), columns<span class="op">=</span>[<span class="st">'x1'</span>, <span class="st">'x2'</span>, <span class="st">'y'</span>])</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>plt_points <span class="op">=</span> sns.relplot(plot_df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'y'</span>, hue_order<span class="op">=</span>clf.classes, palette<span class="op">=</span><span class="st">'dark'</span>, marker<span class="op">=</span><span class="st">"."</span>, alpha<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>plt_points._legend.remove()</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> sns.kdeplot(df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, fill<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>sns.move_legend(s, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="bayes_classifier_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>What if just the covariance are different? Let’s draw random data were the features are still independent (i.e.&nbsp;covariance matrix is symmetic) but the variance of features is different for each class.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>mu_1 <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>mu_2 <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>std_1 <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>std_2 <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>x_1 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu_1, scale<span class="op">=</span>std_1, size<span class="op">=</span>(n, <span class="dv">2</span>))</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>x_2 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu_2, scale<span class="op">=</span>std_2, size<span class="op">=</span>(n, <span class="dv">2</span>))</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.concatenate([x_1, x_2])</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">'x1'</span>: data[:, <span class="dv">0</span>], <span class="st">'x2'</span>: data[:, <span class="dv">1</span>], <span class="st">'y'</span>: [<span class="dv">1</span>] <span class="op">*</span> n <span class="op">+</span> [<span class="dv">2</span>] <span class="op">*</span> n})</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> sns.kdeplot(df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, fill<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>sns.move_legend(s, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>s.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>[(-10.0, 140.0), (-10.0, 140.0)]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayes_classifier_files/figure-html/cell-19-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GBBiClf()</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>sim_data_range <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>, <span class="dv">1</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>sim_data <span class="op">=</span> np.array([np.array([x1, x2]) <span class="cf">for</span> x1 <span class="kw">in</span> sim_data_range <span class="cf">for</span> x2 <span class="kw">in</span> sim_data_range])</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>sim_classes <span class="op">=</span> [clf.predict(x) <span class="cf">for</span> x <span class="kw">in</span> sim_data]</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>plot_df <span class="op">=</span> pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)]), columns<span class="op">=</span>[<span class="st">'x1'</span>, <span class="st">'x2'</span>, <span class="st">'y'</span>])</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>plt_points <span class="op">=</span> sns.relplot(plot_df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'y'</span>, hue_order<span class="op">=</span>clf.classes, palette<span class="op">=</span><span class="st">'dark'</span>, marker<span class="op">=</span><span class="st">"."</span>, alpha<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>plt_points._legend.remove()</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> sns.kdeplot(df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, fill<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>sns.move_legend(s, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>s.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>[(-10.0, 140.0), (-10.0, 140.0)]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayes_classifier_files/figure-html/cell-20-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>As expected, the decision boundary favors class 1 since it had larger variance. Without any correlation between features, I would again expect the decision boundary to be the same for the naive classifier.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GNBBiClf()</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>sim_data_range <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>, <span class="dv">1</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>sim_data <span class="op">=</span> np.array([np.array([x1, x2]) <span class="cf">for</span> x1 <span class="kw">in</span> sim_data_range <span class="cf">for</span> x2 <span class="kw">in</span> sim_data_range])</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>sim_classes <span class="op">=</span> [clf.predict(x) <span class="cf">for</span> x <span class="kw">in</span> sim_data]</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="co"># sanity check</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="co"># from sklearn.naive_bayes import GaussianNB</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co"># clf = GaussianNB()</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="co"># clf.fit(df[['x1', 'x2']].values, df[['y']].values.ravel())</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="co"># sim_data_range = np.arange(-10, 140, 1)</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="co"># sim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="co"># sim_classes = [clf.predict(x.reshape(1, -1)) for x in sim_data]</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>plot_df <span class="op">=</span> pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)]), columns<span class="op">=</span>[<span class="st">'x1'</span>, <span class="st">'x2'</span>, <span class="st">'y'</span>])</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>plt_points <span class="op">=</span> sns.relplot(plot_df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'y'</span>, hue_order<span class="op">=</span>clf.classes, palette<span class="op">=</span><span class="st">'dark'</span>, marker<span class="op">=</span><span class="st">"."</span>, alpha<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>plt_points._legend.remove()</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> sns.kdeplot(df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, fill<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>sns.move_legend(s, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>s.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>[(-10.0, 140.0), (-10.0, 140.0)]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayes_classifier_files/figure-html/cell-21-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>The decision boundary for the naive classifier is roughly identical. Zooming out, we can see the classifier has similar behavior as the univariate case for different variance.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GNBBiClf()</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>sim_data_range <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">10</span>, <span class="dv">200</span>, <span class="dv">1</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>sim_data <span class="op">=</span> np.array([np.array([x1, x2]) <span class="cf">for</span> x1 <span class="kw">in</span> sim_data_range <span class="cf">for</span> x2 <span class="kw">in</span> sim_data_range])</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>sim_classes <span class="op">=</span> [clf.predict(x) <span class="cf">for</span> x <span class="kw">in</span> sim_data]</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>plot_df <span class="op">=</span> pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)]), columns<span class="op">=</span>[<span class="st">'x1'</span>, <span class="st">'x2'</span>, <span class="st">'y'</span>])</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>plt_points <span class="op">=</span> sns.relplot(plot_df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'y'</span>, hue_order<span class="op">=</span>clf.classes, palette<span class="op">=</span><span class="st">'dark'</span>, marker<span class="op">=</span><span class="st">"."</span>, alpha<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>plt_points._legend.remove()</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> sns.kdeplot(df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, fill<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>sns.move_legend(s, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>s.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">200</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">200</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>[(-10.0, 200.0), (-10.0, 200.0)]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayes_classifier_files/figure-html/cell-22-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s finally simulate data with correlation between features. There should be a noticeable difference in the decision boundary for the naive classifier.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>mu_1 <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>mu_2 <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>x_1 <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>[mu_1, mu_1], cov<span class="op">=</span>[[<span class="dv">50</span>, <span class="dv">70</span>], [<span class="dv">70</span>, <span class="dv">200</span>]], size<span class="op">=</span>n)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>x_2 <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>[mu_2, mu_2], cov<span class="op">=</span>[[<span class="dv">100</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">100</span>]], size<span class="op">=</span>n)  <span class="co"># no correlation</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.concatenate([x_1, x_2])</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">'x1'</span>: data[:, <span class="dv">0</span>], <span class="st">'x2'</span>: data[:, <span class="dv">1</span>], <span class="st">'y'</span>: [<span class="dv">1</span>] <span class="op">*</span> n <span class="op">+</span> [<span class="dv">2</span>] <span class="op">*</span> n})</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> sns.kdeplot(df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, fill<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>sns.move_legend(s, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>s.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>[(-10.0, 140.0), (-10.0, 140.0)]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayes_classifier_files/figure-html/cell-23-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GBBiClf()</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>sim_data_range <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>, <span class="dv">1</span>)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>sim_data <span class="op">=</span> np.array([np.array([x1, x2]) <span class="cf">for</span> x1 <span class="kw">in</span> sim_data_range <span class="cf">for</span> x2 <span class="kw">in</span> sim_data_range])</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>sim_classes <span class="op">=</span> [clf.predict(x) <span class="cf">for</span> x <span class="kw">in</span> sim_data]</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>plot_df <span class="op">=</span> pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)]), columns<span class="op">=</span>[<span class="st">'x1'</span>, <span class="st">'x2'</span>, <span class="st">'y'</span>])</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>plt_points <span class="op">=</span> sns.relplot(plot_df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'y'</span>, hue_order<span class="op">=</span>clf.classes, palette<span class="op">=</span><span class="st">'dark'</span>, marker<span class="op">=</span><span class="st">"."</span>, alpha<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>plt_points._legend.remove()</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> sns.kdeplot(df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, fill<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>sns.move_legend(s, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>s.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>[(-10.0, 140.0), (-10.0, 140.0)]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayes_classifier_files/figure-html/cell-24-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GNBBiClf()</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>sim_data_range <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>, <span class="dv">1</span>)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>sim_data <span class="op">=</span> np.array([np.array([x1, x2]) <span class="cf">for</span> x1 <span class="kw">in</span> sim_data_range <span class="cf">for</span> x2 <span class="kw">in</span> sim_data_range])</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>sim_classes <span class="op">=</span> [clf.predict(x) <span class="cf">for</span> x <span class="kw">in</span> sim_data]</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>plot_df <span class="op">=</span> pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)]), columns<span class="op">=</span>[<span class="st">'x1'</span>, <span class="st">'x2'</span>, <span class="st">'y'</span>])</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>plt_points <span class="op">=</span> sns.relplot(plot_df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'y'</span>, hue_order<span class="op">=</span>clf.classes, palette<span class="op">=</span><span class="st">'dark'</span>, marker<span class="op">=</span><span class="st">"."</span>, alpha<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>plt_points._legend.remove()</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> sns.kdeplot(df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, fill<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>sns.move_legend(s, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>s.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>[(-10.0, 140.0), (-10.0, 140.0)]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayes_classifier_files/figure-html/cell-25-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>The difference between the naive and non-naive classifier is more noticeable when there is correlation between features (class 1). The naive classifier clearly ignores the covariance and the decision boundary is much smoother.</p>
<p>One obvious advantage of the naive assumption is computational efficiency. Predictions for the naive classifier ran in faster time compared to the non-naive by an order of magnitude. Fit times were roughly the same.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> time <span class="im">import</span> perf_counter</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">TODO</span><span class="co"> memory reqs</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> perf_counter()</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> GBBiClf()</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    clf.fit(df)</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GB avg fit time: </span><span class="sc">{</span>(perf_counter() <span class="op">-</span> start_time) <span class="op">/</span> m<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> perf_counter()</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    clf.predict(sim_data[i])</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GB avg predict time: </span><span class="sc">{</span>(perf_counter() <span class="op">-</span> start_time) <span class="op">/</span> m<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> perf_counter()</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> GNBBiClf()</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>    clf.fit(df)</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GNB avg fit time: </span><span class="sc">{</span>(perf_counter() <span class="op">-</span> start_time) <span class="op">/</span> m<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> perf_counter()</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>    clf.predict(sim_data[i])</span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GNB avg predict time: </span><span class="sc">{</span>(perf_counter() <span class="op">-</span> start_time) <span class="op">/</span> m<span class="sc">:.6f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>GB avg fit time: 0.004047
GB avg predict time: 0.000654
GNB avg fit time: 0.005097
GNB avg predict time: 0.000047</code></pre>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>