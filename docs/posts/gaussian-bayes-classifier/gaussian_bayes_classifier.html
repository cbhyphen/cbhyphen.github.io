<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="chris">
<meta name="dcterms.date" content="2023-10-01">

<title>Gaussian Bayesian Classifiers – cbhyphen.github.io</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">cbhyphen.github.io</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">about</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cbhyphen"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Gaussian Bayesian Classifiers</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>chris </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 1, 2023</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">November 29, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Even though I studied (and revisited, and revisited..) Bayesian statistics several times over the years, I’ve always felt that it wasn’t an intuitive paradigm. Specifically, the bridge from conditional probability and Bayes theorem to Bayesian classifiers is something I’ve had to refresh. I created this post as a future reference but also as an excuse to do an in-depth review. Here I’ll derive the classifier, touch on maximum a posteriori and maximum likelihood, show what it means to make a naive assumption, and analyze decision boundaries under different conditions.</p>
<section id="maximum-a-posteriori" class="level3">
<h3 class="anchored" data-anchor-id="maximum-a-posteriori">maximum a posteriori</h3>
<p>Let’s assume we have some data <span class="math inline">\(X\)</span> that follows a Gaussian distribution</p>
<p><span class="math display">\[
X \sim N(\mu, \sigma)
\]</span></p>
<p>and a variable <span class="math inline">\(Y\)</span> which is discrete</p>
<p><span class="math display">\[
Y\in\{0, 1\}
\]</span></p>
<p>Suppose we know that the value of <span class="math inline">\(Y\)</span> is dependent upon <span class="math inline">\(X\)</span>, but that the relationship is not deterministic. We can model this relationship using conditional probability</p>
<p><span class="math display">\[
P(Y=y|X=x)
\]</span></p>
<p>But say we want to assign <span class="math inline">\(Y\)</span> a definitive value (i.e., classify). In that case we can simply select the value of <span class="math inline">\(Y\)</span> with the highest probability</p>
<p><span class="math display">\[
\arg\max_ y P(Y|X)
\]</span></p>
<p>And because we are selecting a value for <span class="math inline">\(Y\)</span> when there is uncertainty, this means we are making an estimate. The above is known as the maximum a posteriori (MAP) estimate of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, and <span class="math inline">\(P(Y|X)\)</span> is commonly referred to as the posterior.</p>
<p>Most likely we won’t have knowledge of the posterior (<span class="math inline">\(Y\)</span> is unknown afterall), so we use Bayes theorem to derive an equivalence</p>
<p><span class="math display">\[
P(Y|X) = {P(Y \cap X) \over P(X)} = {P(X|Y) \cdot P(Y) \over P(X)}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(P(X|Y)\)</span> is the likelhood (i.e., probability of the data given the class)</li>
<li><span class="math inline">\(P(Y)\)</span> is the prior (i.e., probability of the class)</li>
<li><span class="math inline">\(P(X)\)</span> is the marginal (i.e., probability of the data)</li>
</ul>
<p>When performing the MAP estimate, we are given some value of <span class="math inline">\(X\)</span> and then calculate the posterior probability for each possible value of <span class="math inline">\(Y\)</span>. This means that the marginal is the same for all values of <span class="math inline">\(Y\)</span> and is just a constant that can be factored out</p>
<p><span class="math display">\[
P(Y|X) \propto {P(X|Y) \cdot P(Y)}
\]</span></p>
<p>which simplifies the MAP classifier to</p>
<p><span class="math display">\[
\arg\max_y {P(X|Y)  \cdot P(Y)}
\]</span></p>
<p>As far as the likelihood function, we made an assumption on the distribution of <span class="math inline">\(X\)</span> so we can use the Gaussian probability density function</p>
<p><span class="math display">\[
p(x|y) = \frac{1}{\sigma_y\sqrt2\pi} e ^ {- \frac{1}{2} ( \frac{x - \mu_y}{\sigma_y} ) ^2}
\]</span></p>
<p>If we don’t know the Gaussian parameters above, we just estimate them using the empirical mean and variance of the training data for each class which is a maximum likelihood estimate.</p>
<p><span class="math display">\[
\mu_y = \frac{1}{n}\sum_{i}^{n}x_i
\]</span></p>
<p><span class="math display">\[
\sigma_y^2 = \frac{1}{n}\sum_{i}^{n}(x_i - \mu_y)^2
\]</span></p>
<p>We don’t know the distribution of the prior, so we have to estimate it. In practice, we simply use the prevalence of each class in the training data which is again a maximum likelihood estimate.</p>
<p><span class="math display">\[
p(y) = \frac{1}{n}\sum_{i}^{n} \mathbb{1}(y_i = y)
\]</span></p>
<p>It’s worth noting that there is also a maximum likelihood estimate (MLE) that could be used for the classifier. As the name suggest we would just use the likelihood term and remove the prior</p>
<p><span class="math display">\[
\arg\max_y {P(X|Y)}
\]</span></p>
<p>but this ignores the prior distribution if we have that information. So if the prior is available, then it’s advantageous to use it.</p>
<p>With some basic theory out of the way, let’s build a classifer.</p>
</section>
<section id="univariate-classifier" class="level3">
<h3 class="anchored" data-anchor-id="univariate-classifier">univariate classifier</h3>
<p>Let’s simulate univariate Gaussian data for two classes. For simplicity, the data will have different means but the same variance.</p>
<div id="2c2d5299" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1061d9b4" class="cell" data-outputid="53ce2bc2-b339-4a0a-a6d0-abb5c0f156a3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>mu_1 <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>mu_2 <span class="op">=</span> <span class="dv">80</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>x_1 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu_1, scale<span class="op">=</span>std, size<span class="op">=</span>n)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>x_2 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu_2, scale<span class="op">=</span>std, size<span class="op">=</span>n)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">'x'</span>: np.concatenate([x_1, x_2]), <span class="st">'y'</span>: [<span class="dv">1</span>] <span class="op">*</span> n <span class="op">+</span> [<span class="dv">2</span>] <span class="op">*</span> n})</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>sns.displot(df, kind<span class="op">=</span><span class="st">'kde'</span>, x<span class="op">=</span><span class="st">'x'</span>, hue<span class="op">=</span><span class="st">'y'</span>,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>            fill<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">0</span>, palette<span class="op">=</span><span class="st">'dark'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="gaussian_bayes_classifier_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Time to estimate priors, means, and standard deviations. This is trivial since we generated the data but let’s pretend that we didn’t :)</p>
<div id="f09dfb66" class="cell" data-outputid="33ff8a7e-31f4-40f8-d372-242e090b06bc">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>priors <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].size <span class="op">/</span> df.size <span class="cf">for</span> k <span class="kw">in</span> df.y.unique()}</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>priors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>{1: 0.5, 2: 0.5}</code></pre>
</div>
</div>
<div id="dc22fa4c" class="cell" data-outputid="1de6e2b6-40bc-4b3e-9680-cdbed1fb2575">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].x.mean() <span class="cf">for</span> k <span class="kw">in</span> df.y.unique()}</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>means</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>{1: 39.81007946653686, 2: 79.64703348340272}</code></pre>
</div>
</div>
<div id="6c0e5460" class="cell" data-outputid="6ea0f2ee-f8cf-41dd-e973-6e421dd897eb">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>stdevs <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].x.std() <span class="cf">for</span> k <span class="kw">in</span> df.y.unique()}</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># .std(ddof=0) if not sample</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>stdevs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>{1: 9.829502308649815, 2: 9.922409640186507}</code></pre>
</div>
</div>
<p>Now that the data is fit, we can build a classifier and predict new instances.</p>
<div id="30710db3" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># note: scipy actually has gaussian pdf: from scipy.stats import norm</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> uni_gaussian_pdf(x, mean, stdev):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    scalar <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (stdev <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    exponential <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">-</span> mean) <span class="op">/</span> stdev) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scalar <span class="op">*</span> exponential</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>classes <span class="op">=</span> df.y.unique().tolist()</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gbayes_uni_classify(x):</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    probas <span class="op">=</span> []</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> classes:</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        likelihood <span class="op">=</span> uni_gaussian_pdf(x, means[c], stdevs[c])</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># likelihood = norm.pdf(x, means[c], stdevs[c])</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        probas.append(likelihood <span class="op">*</span> priors[c])</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> classes[np.argmax(probas)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It’s important to mention here that the priors are the same since we generated equal amounts of data for both classes. Mathematically this means that the prior is a constant and can be factored out in the original MAP equation (for this case) giving</p>
<p><span class="math display">\[
\arg\max_y {P(X|Y)}
\]</span></p>
<p>So in the case where priors are the same, the MAP is equivalent to the MLE.</p>
<p>And now to visualize the decision boundary.</p>
<div id="01033fe3" class="cell" data-outputid="6e0a9c1c-3987-4590-c979-c3b2071b7184">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>sim_data <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">150</span>, <span class="dv">1</span>)  <span class="co"># uniform sequence</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>sim_class_preds <span class="op">=</span> [gbayes_uni_classify(x) <span class="cf">for</span> x <span class="kw">in</span> sim_data]</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>decision_boundary <span class="op">=</span> np.where(np.array(sim_class_preds[:<span class="op">-</span><span class="dv">1</span>]) <span class="op">-</span> np.array(sim_class_preds[<span class="dv">1</span>:]) <span class="op">!=</span> <span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(decision_boundary)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>sns.displot(df, kind<span class="op">=</span><span class="st">'kde'</span>, x<span class="op">=</span><span class="st">'x'</span>, hue<span class="op">=</span><span class="st">'y'</span>, fill<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">0</span>, palette<span class="op">=</span><span class="st">'dark'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> v <span class="kw">in</span> sim_data[decision_boundary]:</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    plt.axvline(v, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[59]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="gaussian_bayes_classifier_files/figure-html/cell-8-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>No surprises here, the decision boundary is roughly halfway between means, as expected. Let’s make things more interesting and make the variances unequal…</p>
<div id="49cdc554" class="cell" data-outputid="5cfd5ccd-8cf9-452c-c51e-e35e19b16295">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gen_uni_dists_df(n1, n2, mu1, mu2, std1, std2):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    x1 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu1, scale<span class="op">=</span>std1, size<span class="op">=</span>n1)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    x2 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu2, scale<span class="op">=</span>std2, size<span class="op">=</span>n2)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame({<span class="st">'x'</span>: np.concatenate([x1, x2]), <span class="st">'y'</span>: [<span class="dv">1</span>] <span class="op">*</span> n1 <span class="op">+</span> [<span class="dv">2</span>] <span class="op">*</span> n2})</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> gen_uni_dists_df(n1<span class="op">=</span><span class="dv">1000</span>, n2<span class="op">=</span><span class="dv">1000</span>, mu1<span class="op">=</span><span class="dv">40</span>, mu2<span class="op">=</span><span class="dv">80</span>, std1<span class="op">=</span><span class="dv">20</span>, std2<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>sns.displot(df, kind<span class="op">=</span><span class="st">'kde'</span>, x<span class="op">=</span><span class="st">'x'</span>, hue<span class="op">=</span><span class="st">'y'</span>, fill<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">0</span>, palette<span class="op">=</span><span class="st">'dark'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="gaussian_bayes_classifier_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="04c174ee" class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GBUniClf:</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classes <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.priors <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.means <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stdevs <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, df):</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classes <span class="op">=</span> df.y.unique().tolist()</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.priors <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].size <span class="op">/</span> df.size <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.means <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].x.mean() <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stdevs <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].x.std() <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> likelihood(<span class="va">self</span>, x, mean, stdev):</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        scalar <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (stdev <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        exponential <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">-</span> mean) <span class="op">/</span> stdev) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> scalar <span class="op">*</span> exponential</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        probas <span class="op">=</span> []</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> <span class="va">self</span>.classes:</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>            likelihood <span class="op">=</span> <span class="va">self</span>.likelihood(x, <span class="va">self</span>.means[c], <span class="va">self</span>.stdevs[c])</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>            probas.append(likelihood <span class="op">*</span> <span class="va">self</span>.priors[c])</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.classes[np.argmax(probas)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="_tx-yuX9Jjga" class="cell" data-outputid="4daf5128-cf5c-4ca9-b289-0f445f02d60f">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> uni_decision_boundary(clf, df, sim_range):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    sim_class_preds <span class="op">=</span> [clf.predict(x) <span class="cf">for</span> x <span class="kw">in</span> sim_range]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    decision_boundary <span class="op">=</span> np.where(np.array(sim_class_preds[:<span class="op">-</span><span class="dv">1</span>]) <span class="op">-</span> np.array(sim_class_preds[<span class="dv">1</span>:]) <span class="op">!=</span> <span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(decision_boundary)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    df_preds <span class="op">=</span> pd.DataFrame({<span class="st">'x'</span>: sim_range, <span class="st">'y'</span>: sim_class_preds})</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    sns.displot(df, kind<span class="op">=</span><span class="st">'kde'</span>, x<span class="op">=</span><span class="st">'x'</span>, hue<span class="op">=</span><span class="st">'y'</span>, fill<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">0</span>, palette<span class="op">=</span><span class="st">'dark'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    rug_plot <span class="op">=</span> sns.rugplot(df_preds, x<span class="op">=</span><span class="st">"x"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    rug_plot.get_legend().remove()</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> v <span class="kw">in</span> sim_range[decision_boundary]:</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        plt.axvline(v, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GBUniClf()</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>sim_range <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">50</span>, <span class="dv">200</span>, <span class="dv">1</span>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>uni_decision_boundary(clf, df, sim_range)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[113 175]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="gaussian_bayes_classifier_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Because class 1 has a larger variance, there is now a second decision boundary. Instances with high values of <span class="math inline">\(x\)</span> (far right) are less likely to belong to class 2 even though they are closer to its’ mean. Instead they get classified as 1.</p>
<p>What if the priors are different?</p>
<div id="O83BJ1LoKtWd" class="cell" data-outputid="260e399a-30a1-412c-ff2e-096f950bf5d0">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> gen_uni_dists_df(n1<span class="op">=</span><span class="dv">2000</span>, n2<span class="op">=</span><span class="dv">500</span>, mu1<span class="op">=</span><span class="dv">40</span>, mu2<span class="op">=</span><span class="dv">80</span>, std1<span class="op">=</span><span class="dv">20</span>, std2<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GBUniClf()</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>sim_range <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">50</span>, <span class="dv">200</span>, <span class="dv">1</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>uni_decision_boundary(clf, df, sim_range)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[120 163]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="gaussian_bayes_classifier_files/figure-html/cell-12-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It simply makes the more prevalent class more likely, not unexpected.</p>
</section>
<section id="multivariate" class="level3">
<h3 class="anchored" data-anchor-id="multivariate">multivariate</h3>
<p>Now we can look at bivariate data where covariance between features come into play. The naive assumption ignores covariance so we can compare classifiers that do and do not make that assumption.</p>
<p>Mathematically, the posterior is now conditioned on multiple features</p>
<p><span class="math display">\[
P(Y|X) = P(Y|x_1, x_2, \ldots , x_i)
\]</span></p>
<p>and the MAP classifier in the multivariate case is</p>
<p><span class="math display">\[
\arg\max_y {P(Y) \cdot P(x_1, x_2, \ldots , x_i|Y)}
\]</span></p>
<p>Therefore we use the multivariate likelihood function which makes use of covariance</p>
<p><span class="math display">\[
p(x|y) = \frac{1}{\sqrt{(2\pi)^n |\Sigma_y|}} e^{ - \frac{1}{2} (x - \mu_y)^T \Sigma_y^{-1} (x - \mu_y)}
\]</span></p>
<p>This is a drop-in replacement though, and the rest of the classifier is the same.</p>
<div id="rxWvSL6vLz0Y" class="cell" data-outputid="cf761abd-aa98-474b-bc3e-2a2751adcd80">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gen_bi_dists_df(n1, n2, mu1, mu2, std1, std2):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    x1 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu1, scale<span class="op">=</span>std1, size<span class="op">=</span>(n1, <span class="dv">2</span>))</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    x2 <span class="op">=</span> np.random.normal(loc<span class="op">=</span>mu2, scale<span class="op">=</span>std2, size<span class="op">=</span>(n2, <span class="dv">2</span>))</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> np.concatenate([x1, x2])</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame({<span class="st">'x1'</span>: data[:, <span class="dv">0</span>], <span class="st">'x2'</span>: data[:, <span class="dv">1</span>], <span class="st">'y'</span>: [<span class="dv">1</span>] <span class="op">*</span> n1 <span class="op">+</span> [<span class="dv">2</span>] <span class="op">*</span> n2})</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> gen_bi_dists_df(n1<span class="op">=</span><span class="dv">1000</span>, n2<span class="op">=</span><span class="dv">1000</span>, mu1<span class="op">=</span><span class="dv">40</span>, mu2<span class="op">=</span><span class="dv">80</span>, std1<span class="op">=</span><span class="dv">10</span>, std2<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co"># s = sns.scatterplot(df, x='x1', y='x2', hue='y', hue_order=classes, palette='dark', alpha=0.25)</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> sns.kdeplot(df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, fill<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>sns.move_legend(s, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="gaussian_bayes_classifier_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="adf0a03a" class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GBBiClf:</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classes <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.priors <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.means <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.covars <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.covar_dets <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.covar_invs <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, df):</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classes <span class="op">=</span> df.y.unique().tolist()</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.priors <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].shape[<span class="dv">0</span>] <span class="op">/</span> df.shape[<span class="dv">0</span>] <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.means <span class="op">=</span> {k: df[[<span class="st">'x1'</span>, <span class="st">'x2'</span>]][df.y <span class="op">==</span> k].mean(axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.covars <span class="op">=</span> {k: np.cov(df[[<span class="st">'x1'</span>, <span class="st">'x2'</span>]][df.y <span class="op">==</span> k], rowvar<span class="op">=</span><span class="va">False</span>, bias<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.covar_dets <span class="op">=</span> {k: np.linalg.det(<span class="va">self</span>.covars[k]) <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.covar_invs <span class="op">=</span> {k: np.linalg.inv(<span class="va">self</span>.covars[k]) <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> likelihood(<span class="va">self</span>, x, c):</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        dims <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        scalar <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> np.sqrt(((<span class="dv">2</span> <span class="op">*</span> np.pi) <span class="op">**</span> dims) <span class="op">*</span> <span class="va">self</span>.covar_dets[c])</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        exponential <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> (x <span class="op">-</span> <span class="va">self</span>.means[c]).T <span class="op">@</span> <span class="va">self</span>.covar_invs[c] <span class="op">@</span> (x <span class="op">-</span> <span class="va">self</span>.means[c]))</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> scalar <span class="op">*</span> exponential</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        probas <span class="op">=</span> []</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> <span class="va">self</span>.classes:</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>            likelihood <span class="op">=</span> <span class="va">self</span>.likelihood(x, c)</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>            probas.append(likelihood <span class="op">*</span> <span class="va">self</span>.priors[c])</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.classes[np.argmax(probas)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="BBIehCKHMhV6" class="cell" data-outputid="f4190e2a-99a9-4433-ffd8-a30e664478dc">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bi_decision_boundary(clf, df, sim_range):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    sim_data <span class="op">=</span> np.array([np.array([x1, x2]) <span class="cf">for</span> x1 <span class="kw">in</span> sim_range <span class="cf">for</span> x2 <span class="kw">in</span> sim_range])</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    sim_classes <span class="op">=</span> [clf.predict(x) <span class="cf">for</span> x <span class="kw">in</span> sim_data]</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    plot_df <span class="op">=</span> pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)]), columns<span class="op">=</span>[<span class="st">'x1'</span>, <span class="st">'x2'</span>, <span class="st">'y'</span>])</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># sns.scatterplot(plot_df, x='x1', y="x2", hue="y", hue_order=classes, palette='dark', marker=".", alpha=0.15)</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    plt_points <span class="op">=</span> sns.relplot(plot_df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">'x2'</span>, hue<span class="op">=</span><span class="st">'y'</span>, hue_order<span class="op">=</span>clf.classes, palette<span class="op">=</span><span class="st">'dark'</span>, marker<span class="op">=</span><span class="st">"."</span>, alpha<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    plt_points._legend.remove()</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> sns.kdeplot(df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, fill<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    sns.move_legend(s, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> s</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GBBiClf()</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>sim_range <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">140</span>, <span class="dv">1</span>)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>bi_decision_boundary(clf, df, sim_range)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="gaussian_bayes_classifier_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The variance was same for both distributions and the features were sampled independently, so the decision boundary isn’t complex. Slight curvature is due to the estimate of covariance which is different from the true value.</p>
<div id="90523d7d" class="cell" data-outputid="c831ac48-6eff-4883-ff42-526eb90a969c">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>clf.covars</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>{1: array([[100.87705238,  -3.73052293],
        [ -3.73052293,  99.42549259]]),
 2: array([[106.57505726,  -3.75129152],
        [ -3.75129152, 107.04125472]])}</code></pre>
</div>
</div>
<p>Even though this data is uninteresting, let’s compare the decision boundary of a <strong>naive</strong> classifier. The naive assumption is that all features are independent, so we can use the chain rule of probability for a simpler calculation of the likelihood.</p>
<p><span class="math display">\[
P(X|Y) = P(x_1, x_2, \ldots , x_i|Y) = \prod\limits_{i}P(x_i|Y)
\]</span></p>
<p>The MAP classifier under the naive assumption then becomes</p>
<p><span class="math display">\[
\arg\max_y {P(Y) \cdot P(x_1|Y) \cdot P(x_2|Y) \cdot \ldots \cdot P(x_m|Y)}
\]</span></p>
<p>For this case though, since the features were generated independently, the decision boundary should be roughly the same.</p>
<div id="517cede1" class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GNBBiClf:</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classes <span class="op">=</span> <span class="va">None</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.priors <span class="op">=</span> <span class="va">None</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.means <span class="op">=</span> <span class="va">None</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, df):</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classes <span class="op">=</span> df.y.unique().tolist()</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.priors <span class="op">=</span> {k: df[df.y <span class="op">==</span> k].shape[<span class="dv">0</span>] <span class="op">/</span> df.shape[<span class="dv">0</span>] <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.means <span class="op">=</span> {k: df[[<span class="st">'x1'</span>, <span class="st">'x2'</span>]][df.y <span class="op">==</span> k].mean(axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stdevs <span class="op">=</span> {k: np.std(df[[<span class="st">'x1'</span>, <span class="st">'x2'</span>]][df.y <span class="op">==</span> k], axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.classes}</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> likelihood(<span class="va">self</span>, x, mean, stdev):</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        scalar <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (stdev <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        exponential <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">-</span> mean) <span class="op">/</span> stdev) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> scalar <span class="op">*</span> exponential</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>        probas <span class="op">=</span> []</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> <span class="va">self</span>.classes:</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>            joint_likelihood <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i, v <span class="kw">in</span> <span class="bu">enumerate</span>(x):</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>                likelihood <span class="op">=</span> <span class="va">self</span>.likelihood(v, <span class="va">self</span>.means[c][i], <span class="va">self</span>.stdevs[c][i])</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>                joint_likelihood <span class="op">*=</span> likelihood</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>            probas.append(joint_likelihood <span class="op">*</span> <span class="va">self</span>.priors[c])</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.classes[np.argmax(probas)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="GQQx7jOKN_bP" class="cell" data-outputid="2fc47944-7f12-4648-d09c-2f7619c26e99">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GNBBiClf()</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>sim_range <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">140</span>, <span class="dv">1</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>bi_decision_boundary(clf, df, sim_range)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="gaussian_bayes_classifier_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>What if just the covariance are different? Let’s draw random data were the features are still independent (i.e.&nbsp;covariance matrix is symmetic) but the variance of features is different for each class.</p>
<div id="tiJ8bAdLOQi1" class="cell" data-outputid="5d633f73-10b7-4642-e2da-023f4615125d">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> gen_bi_dists_df(n1<span class="op">=</span><span class="dv">1000</span>, n2<span class="op">=</span><span class="dv">1000</span>, mu1<span class="op">=</span><span class="dv">40</span>, mu2<span class="op">=</span><span class="dv">100</span>, std1<span class="op">=</span><span class="dv">20</span>, std2<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GBBiClf()</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>sim_range <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">140</span>, <span class="dv">1</span>)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>plot <span class="op">=</span> bi_decision_boundary(clf, df, sim_range)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>plot.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="gaussian_bayes_classifier_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As expected, the decision boundary favors class 1 since it had larger variance. Without any correlation between features, I would again expect the decision boundary to be the same for the naive classifier.</p>
<div id="RiM92CPLP2F4" class="cell" data-outputid="ea56f204-9fd2-4ad5-c650-df66e3153057">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GNBBiClf()</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># sanity check</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># from sklearn.naive_bayes import GaussianNB</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co"># clf = GaussianNB()</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>sim_range <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">10</span>, <span class="dv">200</span>, <span class="dv">1</span>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>plot <span class="op">=</span> bi_decision_boundary(clf, df, sim_range)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>plot.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">200</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">200</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="gaussian_bayes_classifier_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The decision boundary for the naive classifier is roughly identical. Zooming out, we can see the classifier has similar behavior as the univariate case for different variance.</p>
<p>Let’s finally simulate data with correlation between features. There should be a noticeable difference in the decision boundary for the naive classifier.</p>
<div id="4e53adeb" class="cell" data-outputid="1a81f63e-d10f-4882-e459-7db32b87b8c3">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>mu_1 <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>mu_2 <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>x_1 <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>[mu_1, mu_1], cov<span class="op">=</span>[[<span class="dv">50</span>, <span class="dv">70</span>], [<span class="dv">70</span>, <span class="dv">200</span>]], size<span class="op">=</span>n)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>x_2 <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>[mu_2, mu_2], cov<span class="op">=</span>[[<span class="dv">100</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">100</span>]], size<span class="op">=</span>n)  <span class="co"># no correlation</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.concatenate([x_1, x_2])</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">'x1'</span>: data[:, <span class="dv">0</span>], <span class="st">'x2'</span>: data[:, <span class="dv">1</span>], <span class="st">'y'</span>: [<span class="dv">1</span>] <span class="op">*</span> n <span class="op">+</span> [<span class="dv">2</span>] <span class="op">*</span> n})</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> sns.kdeplot(df, x<span class="op">=</span><span class="st">'x1'</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span><span class="st">"y"</span>, palette<span class="op">=</span><span class="st">'dark'</span>, fill<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>sns.move_legend(s, <span class="st">"upper left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>s.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="gaussian_bayes_classifier_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="9rR1iKcWRLZ1" class="cell" data-outputid="6e7360ed-6a30-4024-8a91-5ef138d9ae84">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GBBiClf()</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>sim_range <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>, <span class="dv">1</span>)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>plot <span class="op">=</span> bi_decision_boundary(clf, df, sim_range)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>s.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="gaussian_bayes_classifier_files/figure-html/cell-22-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="7gK1vZK7RkVn" class="cell" data-outputid="f10524a3-340b-4b92-95f3-7ad1aa6223f4">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># naive classifier</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GNBBiClf()</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>clf.fit(df)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>sim_range <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>, <span class="dv">1</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>plot <span class="op">=</span> bi_decision_boundary(clf, df, sim_range)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>s.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">140</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="gaussian_bayes_classifier_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The difference between the naive and non-naive classifier is more noticeable when there is correlation between features (class 1). The naive classifier clearly ignores covariance and the decision boundary is much smoother.</p>
<p>One advantage of the naive assumption is computational efficiency. Each feature is treated independently and there is no need to do matrix multiplication with the covariance matrix. As a result, predictions for the naive classifier ran in faster time compared to non-naive by an order of magnitude.</p>
<div id="e06a3304" class="cell" data-outputid="8625196d-c15d-4ebe-c165-5faa6094133a">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> time <span class="im">import</span> perf_counter</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">TODO</span><span class="co"> memory reqs</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> perf_counter()</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> GBBiClf()</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    clf.fit(df)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GB avg fit time: </span><span class="sc">{</span>(perf_counter() <span class="op">-</span> start_time) <span class="op">/</span> m<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> perf_counter()</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>    clf.predict(sim_data[i])</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GB avg predict time: </span><span class="sc">{</span>(perf_counter() <span class="op">-</span> start_time) <span class="op">/</span> m<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> perf_counter()</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> GNBBiClf()</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>    clf.fit(df)</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GNB avg fit time: </span><span class="sc">{</span>(perf_counter() <span class="op">-</span> start_time) <span class="op">/</span> m<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> perf_counter()</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>    clf.predict(sim_data[i])</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GNB avg predict time: </span><span class="sc">{</span>(perf_counter() <span class="op">-</span> start_time) <span class="op">/</span> m<span class="sc">:.6f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>GB avg fit time: 0.004047
GB avg predict time: 0.000654
GNB avg fit time: 0.005097
GNB avg predict time: 0.000047</code></pre>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>