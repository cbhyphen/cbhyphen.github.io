[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/bayes-classifier/bayes_classifier.html",
    "href": "posts/bayes-classifier/bayes_classifier.html",
    "title": "maximum a posteriori",
    "section": "",
    "text": "Even though I’ve studied (and revisited, and revisited..) Bayesian statistics several times over the years, I always felt that, over time, my understanding would lose it’s sharpness. In my opinion, the Bayesian paradigm isn’t very intuitive. So I created this post as future reference to myself, but also as a way to dive deeper into things like the naive assumption, maximum a posterior vs maximum likelihood, and decision boundaries.\nLet’s assume there is a random variable \\(X\\) that follows a Gaussian distribution\n\\[\nX \\sim N(\\mu, \\sigma)\n\\]\nand a variable \\(Y\\) which is discrete\n\\[\nY\\in\\{0, 1\\}\n\\]\nSuppose we know that the value of \\(Y\\) is dependent upon \\(X\\), but that the relationship is not deterministic. We can model this relationship using conditional probability\n\\[\nP(Y=y|X=x)\n\\]\nBut say we want to assign \\(Y\\) a definitive value (i.e., classify). In that case we can simply select the value of \\(Y\\) with the highest probability\n\\[\n\\arg\\max_ y P(Y|X)\n\\]\nAnd because we are selecting a value for \\(Y\\) when there is uncertainty, this means we are making an estimate. The above is known as the maximum a posteriori (MAP) estimate of \\(Y\\) given \\(X\\), and \\(P(Y|X)\\) is commonly referred to as the posterior.\nMost likely we won’t have knowledge of the posterior (\\(Y\\) is unknown afterall), so we use Bayes theorem to derive an equivalence\n\\[\nP(Y|X) = {P(Y \\cap X) \\over P(X)} = {P(X|Y) \\cdot P(Y) \\over P(X)}\n\\]\nwhere\nWhen performing the MAP estimate, we are given some value of \\(X\\) and then calculate the posterior probability for each possible value of \\(Y\\). This means that the marginal is the same for all values of \\(Y\\) and is just a constant that can be factored out\n\\[\nP(Y|X) \\propto {P(X|Y) \\cdot P(Y)}\n\\]\nwhich simplifies the MAP classifier to\n\\[\n\\arg\\max_y {P(X|Y)  \\cdot P(Y)}\n\\]\nAs far as the likelihood function, we made an assumption on the distribution of \\(X\\) so we can use the Gaussian probability density function\n\\[\np(x|y) = \\frac{1}{\\sigma_y\\sqrt2\\pi} e ^ {- \\frac{1}{2} ( \\frac{x - \\mu_y}{\\sigma_y} ) ^2}\n\\]\nIf we don’t know the Gaussian parameters above, we just estimate them using the empirical mean and variance of the training data for each class which is a maximum likelihood estimate.\n\\[\n\\mu_y = \\frac{1}{n}\\sum_{i}^{n}x_i\n\\]\n\\[\n\\sigma_y^2 = \\frac{1}{n}\\sum_{i}^{n}(x_i - \\mu_y)^2\n\\]\nWe don’t know the distribution of the prior, so we have to estimate it. In practice, we simply use the prevalence of each class in the training data which is again a maximum likelihood estimate.\n\\[\np(y) = \\frac{1}{n}\\sum_{i}^{n} \\mathbb{1}(y_i = y)\n\\]\nIt’s worth noting that there is also a maximum likelihood estimate (MLE) that could be used for the classifier. As the name suggest we would just use the likelihood term and remove the prior\n\\[\n\\arg\\max_y {P(X|Y)}\n\\]\nbut this ignores the prior distribution if we have that information.\nWith some basic theory out of the way, let’s build a classifer."
  },
  {
    "objectID": "posts/bayes-classifier/bayes_classifier.html#univariate-classifier",
    "href": "posts/bayes-classifier/bayes_classifier.html#univariate-classifier",
    "title": "maximum a posteriori",
    "section": "univariate classifier",
    "text": "univariate classifier\nLet’s simulate univariate Gaussian data for the two classes. For simplicity, the data will have different means but the same variance.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\nsns.set()\n%matplotlib inline\n\n\nn = 1000\nmu_1 = 40\nmu_2 = 80\nstd = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std, size=n)\nx_2 = np.random.normal(loc=mu_2, scale=std, size=n)\n\ndf = pd.DataFrame({'x': np.concatenate([x_1, x_2]), 'y': [1] * n + [2] * n})\n\nsns.displot(df, kind='kde', x='x', hue='y',\n            fill=True, linewidth=0, palette='dark', alpha=0.5)\n\n\n\n\nTime to estimate priors, means, and standard deviations. This is trivial since we generated the data but let’s pretend that we didn’t :)\n\npriors = {k: df[df.y == k].size / df.size for k in df.y.unique()}\npriors\n\n{1: 0.5, 2: 0.5}\n\n\n\nmeans = {k: df[df.y == k].x.mean() for k in df.y.unique()}\nmeans\n\n{1: 39.74917237733038, 2: 80.16187136171098}\n\n\n\nstdevs = {k: df[df.y == k].x.std() for k in df.y.unique()}\n# .std(ddof=0) if not sample\nstdevs\n\n{1: 10.004638113965834, 2: 10.265729149219876}\n\n\nNow that the data is fit, we can build a classifier and predict new instances.\n\n# scipy actually has gaussian pdf: from scipy.stats import norm\n\ndef uni_gaussian_pdf(x, mean, stdev):\n    scalar = 1.0 / (stdev * np.sqrt(2 * np.pi))\n    exponential = np.exp(-0.5 * ((x - mean) / stdev) ** 2)\n    return scalar * exponential\n\nclasses = df.y.unique().tolist()  \n\ndef gbayes_uni_classify(x):\n    probas = []\n    for c in classes:\n        likelihood = uni_gaussian_pdf(x, means[c], stdevs[c])\n        # likelihood = norm.pdf(x, means[c], stdevs[c])\n        probas.append(likelihood * priors[c])\n    return classes[np.argmax(probas)]\n\nIt’s important to mention here that the priors are the same since we generated equal amounts of data for both classes. Mathematically this means that the prior is a constant and can be factored out in the original MAP equation (for this case) giving\n\\[\n\\arg\\max_y {P(X|Y)}\n\\]\nSo in the case where priors are the same, the MAP is equivalent to the MLE.\nAnd now to visualize the decision boundary.\n\nsim_data = np.arange(0, 150, 1)  # uniform sequence\nsim_class_preds = [gbayes_uni_classify(x) for x in sim_data]\n\ndecision_boundary = np.where(np.array(sim_class_preds[:-1]) - np.array(sim_class_preds[1:]) != 0)[0]\nprint(decision_boundary)\n\nsns.displot(df, kind='kde', x='x', hue='y', fill=True, linewidth=0, palette='dark', alpha=0.5)\nfor v in sim_data[decision_boundary]:\n    plt.axvline(v, color='black', linestyle='--')\n\n[59]\n\n\n\n\n\nThe decision boundary is roughly halfway between means, as expected. This isn’t super interesting but what if the variances are different?\n\nn = 1000\nmu_1 = 40\nmu_2 = 80\nstd_1 = 20\nstd_2 = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std_1, size=n)\nx_2 = np.random.normal(loc=mu_2, scale=std_2, size=n)\n\ndf = pd.DataFrame({'x': np.concatenate([x_1, x_2]), 'y': [1] * n + [2] * n})\n\nsns.displot(df, kind='kde', x='x', hue='y', fill=True, linewidth=0, palette='dark', alpha=0.5)\n\n\n\n\n\n# class so we don't repeat same spiel\n\nclass GBUniClf:\n    \n    def __init__(self):\n        self.classes = None\n        self.priors = None\n        self.means = None\n        self.stdevs = None\n        \n    def fit(self, df):\n        self.classes = df.y.unique().tolist()\n        self.priors = {k: df[df.y == k].size / df.size for k in self.classes}\n        self.means = {k: df[df.y == k].x.mean() for k in self.classes}\n        self.stdevs = {k: df[df.y == k].x.std() for k in self.classes}\n        \n    def likelihood(self, x, mean, stdev):\n        scalar = 1.0 / (stdev * np.sqrt(2 * np.pi))\n        exponential = np.exp(-0.5 * ((x - mean) / stdev) ** 2)\n        return scalar * exponential\n    \n    def predict(self, x):\n        probas = []\n        for c in self.classes:\n            likelihood = self.likelihood(x, self.means[c], self.stdevs[c])\n            probas.append(likelihood * self.priors[c])\n        return self.classes[np.argmax(probas)]\n\n\nclf = GBUniClf()\nclf.fit(df)\n\nsim_data = np.arange(-50, 200, 1)\nsim_class_preds = [clf.predict(x) for x in sim_data]\n\ndecision_boundary = np.where(np.array(sim_class_preds[:-1]) - np.array(sim_class_preds[1:]) != 0)[0]\nprint(decision_boundary)\n\ndf_preds = pd.DataFrame({'x': sim_data, 'y': sim_class_preds})\n\nsns.displot(df, kind='kde', x='x', hue='y', fill=True, linewidth=0, palette='dark', alpha=0.3)\nrug_plot = sns.rugplot(df_preds, x=\"x\", hue=\"y\", palette='dark', alpha=0.7)\nrug_plot.get_legend().remove()\nfor v in sim_data[decision_boundary]:\n    plt.axvline(v, color='black', linestyle='--')\n\n[113 174]\n\n\n\n\n\nBecause class 1 has a larger variance, there is now a second decision boundary. Instances with high values of \\(x\\) (far right) are less likely to belong to class 2 even though they are closer to its’ mean. Instead they get classified as 1.\nWhat if the priors are different?\n\nn_1 = 2000\nn_2 = 500\nmu_1 = 40\nmu_2 = 80\nstd_1 = 20\nstd_2 = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std_1, size=n_1)\nx_2 = np.random.normal(loc=mu_2, scale=std_2, size=n_2)\n\ndf = pd.DataFrame({'x': np.concatenate([x_1, x_2]), 'y': [1] * n_1 + [2] * n_2})\n\nclf = GBUniClf()\nclf.fit(df)\n\nsim_data = np.arange(-50, 200, 1)\nsim_class_preds = [clf.predict(x) for x in sim_data]\n\ndecision_boundary = np.where(np.array(sim_class_preds[:-1]) - np.array(sim_class_preds[1:]) != 0)[0]\nprint(decision_boundary)\n\ndf_preds = pd.DataFrame({'x': sim_data, 'y': sim_class_preds})\n\nsns.displot(df, kind='kde', x='x', hue='y', fill=True, linewidth=0, palette='dark', alpha=0.3)\nrug_plot = sns.rugplot(df_preds, x=\"x\", hue=\"y\", palette='dark', alpha=0.7)\nrug_plot.get_legend().remove()\nfor v in sim_data[decision_boundary]:\n    plt.axvline(v, color='black', linestyle='--')\n\n[120 164]\n\n\n\n\n\nIt simply makes the more prevalent class more likely as expected.\n\nmultivariate\nNow we can look at bivariate data where covariance between features come into play. The naive assumption ignores covariance so we can compare classifiers that do and do not make that assumption.\nMathematically, the posterior is now conditioned on multiple features\n\\[\nP(Y|X) = P(Y|x_1, x_2, \\ldots , x_i)\n\\]\nand the MAP classifier in the multivariate case is\n\\[\n\\arg\\max_y {P(Y) \\cdot P(x_1, x_2, \\ldots , x_i|Y)}\n\\]\nTherefore we use the multivariate likelihood function which makes use of covariance\n\\[\np(x|y) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma_y|}} e^{ - \\frac{1}{2} (x - \\mu_y)^T \\Sigma_y^{-1} (x - \\mu_y)}\n\\]\nThis is a drop-in replacement though, and the rest of the classifier is the same.\n\nn = 1000\nmu_1 = 40\nmu_2 = 80\nstd = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std, size=(n, 2))\nx_2 = np.random.normal(loc=mu_2, scale=std, size=(n, 2))\ndata = np.concatenate([x_1, x_2])\n\ndf = pd.DataFrame({'x1': data[:, 0], 'x2': data[:, 1], 'y': [1] * n + [2] * n})\n\n# s = sns.scatterplot(df, x='x1', y='x2', hue='y', hue_order=classes, palette='dark', alpha=0.25)\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\n\nclass GBBiClf:\n    \n    def __init__(self):\n        self.classes = None\n        self.priors = None\n        self.means = None\n        self.covars = None\n        self.covar_dets = None\n        self.covar_invs = None\n\n    def fit(self, df):\n        self.classes = df.y.unique().tolist()\n        self.priors = {k: df[df.y == k].shape[0] / df.shape[0] for k in self.classes}\n        self.means = {k: df[['x1', 'x2']][df.y == k].mean(axis=0) for k in self.classes}\n        self.covars = {k: np.cov(df[['x1', 'x2']][df.y == k], rowvar=False, bias=True) for k in self.classes}\n        self.covar_dets = {k: np.linalg.det(self.covars[k]) for k in self.classes}\n        self.covar_invs = {k: np.linalg.inv(self.covars[k]) for k in self.classes}\n\n    def likelihood(self, x, c):\n        dims = 2\n        scalar = 1.0 / np.sqrt(((2 * np.pi) ** dims) * self.covar_dets[c])\n        exponential = np.exp(-0.5 * (x - self.means[c]).T @ self.covar_invs[c] @ (x - self.means[c]))\n        return scalar * exponential\n\n    def predict(self, x):\n        probas = []\n        for c in self.classes:\n            likelihood = self.likelihood(x, c)\n            probas.append(likelihood * self.priors[c])\n        return self.classes[np.argmax(probas)]\n\n\nclf = GBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(0, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\n# sns.scatterplot(plot_df, x='x1', y=\"x2\", hue=\"y\", hue_order=classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\nThe variance was same for both distributions and the features were sampled independently, so the decision boundary isn’t complex. Slight curvature is due to the estimate of covariance which is different from the true value.\n\nclf.covars\n\n{1: array([[97.13589025,  3.99602431],\n        [ 3.99602431, 99.21513485]]),\n 2: array([[104.87159824,  -2.09309889],\n        [ -2.09309889, 102.99262222]])}\n\n\nEven though this data is uninteresting, let’s compare the decision boundary of a naive classifier. The naive assumption is that all features are independent, so we can use the chain rule of probability for a simpler calculation of the likelihood.\n\\[\nP(X|Y) = P(x_1, x_2, \\ldots , x_i|Y) = \\prod\\limits_{i}P(x_i|Y)\n\\]\nThe MAP classifier under the naive assumption then becomes\n\\[\n\\arg\\max_y {P(Y) \\cdot P(x_1|Y) \\cdot P(x_2|Y) \\cdot \\ldots \\cdot P(x_m|Y)}\n\\]\nFor this case though, since the features were generated independently, the decision boundary should be roughly the same.\n\nclass GNBBiClf:\n    \n    def __init__(self):\n        self.classes = None\n        self.priors = None\n        self.means = None\n\n    def fit(self, df):\n        self.classes = df.y.unique().tolist()\n        self.priors = {k: df[df.y == k].shape[0] / df.shape[0] for k in self.classes}\n        self.means = {k: df[['x1', 'x2']][df.y == k].mean(axis=0) for k in self.classes}\n        self.stdevs = {k: np.std(df[['x1', 'x2']][df.y == k], axis=0) for k in self.classes}\n\n    def likelihood(self, x, mean, stdev):\n        scalar = 1.0 / (stdev * np.sqrt(2 * np.pi))\n        exponential = np.exp(-0.5 * ((x - mean) / stdev) ** 2)\n        return scalar * exponential\n    \n    def predict(self, x):\n        probas = []\n        for c in self.classes:\n            joint_likelihood = 1\n            for i, v in enumerate(x):\n                likelihood = self.likelihood(v, self.means[c][i], self.stdevs[c][i])\n                joint_likelihood *= likelihood\n            probas.append(joint_likelihood * self.priors[c])\n        return self.classes[np.argmax(probas)]\n\n\nclf = GNBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(0, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\nWhat if just the covariance are different? Let’s draw random data were the features are still independent (i.e. covariance matrix is symmetic) but the variance of features is different for each class.\n\nn = 1000\nmu_1 = 40\nmu_2 = 100\nstd_1 = 20\nstd_2 = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std_1, size=(n, 2))\nx_2 = np.random.normal(loc=mu_2, scale=std_2, size=(n, 2))\ndata = np.concatenate([x_1, x_2])\n\ndf = pd.DataFrame({'x1': data[:, 0], 'x2': data[:, 1], 'y': [1] * n + [2] * n})\n\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\n\nclf = GBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(-10, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\nAs expected, the decision boundary favors class 1 since it had larger variance. Without any correlation between features, I would again expect the decision boundary to be the same for the naive classifier.\n\nclf = GNBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(-10, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\n# sanity check\n# from sklearn.naive_bayes import GaussianNB\n# clf = GaussianNB()\n# clf.fit(df[['x1', 'x2']].values, df[['y']].values.ravel())\n# sim_data_range = np.arange(-10, 140, 1)\n# sim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\n# sim_classes = [clf.predict(x.reshape(1, -1)) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\nThe decision boundary for the naive classifier is roughly identical. Zooming out, we can see the classifier has similar behavior as the univariate case for different variance.\n\nclf = GNBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(-10, 200, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 200), ylim=(-10, 200))\n\n[(-10.0, 200.0), (-10.0, 200.0)]\n\n\n\n\n\nLet’s finally simulate data with correlation between features. There should be a noticeable difference in the decision boundary for the naive classifier.\n\nn = 1000\nmu_1 = 40\nmu_2 = 100\n\nx_1 = np.random.multivariate_normal(mean=[mu_1, mu_1], cov=[[50, 70], [70, 200]], size=n)\nx_2 = np.random.multivariate_normal(mean=[mu_2, mu_2], cov=[[100, 1], [1, 100]], size=n)  # no correlation\ndata = np.concatenate([x_1, x_2])\n\ndf = pd.DataFrame({'x1': data[:, 0], 'x2': data[:, 1], 'y': [1] * n + [2] * n})\n\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\n\nclf = GBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(-10, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\n\nclf = GNBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(-10, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\nThe difference between the naive and non-naive classifier is more noticeable when there is correlation between features (class 1). The naive classifier clearly ignores the covariance and the decision boundary is much smoother.\nOne obvious advantage of the naive assumption is computational efficiency. Predictions for the naive classifier ran in faster time compared to the non-naive by an order of magnitude. Fit times were roughly the same.\n\nfrom time import perf_counter\n\n# TODO memory reqs\n\nm = 100\n\nstart_time = perf_counter()\nfor i in range(m):\n    clf = GBBiClf()\n    clf.fit(df)\nprint(f\"GB avg fit time: {(perf_counter() - start_time) / m:.6f}\")\nstart_time = perf_counter()\nfor i in range(m):\n    clf.predict(sim_data[i])\nprint(f\"GB avg predict time: {(perf_counter() - start_time) / m:.6f}\")\n\nstart_time = perf_counter()\nfor i in range(m):\n    clf = GNBBiClf()\n    clf.fit(df)\nprint(f\"GNB avg fit time: {(perf_counter() - start_time) / m:.6f}\")\nstart_time = perf_counter()\nfor i in range(m):\n    clf.predict(sim_data[i])\nprint(f\"GNB avg predict time: {(perf_counter() - start_time) / m:.6f}\")\n\nGB avg fit time: 0.004047\nGB avg predict time: 0.000654\nGNB avg fit time: 0.005097\nGNB avg predict time: 0.000047"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "brain dumpsite",
    "section": "",
    "text": "maximum a posteriori\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]