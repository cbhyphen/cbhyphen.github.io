[
  {
    "objectID": "posts/sample-size/sample_size.html",
    "href": "posts/sample-size/sample_size.html",
    "title": "Sample Size for A/B Hypothesis Testing",
    "section": "",
    "text": "The other day I had a conversation at work about model training and evaluation metrics. A colleague of mine suggested that the small improvements we were seeing might not actually be statistically significant. I speculated that we could use hypothesis testing (and sampling) to determine a threshold at which an improvement was or wasn’t significant. After some research, for various reasons, we concluded that this was not a good approach (see link) but the topic reminded me of an A/B testing exercise I did a few years back. I hadn’t fully documented that work, so in this post I intend to do that. Here I’ll go through a derivation for sample size as it relates to A/B testing and ultimately create a sample size calculator as a shiny app (in R). Before I dive into the sample size derivation, I’ll briefly go over what A/B testing is and describe it’s components.\nPut simply, an A/B Test is a form of statistical hypothesis testing. We start with a hypothesis, data is collected, then a resulting test statistic determines the validity of the hypothesis. A hypothesis could something like “a change to my landing page will increase the number of sign-ups” and the test statistic would be sign-up rate. The test would involve making a change to the website while collecting number of sign-ups, and the new sign-up rate would indicate if your hypothesis should be rejected or accepted. Note that in this post I’ll be referring to the test statistic as a conversion rate for simplicity. And there many other scenarios (not just website design and sign-up rate) that fit within the A/B testing framework, this is just one example.\nBefore performing an A/B test, there are a few parameters which are either already known or defined:\n\ncurrent conversion rate\ntype I error\ntype II error\nminimum detectable change\n\nCurrent conversion rate is the value of the test statistic without any changes (i.e. control group) and should be known apriori. Type I and II errors are tolerance parameters you will define for the test. Type I error is the false positive rate (i.e. accept there was a change due to the treatment when there wasn’t) and type II error is the false negative rate (i.e. reject there was a change due to the treatment when there was). As you might expect, the higher the tolerance for error the fewer samples you will need. Lastly, the somewhat self explanatory minimum detectable change is the smallest change you’d like to be able to detect from the test. Conversely, the smaller the change, the more samples you’ll need. Thus, the unknown parameters are the sample size as well as the critical value at which to accept or reject the hypothesis.\n\nOk, so defining things more formally, the number of conversions would be a binomial random variable\n\\[ X \\sim B(n, p) \\]\nwhere \\(n\\) is number of trials (or visits in the hypothetical example above) and \\(p\\) is the probability of a success (or sign-up). By definition, the expected value and variance for the binomial random variable are\n\\[ E[X] = np, \\ Var(X) = np(1 - p) \\]\nSince we are interested in conersion rate, we can define a new variable\n\\[ Y = \\frac{X}{n} \\]\nand by properties of expectation and variance the new parameters are\n\\[ E[Y] = p, \\ Var(Y) = \\frac{p(1 - p)}{n} \\]\nNow, let’s define the current conversion rate as \\(Y_c\\) and the treatment conversion rate as \\(Y_e\\). To make the hypothesis convenient, our test statistic will be the difference between the two \\(Y_d = Y_e - Y_c\\), and the null hypothesis is that the expected value of this difference is zero\n\\[ H_0: E[Y_d] = 0 \\]\nand the alternative hypothesis that the difference is equal to or greater than some value \\(d\\) (i.e. the treatment had a positive effect):\n\\[ H_a: E[Y_d] \\ge d \\]\nNote that this implies a one-sided test, but there can also be a two-sided test. A one-sided test evaluates for a change in one direction whereas two-sided test evalutes change in both directions (i.e. the change could also be negative). This is important because it affects the critical value and thus sample size, which I’ll elaborate on later.\nVisual representations are always helpful, so below is the distribution of the test statistic \\(Y_d\\) under the null hypothesis when type I error \\(\\alpha = 0.05\\). An important note here is that I’m using a Gaussian approximation to the Binomial which is valid when the number of samples is large enough.\n\nlibrary(ggplot2)\n\nmu_null &lt;- 0\nn &lt;- 100000\nalpha &lt;- 0.05\n\nz_alpha &lt;- qnorm(1 - alpha)\n\ndensity_null &lt;- density(rnorm(n, mu_null, 1))\ndf_null &lt;- data.frame(x = density_null$x, y = density_null$y)\n\nggplot() +\n  geom_ribbon(data = df_null[df_null$x &lt; z_alpha, ],\n              aes(x = x, ymax = y, fill = \"no change (true negative)\"),\n              ymin = 0, alpha = 0.3) +\n  geom_ribbon(data = df_null[df_null$x &gt;= z_alpha, ],\n              aes(x = x, ymax = y, fill = \"type I error (false positive)\"),\n              ymin = 0, alpha = 0.3) +\n  scale_fill_manual(values = c(\"black\", \"red\")) +\n  geom_segment(aes(x = mu_null, y = 0, xend = z_alpha, yend = 0)) +\n  geom_text(aes(0.75, 0, label = \"d_alpha\", vjust = -1)) +\n  geom_vline(xintercept = 0, linetype=\"dashed\", linewidth = 0.3) +\n  geom_vline(xintercept = z_alpha, linetype=\"dashed\", linewidth = 0.3) +\n  geom_text(aes(x = 1.9, label = \"\\nx_d\", y = 0, vjust = 0.75), angle=0) +\n  scale_x_continuous(limits = c(-3, 3)) +\n  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(),\n        axis.text.y=element_blank(), axis.ticks.y=element_blank(), axis.title.y=element_blank(),\n        legend.title=element_blank()) +\n   xlab(mu_null)\n\n\n\n\nThe gray shaded region is the probability density of a true negative and the red region corresponds to a false positive. Here the variable \\(d_\\alpha\\) is the distance from the expected value under the null hypothesis to the critical value \\(x_d\\). This distance depends upon the allowable type I error \\(\\alpha\\).\nNow let’s take a look at the alternative hypothesis for \\(d = 0.4\\) and a type II error \\(\\beta = 0.20\\).\n\nmu_alt &lt;- 0.4\nn &lt;- 100000\nbeta &lt;- 0.20\n\nz_beta &lt;- qnorm(1 - beta)\n\ndensity_alt &lt;- density(rnorm(n, mu_alt, 1))\ndf_alt &lt;- data.frame(x = density_alt$x, y = density_alt$y)\n\nggplot() +\n  geom_ribbon(data = df_alt[df_alt$x &gt; -z_beta, ],\n              aes(x = x, ymax = y, fill = \"change (true positive)\"),\n              ymin = 0, alpha = 0.3) +\n  geom_ribbon(data = df_alt[df_alt$x &lt;= -z_beta, ],\n              aes(x = x, ymax = y, fill = \"type II error (false negative)\"),\n              ymin = 0, alpha = 0.3) +\n  scale_fill_manual(values = c( \"green\", \"blue\")) +\n  geom_segment(aes(x = mu_alt, y = 0, xend = -z_beta, yend = 0)) +\n  geom_text(aes(-0.25, 0, label = \"d_beta\", vjust = -1)) +\n  geom_vline(xintercept = mu_alt, linetype=\"dashed\", linewidth = 0.3) +\n  geom_vline(xintercept = -z_beta, linetype=\"dashed\", linewidth = 0.3) +\n  geom_text(aes(x = -1.1, label = \"\\nx_d\", y = 0, vjust = 0.75), angle=0) +\n  scale_x_continuous(limits = c(-3 + mu_alt, 3 + mu_alt)) +\n  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(),\n        axis.text.y=element_blank(), axis.ticks.y=element_blank(), axis.title.y=element_blank(),\n        legend.title=element_blank()) +\n  xlab(mu_alt)\n\n\n\n\nThe green shaded region is the probability density of a true positive and the blue region corresponds to a false negative. Akin to the previous plot, the variable \\(d_\\beta\\) defines the distance from the expected value under the alternative hypothesis to the critical value \\(x_d\\). This distance depends upon the allowable type II error \\(\\beta\\).\nIf you look closely, you can see that the critival value allows us to relate both distances to the minimum detectable change \\(d\\).\n\\[d = d_\\alpha + d_\\beta\\]\nAgain, \\(d\\) is the difference between the expected value under the null hypothesis and the expected value under the alternative hypothesis. And it’s not immediately clear how defining these distances will help us determine sample size and critival value but we’ll see below.\n\nIn order to quantify \\(d_\\alpha\\) and \\(d_\\beta\\), we need the expected value and variance of the test statistic \\(Y_d\\) under the null and alternative hypothesis. The expected value is pretty straighforward\n\\[ E[Y_d] = E[Y_e] - E[Y_c] = p_e - p_c \\]\nwhereas the variance is (assuming equal number of samples)\n\\[ Var(Y_d) = Var(Y_e) + Var(Y_c) = \\frac{p_e(1 - p_e)}{n} + \\frac{p_c(1 - p_c)}{n} \\]\nSo, to elaborate on our hypothesis, under the null where \\(p_e = p_c\\) we have\n\\[ H_0: E[Y_d] = 0, \\ Var(Y_d) = \\frac{2p_c(1 - p_c)}{n} \\]\nand under the alternative there is\n\\[ H_a: E[Y_d] \\le d, \\ Var(Y_d) = \\frac{p_c(1 - p_c) + p_e(1 - p_e)}{n} \\]\nUsing the Gaussian approximation as noted above, we can define the following distances\n\\[ d_\\alpha = Z_{1-\\alpha} \\sqrt{\\frac{2p_c(1-p_c)}{n}} \\]\n\\[ d_\\beta = Z_{1-\\beta}\\sqrt{\\frac{p_c(1 - p_c) + p_e(1 - p_e)}{n}} \\]\nand some rearranging to isolate \\(n\\) our variable of interest\n\\[ d_\\alpha = \\frac{1}{\\sqrt{n}} Z_{1-\\alpha} \\sqrt{2p_c(1-p_c)} \\]\n\\[ d_\\beta = \\frac{1}{\\sqrt{n}} Z_{1-\\beta} \\sqrt{p_c(1-p_c) + p_e(1-p_e)} \\]\nNow we can define the minimum detectable effect as\n\\[ d = \\frac{1}{\\sqrt{n}} \\biggl( Z_{1-\\alpha} \\sqrt{2p_c(1-p_c)} + Z_{1-\\beta} \\sqrt{p_c(1-p_c) + p_e(1-p_e)} \\biggr) \\]\nand solving for \\(n\\), the sample size of the test is\n\\[ n = \\biggl( \\frac{Z_{1-\\alpha} \\sqrt{2p_c(1-p_c)} + Z_{1-\\beta} \\sqrt{p_c(1-p_c) + p_e(1-p_e)}}{d} \\biggr) ^2 \\]\nwhere the critical value to acceptor reject the null hypothesis is\n\\[ x_d = p_c + d_\\alpha = p_c + Z_{1-\\alpha} \\sqrt{\\frac{2p_c(1-p_c)}{n}} \\]\nIt’s important to re-iterate that this sample size calculation is for a one-sided test and a scenario where the change in rate is expected to be positive. If we expected a negative change in rate, the minimum detectable effect \\(d\\) would be negative and the equality in the alternative hypothesis flipped.\nFor a two-sided test, Z-score corresponding to the \\(\\alpha\\) value would need to divided in half in order to account for false positives in both directions. Another important caveat under the two-sided alternative hypothesis is that variance could take on two different values. For instance, the alternative conversion rate \\(p_e\\) could be equal to either \\(p_c + d\\) or \\(p_c - d\\) each of which implies a different variance and thus sample size. To make this clear, here is a plot of variance of as a function \\(p_e\\) (recall this is \\(E[Y_e]\\)).\n\n# plot of variance for alternative hypothesis as a function E(Y_e)\n\nmu_e = seq(0.01, 0.99, by=0.01)  # i.e. p_e\n\nvar_e_fn &lt;- function(p_e) {\n  return(p_e * (1 - p_e) / n)\n}\n\nvar_e = sapply(mu_e, var_e_fn)\n\nggplot(data = data.frame(x = mu_e, y = var_e)) +\n  geom_point(aes(x = x, y = y), alpha = 0.5) +\n  xlab(\"E(Y_e)\") + ylab(\"Var(Y_e)\")\n\n\n\n\nSo, if we are performing a two-sided test, in order to be conservative, we should choose the value of \\(p_e\\) that maximizes the variance and sample size. As you can see above, this amounts to choosing the value of \\(p_e\\) which is closest to \\(\\frac{1}{2}\\).\nOk so, that wraps up the theory part of this post. In addition to the explanation here, I also created an app. The following is a link to a sample size calculator I created in R using shiny. In the app you can select the current conversion rate along with test parameters such as minimum detectable change, test direction, and significance/power level. The output will show sample size, the change in conversion rate at which to reject the null hypothesis, and a plot of the probability densities. Code used to create the app can be found here."
  },
  {
    "objectID": "posts/pytorch-eager-qat/pytorch_eager_qat.html",
    "href": "posts/pytorch-eager-qat/pytorch_eager_qat.html",
    "title": "Eager Mode Quantization in PyTorch: Quantizing the Backbone of an Object Detection Model",
    "section": "",
    "text": "Model quantization is a powerful tool to reduce memory and compute by using lower precision data types. Although I’m familiar with the concept and a few high-level APIs, I hadn’t yet implemented it in PyTorch and wanted a deep dive for something non-trivial. After reading about PyTorch’s different quantization modes (eager vs graph fx) and methods (dynamic, post-training static, static aware-training), I decide to start with eager mode quantization aware training (QAT). At the time of writing this, eager mode quantization was a more mature feature and this seemed like a natural place to start.\nIn my research for a project, I came across multiple discussions online requesting either help or a tutorial for quantizing the backbone of an object detection model (faster R-CNN in this case). As far as I could tell there was nothing available so this was the perfect excuse.\nSo in this post, I will go through that process of QAT while quantizing the backbone of faster R-CNN and subsequently include analysis on the benefits.\n\nResNet and Feature Pyramid Network\nThis assumes some familiarity with the R-CNN architecture, but to refresh, the feature extraction backbone consists of two components; the resnet and the feature pyramid network. The FPN combines output from consecutive layers of the resnet (via upsampling) which allows it to extract semantic information at higher resolutions. These two components of the backbone can be quantized while the rest of the network still uses floating point precision.\nFrom an implementation standpoint, there is a utility class IntermediateLayerGetter for extracting each layer output (no fully connected) from the resnet. And another convenience class for the FPN which takes the layer ouputs as input. Combining these two is BackboneWithFPN which is mostly just a thin wrapper around both.\nAs we are doing eager mode static quantization, we’ll need to prepare the model before we can train and subsequently quantize it.\n\n\nModel Preparation\nThe first step in preparing the network for quantization is to create a modified bottleneck block. This isn’t obvious until you try to quantize the ResNet without it. You will get an error .. out += identity .. Could not run 'aten::add.out' .. which means that PyTorch isn’t able to quantize the skip connection using the += operator in eager mode. This discussion on the pytorch forums was helpful for describing the error as well as how to fix it. The modified bottleneck block just uses FloatFunctional which has a quantized addition operator. I’m using ResNet 101 here but for much smaller networks you would want to modify the basic block. Also, the original bottleneck class reuses the ReLU layer which won’t work when fusing. Finding this blog post about quantizing ResNet was helpful for realizing and avoiding that pitfall.\n\nfrom typing import Optional, Callable\n\nimport torch.nn as nn\nfrom torch import Tensor\nfrom torch.ao.nn.quantized import FloatFunctional\n\nfrom torchvision.models.resnet import conv1x1, conv3x3\n\n\nclass BottleneckQuantizeable(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.0)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.ff_add = FloatFunctional()\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu2(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = self.ff_add.add(out, identity)\n        out = self.relu3(out)\n\n        return out\n\nNow that we have a quantizeable bottleneck, we can simply reference it when generating the ResNet. Even though the float functional operator was added, we can still load pretrained imagenet weights since the trainable submodules didn’t change. Note that the number of classes for the ResNet don’t matter here because we will extract intermediate layers and ignore the final fully connected layer.\n\nfrom torchvision.models.resnet import ResNet, ResNet50_Weights, ResNet101_Weights\n\n\ndef resnet_101():\n    resnet = ResNet(block=BottleneckQuantizeable, layers=[3, 4, 23, 3])\n    resnet.load_state_dict(ResNet101_Weights.DEFAULT.get_state_dict(progress=True))\n    return resnet\n\n\nresnet = resnet_101()\n\nThe next step is to pass the resnet to the IntermediateLayerGetter. In addition to the resnet we created, this class also requires a dictionary of the layer names (to know what to extract). It returns an OrderedDict of those layer outputs. Here’s an example using a toy image.\n\nimport torch\nfrom torchvision.models._utils import IntermediateLayerGetter\n\n\nreturned_layers = [1, 2, 3, 4]  # get all 4 layers\nreturn_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}  # {'layer1': 0, 'layer2': 1, ...}\n\nresnet_layers = IntermediateLayerGetter(resnet, return_layers=return_layers)\n\nout = resnet_layers(torch.rand(1, 3, 200, 200))  # e.g. 200 x 200 image with 3 channels\n[(k, v.shape) for k, v in out.items()]\n\n[('0', torch.Size([1, 256, 50, 50])),\n ('1', torch.Size([1, 512, 25, 25])),\n ('2', torch.Size([1, 1024, 13, 13])),\n ('3', torch.Size([1, 2048, 7, 7]))]\n\n\nAgain, the output of the resnet layers will be fed to the feature pyramid network. Before we can do that, the FPN also needs to be modified as it uses a + addition operator. Note that there is also a functional F.interpolate but that doesn’t actually need to be replaced. However, it does need to be referenced differently as importing torch.nn.functional as F causes a namespace issue later with torchvision.\n\nfrom collections import OrderedDict\nfrom typing import List, Dict\n\n# importing as 'F' causes namespace collision with torchvision and QAT fails later\n# import torch.nn.functional as F\nimport torch\n\nfrom torchvision.ops.misc import Conv2dNormActivation\nfrom torchvision.utils import _log_api_usage_once\nfrom torchvision.ops.feature_pyramid_network import ExtraFPNBlock\n\n\nclass FeaturePyramidNetworkQuantizeable(nn.Module):\n    \"\"\"\n    Module that adds a FPN from on top of a set of feature maps. This is based on\n    `\"Feature Pyramid Network for Object Detection\" &lt;https://arxiv.org/abs/1612.03144&gt;`_.\n\n    The feature maps are currently supposed to be in increasing depth\n    order.\n\n    The input to the model is expected to be an OrderedDict[Tensor], containing\n    the feature maps on top of which the FPN will be added.\n\n    Args:\n        in_channels_list (list[int]): number of channels for each feature map that\n            is passed to the module\n        out_channels (int): number of channels of the FPN representation\n        extra_blocks (ExtraFPNBlock or None): if provided, extra operations will\n            be performed. It is expected to take the fpn features, the original\n            features and the names of the original features as input, and returns\n            a new list of feature maps and their corresponding names\n        norm_layer (callable, optional): Module specifying the normalization layer to use. Default: None\n\n    Examples::\n\n        &gt;&gt;&gt; m = torchvision.ops.FeaturePyramidNetwork([10, 20, 30], 5)\n        &gt;&gt;&gt; # get some dummy data\n        &gt;&gt;&gt; x = OrderedDict()\n        &gt;&gt;&gt; x['feat0'] = torch.rand(1, 10, 64, 64)\n        &gt;&gt;&gt; x['feat2'] = torch.rand(1, 20, 16, 16)\n        &gt;&gt;&gt; x['feat3'] = torch.rand(1, 30, 8, 8)\n        &gt;&gt;&gt; # compute the FPN on top of x\n        &gt;&gt;&gt; output = m(x)\n        &gt;&gt;&gt; print([(k, v.shape) for k, v in output.items()])\n        &gt;&gt;&gt; # returns\n        &gt;&gt;&gt;   [('feat0', torch.Size([1, 5, 64, 64])),\n        &gt;&gt;&gt;    ('feat2', torch.Size([1, 5, 16, 16])),\n        &gt;&gt;&gt;    ('feat3', torch.Size([1, 5, 8, 8]))]\n\n    \"\"\"\n\n    _version = 2\n\n    def __init__(\n        self,\n        in_channels_list: List[int],\n        out_channels: int,\n        extra_blocks: Optional[ExtraFPNBlock] = None,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ):\n        super().__init__()\n        _log_api_usage_once(self)\n        self.inner_blocks = nn.ModuleList()\n        self.layer_blocks = nn.ModuleList()\n        for in_channels in in_channels_list:\n            if in_channels == 0:\n                raise ValueError(\"in_channels=0 is currently not supported\")\n            inner_block_module = Conv2dNormActivation(\n                in_channels, out_channels, kernel_size=1, padding=0, norm_layer=norm_layer, activation_layer=None\n            )\n            layer_block_module = Conv2dNormActivation(\n                out_channels, out_channels, kernel_size=3, norm_layer=norm_layer, activation_layer=None\n            )\n            self.inner_blocks.append(inner_block_module)\n            self.layer_blocks.append(layer_block_module)\n\n        # initialize parameters now to avoid modifying the initialization of top_blocks\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_uniform_(m.weight, a=1)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n        if extra_blocks is not None:\n            if not isinstance(extra_blocks, ExtraFPNBlock):\n                raise TypeError(f\"extra_blocks should be of type ExtraFPNBlock not {type(extra_blocks)}\")\n        self.extra_blocks = extra_blocks\n        self.ff_add = FloatFunctional()\n\n    def _load_from_state_dict(\n        self,\n        state_dict,\n        prefix,\n        local_metadata,\n        strict,\n        missing_keys,\n        unexpected_keys,\n        error_msgs,\n    ):\n        version = local_metadata.get(\"version\", None)\n\n        if version is None or version &lt; 2:\n            num_blocks = len(self.inner_blocks)\n            for block in [\"inner_blocks\", \"layer_blocks\"]:\n                for i in range(num_blocks):\n                    for type in [\"weight\", \"bias\"]:\n                        old_key = f\"{prefix}{block}.{i}.{type}\"\n                        new_key = f\"{prefix}{block}.{i}.0.{type}\"\n                        if old_key in state_dict:\n                            state_dict[new_key] = state_dict.pop(old_key)\n\n        super()._load_from_state_dict(\n            state_dict,\n            prefix,\n            local_metadata,\n            strict,\n            missing_keys,\n            unexpected_keys,\n            error_msgs,\n        )\n\n    def get_result_from_inner_blocks(self, x: Tensor, idx: int) -&gt; Tensor:\n        \"\"\"\n        This is equivalent to self.inner_blocks[idx](x),\n        but torchscript doesn't support this yet\n        \"\"\"\n        num_blocks = len(self.inner_blocks)\n        if idx &lt; 0:\n            idx += num_blocks\n        out = x\n        for i, module in enumerate(self.inner_blocks):\n            if i == idx:\n                out = module(x)\n        return out\n\n    def get_result_from_layer_blocks(self, x: Tensor, idx: int) -&gt; Tensor:\n        \"\"\"\n        This is equivalent to self.layer_blocks[idx](x),\n        but torchscript doesn't support this yet\n        \"\"\"\n        num_blocks = len(self.layer_blocks)\n        if idx &lt; 0:\n            idx += num_blocks\n        out = x\n        for i, module in enumerate(self.layer_blocks):\n            if i == idx:\n                out = module(x)\n        return out\n\n    def forward(self, x: Dict[str, Tensor]) -&gt; Dict[str, Tensor]:\n        \"\"\"\n        Computes the FPN for a set of feature maps.\n\n        Args:\n            x (OrderedDict[Tensor]): feature maps for each feature level.\n\n        Returns:\n            results (OrderedDict[Tensor]): feature maps after FPN layers.\n                They are ordered from the highest resolution first.\n        \"\"\"\n        # unpack OrderedDict into two lists for easier handling\n        names = list(x.keys())\n        x = list(x.values())\n\n        last_inner = self.get_result_from_inner_blocks(x[-1], -1)\n        results = []\n        results.append(self.get_result_from_layer_blocks(last_inner, -1))\n\n        for idx in range(len(x) - 2, -1, -1):\n            inner_lateral = self.get_result_from_inner_blocks(x[idx], idx)\n            feat_shape = inner_lateral.shape[-2:]\n            inner_top_down = torch.nn.functional.interpolate(last_inner, size=feat_shape, mode=\"nearest\")\n            # last_inner = inner_lateral + inner_top_down\n            last_inner = self.ff_add.add(inner_lateral, inner_top_down)\n            results.insert(0, self.get_result_from_layer_blocks(last_inner, idx))\n\n        if self.extra_blocks is not None:\n            results, names = self.extra_blocks(results, x, names)\n\n        # make it back an OrderedDict\n        out = OrderedDict([(k, v) for k, v in zip(names, results)])\n\n        return out\n\nAs you can see from the signature of the modified FPN, it also needs to know input dimensions of each layer from the resnet. There are several ways to get this but one way is to simply get the number of features in the final module of each layer.\n\n# from backbone_utils.py\n# https://github.com/pytorch/vision/blob/main/torchvision/models/detection/backbone_utils.py#L145\n# in_channels_stage2 = res101_layers.inplanes // 8\n# in_channels_list = [in_channels_stage2 * 2 ** (i - 1) for i in returned_layers]\n\nin_channels_list = []\nfor k1, m1 in resnet.named_children():\n  if 'layer' in k1:\n    in_channels_list.append((m1[-1].bn3.num_features))\n\nin_channels_list\n\n[256, 512, 1024, 2048]\n\n\nNext step is to create a modified BackboneWithFPN that uses our FeaturePyramidNetworkQuantizeable. Here we’ll also make sure that the inputs are quantized and the outputs subsequently dequantized so that they can be fed to the rest of the R-CNN.\nOne important note is that regular BatchNorm2d is the default normalization layer which is used instead of FrozenBatchNorm2d. Frozen batch norm is the recommended layer because batches are generally too small for good estimates of mean and variance statistics but that module isn’t quantizeable. So using regular batch norm could be unstable and less performant if those layers aren’t frozen before training.\n\nfrom torchvision.ops.feature_pyramid_network import LastLevelMaxPool\nfrom torchvision.models._utils import IntermediateLayerGetter\nfrom torch.ao.quantization import QuantStub, DeQuantStub\n\n\nclass BackboneWithFPNQuantizeable(nn.Module):\n\n    def __init__(\n        self,\n        backbone: nn.Module,\n        return_layers: Dict[str, str],\n        in_channels_list: List[int],\n        out_channels: int,\n        extra_blocks: Optional[ExtraFPNBlock] = None,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -&gt; None:\n        super().__init__()\n\n        self.quant = QuantStub()\n        self.dequant = DeQuantStub()\n\n        if extra_blocks is None:\n            extra_blocks = LastLevelMaxPool()\n\n        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n        self.fpn = FeaturePyramidNetworkQuantizeable(\n            in_channels_list=in_channels_list,\n            out_channels=out_channels,\n            extra_blocks=extra_blocks,\n            norm_layer=norm_layer\n        )\n        self.out_channels = out_channels\n\n    def forward(self, x: Tensor) -&gt; Dict[str, Tensor]:\n        x = self.quant(x)\n        x = self.body(x)\n        x = self.fpn(x)\n        for k, v in x.items():\n            x[k] = self.dequant(v)\n        return x\n\nNow we can create the modified backbone with FPN. Once created, there should be quant/dequant stubs visible in the network like so\nBackboneWithFPNQuantizeable(\n  (quant): QuantStub()\n  (dequant): DeQuantStub()\n  (body): IntermediateLayerGetter(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): BottleneckQuantizeable(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        ...\n\n# resnet = resnet_101()\n# returned_layers = [1, 2, 3, 4]\n# return_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}\n# in_channels_list = []\n# for k1, m1 in resnet.named_children():\n#   if 'layer' in k1:\n#     in_channels_list.append((m1[-1].bn3.num_features))\n\nbbfpn = BackboneWithFPNQuantizeable(\n    backbone=resnet,\n    return_layers=return_layers,\n    in_channels_list=in_channels_list,\n    out_channels=256,\n    extra_blocks=None,\n    norm_layer=None,\n)\n# bbfpn\n\nThe last step is to plug in the modified backbone with FPN when creating the Faster R-CNN. Note that the number of classes is set to 2 (object or background) which is specific to the dataset used.\n\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN\n\n\nquant_rcnn = FasterRCNN(bbfpn, num_classes=2)\n\n\n\nLayer Fusion and Quantization Config\nBefore training and subsequently converting the model, we can fuse specific sequences of modules in the backbone. Fusing compresses the model making it smaller and run faster by combining modules like Conv2d-BatchNorm2d-ReLU and Conv2d-BatchNorm2d. After fusing you should see new fused modules in the network like ConvReLU2d as well as Identity where previous modules were.\nFasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPNQuantizeable(\n    (body): IntermediateLayerGetter(\n      (conv1): ConvReLU2d(\n        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n        (1): ReLU(inplace=True)\n      )\n      (bn1): Identity()\n      (relu): Identity()\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): BottleneckQuantizeable(\n          (conv1): ConvReLU2d(\n            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n            (1): ReLU(inplace=True)\n          )\n          (bn1): Identity()\n          (relu1): Identity()\n          ...\n\nfrom torch.ao.quantization import fuse_modules\n\n\nquant_rcnn.eval()\n# fuse stem\nfuse_modules(quant_rcnn.backbone.body, [['conv1', 'bn1', 'relu']], inplace=True)\n# fuse blocks\nfor k1, m1 in quant_rcnn.backbone.body.named_children():\n    if \"layer\" in k1:  # in sequential layer with blocks\n        for k2, m2 in m1.named_children():\n            fuse_modules(m2, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\", \"relu2\"], [\"conv3\", \"bn3\"]], inplace=True)\n            for k3, m3 in m2.named_children():\n                if \"downsample\" in k3:  # fuse downsample\n                    fuse_modules(m3, [[\"0\", \"1\"]], inplace=True)\n\nBefore training, the quantization config needs to be set on the backbone only. And again, because the batches are so small, batch norm gets frozen (see this pytorch tutorial for another example). Last, I’ll freeze the stem and the first layer in the backbone since the pretrained imagenet weights were loaded. After preparation you should be able to see the observers in the network.\nFasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPNQuantizeable(\n    (body): IntermediateLayerGetter(\n      (conv1): ConvReLU2d(\n        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n        )\n        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n        )\n      )\n      (bn1): Identity()\n      ...\n\nimport re\nimport torch\nfrom torch.ao.quantization import get_default_qat_qconfig, prepare_qat\n\n\nquant_rcnn.train()\nquant_rcnn.backbone.qconfig = get_default_qat_qconfig('fbgemm')\nquant_rcnn_prepared = prepare_qat(quant_rcnn, inplace=False)\n\nquant_rcnn_prepared = quant_rcnn_prepared.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n\nquant_rcnn_prepared.backbone.body.conv1.weight.requires_grad = False\nfor name, parameter in quant_rcnn_prepared.backbone.named_parameters():\n    if re.search(r\".layer1\", name):\n        parameter.requires_grad = False\n\n\n\nDataset, Training, and Conversion\nI’ll be using the PennFudan dataset from the Torchvision object detection finetuning tutorial for QAT. Most of the code below is borrowed from that tutorial with slight modifications and no segmentation.\n\nimport os\nimport torch\n\nfrom torchvision.io import read_image\nfrom torchvision.ops.boxes import masks_to_boxes\nfrom torchvision import tv_tensors\nfrom torchvision.transforms.v2 import functional as F  # careful namespace 'F'\nfrom torchvision.transforms import v2 as T\n\n\nclass PennFudanDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = read_image(img_path)\n        mask = read_image(mask_path)\n        # instances are encoded as different colors\n        obj_ids = torch.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n        num_objs = len(obj_ids)\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n\n        # get bounding box coordinates for each mask\n        boxes = masks_to_boxes(masks)\n\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n\n        image_id = idx\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        # Wrap sample and targets into torchvision tv_tensors:\n        img = tv_tensors.Image(img)\n\n        target = {}\n        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n        target[\"masks\"] = tv_tensors.Mask(masks)\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)\n\n\ndef get_transform(train):\n    transforms = []\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    transforms.append(T.ToDtype(torch.float, scale=True))\n    transforms.append(T.ToPureTensor())\n    return T.Compose(transforms)\n\n\n%%capture\n\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")\n\n!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n!unzip PennFudanPed.zip -d ./\n\n\nimport utils\nfrom engine import train_one_epoch, evaluate\n\n\n# train on the GPU or on the CPU, if a GPU is not available\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n# use our dataset and defined transformations\ndataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\ndataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n\n# split the dataset in train and test set\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=2,\n    shuffle=True,\n    num_workers=1,\n    collate_fn=utils.collate_fn\n)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test,\n    batch_size=1,\n    shuffle=False,\n    num_workers=1,\n    collate_fn=utils.collate_fn\n)\n\n\n# move model to the right device\nquant_rcnn_prepared.to(device)\n\n# construct an optimizer\nparams = [p for p in quant_rcnn_prepared.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(\n    params,\n    lr=0.005,\n    momentum=0.9,\n    weight_decay=0.0005\n)\n\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=3,\n    gamma=0.1\n)\n\n# let's train it for 10 epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 20 iterations\n    train_one_epoch(quant_rcnn_prepared, optimizer, data_loader, device, epoch, print_freq=20)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(quant_rcnn_prepared, data_loader_test, device=device)\n\nEpoch: [0]  [ 0/60]  eta: 0:00:49  lr: 0.000090  loss: 1.5364 (1.5364)  loss_classifier: 0.8895 (0.8895)  loss_box_reg: 0.0007 (0.0007)  loss_objectness: 0.6395 (0.6395)  loss_rpn_box_reg: 0.0069 (0.0069)  time: 0.8299  data: 0.1936  max mem: 6367\nEpoch: [0]  [20/60]  eta: 0:00:25  lr: 0.001783  loss: 0.6043 (0.7210)  loss_classifier: 0.1105 (0.2343)  loss_box_reg: 0.0665 (0.0648)  loss_objectness: 0.3679 (0.3956)  loss_rpn_box_reg: 0.0270 (0.0263)  time: 0.6163  data: 0.0064  max mem: 6367\nEpoch: [0]  [40/60]  eta: 0:00:12  lr: 0.003476  loss: 0.3219 (0.5298)  loss_classifier: 0.0957 (0.1841)  loss_box_reg: 0.1185 (0.0904)  loss_objectness: 0.0582 (0.2321)  loss_rpn_box_reg: 0.0194 (0.0233)  time: 0.6057  data: 0.0056  max mem: 6367\nEpoch: [0]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2767 (0.4805)  loss_classifier: 0.0943 (0.1721)  loss_box_reg: 0.1098 (0.0993)  loss_objectness: 0.0459 (0.1849)  loss_rpn_box_reg: 0.0160 (0.0242)  time: 0.6120  data: 0.0057  max mem: 6367\nEpoch: [0] Total time: 0:00:37 (0.6183 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:23  model_time: 0.2870 (0.2870)  evaluator_time: 0.0120 (0.0120)  time: 0.4753  data: 0.1745  max mem: 6367\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2425 (0.2465)  evaluator_time: 0.0037 (0.0049)  time: 0.2517  data: 0.0030  max mem: 6367\nTest: Total time: 0:00:13 (0.2624 s / it)\nAveraged stats: model_time: 0.2425 (0.2465)  evaluator_time: 0.0037 (0.0049)\nAccumulating evaluation results...\nDONE (t=0.04s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.111\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.432\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.013\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.119\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.074\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.238\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.299\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.013\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.322\nEpoch: [1]  [ 0/60]  eta: 0:00:50  lr: 0.005000  loss: 0.2406 (0.2406)  loss_classifier: 0.0861 (0.0861)  loss_box_reg: 0.1205 (0.1205)  loss_objectness: 0.0287 (0.0287)  loss_rpn_box_reg: 0.0053 (0.0053)  time: 0.8447  data: 0.1997  max mem: 6367\nEpoch: [1]  [20/60]  eta: 0:00:25  lr: 0.005000  loss: 0.2323 (0.2870)  loss_classifier: 0.0756 (0.1012)  loss_box_reg: 0.1083 (0.1261)  loss_objectness: 0.0416 (0.0443)  loss_rpn_box_reg: 0.0130 (0.0154)  time: 0.6140  data: 0.0056  max mem: 6367\nEpoch: [1]  [40/60]  eta: 0:00:12  lr: 0.005000  loss: 0.3195 (0.3082)  loss_classifier: 0.1031 (0.1040)  loss_box_reg: 0.1658 (0.1469)  loss_objectness: 0.0288 (0.0396)  loss_rpn_box_reg: 0.0207 (0.0177)  time: 0.6341  data: 0.0056  max mem: 6983\nEpoch: [1]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2555 (0.2919)  loss_classifier: 0.0822 (0.0958)  loss_box_reg: 0.1283 (0.1443)  loss_objectness: 0.0088 (0.0319)  loss_rpn_box_reg: 0.0179 (0.0199)  time: 0.6182  data: 0.0055  max mem: 6983\nEpoch: [1] Total time: 0:00:37 (0.6288 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:24  model_time: 0.3046 (0.3046)  evaluator_time: 0.0079 (0.0079)  time: 0.4994  data: 0.1854  max mem: 6983\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2479 (0.2522)  evaluator_time: 0.0022 (0.0028)  time: 0.2579  data: 0.0033  max mem: 6983\nTest: Total time: 0:00:13 (0.2663 s / it)\nAveraged stats: model_time: 0.2479 (0.2522)  evaluator_time: 0.0022 (0.0028)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.831\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.139\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.172\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.321\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.190\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.413\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.437\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.300\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.448\nEpoch: [2]  [ 0/60]  eta: 0:00:51  lr: 0.005000  loss: 0.2004 (0.2004)  loss_classifier: 0.0508 (0.0508)  loss_box_reg: 0.1245 (0.1245)  loss_objectness: 0.0078 (0.0078)  loss_rpn_box_reg: 0.0173 (0.0173)  time: 0.8501  data: 0.1844  max mem: 6983\nEpoch: [2]  [20/60]  eta: 0:00:26  lr: 0.005000  loss: 0.2482 (0.2507)  loss_classifier: 0.0578 (0.0676)  loss_box_reg: 0.1521 (0.1533)  loss_objectness: 0.0069 (0.0085)  loss_rpn_box_reg: 0.0191 (0.0213)  time: 0.6400  data: 0.0056  max mem: 6983\nEpoch: [2]  [40/60]  eta: 0:00:12  lr: 0.005000  loss: 0.1892 (0.2265)  loss_classifier: 0.0588 (0.0633)  loss_box_reg: 0.1038 (0.1351)  loss_objectness: 0.0061 (0.0092)  loss_rpn_box_reg: 0.0143 (0.0189)  time: 0.6334  data: 0.0054  max mem: 6983\nEpoch: [2]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.1695 (0.2197)  loss_classifier: 0.0545 (0.0631)  loss_box_reg: 0.0872 (0.1279)  loss_objectness: 0.0054 (0.0102)  loss_rpn_box_reg: 0.0138 (0.0186)  time: 0.6422  data: 0.0060  max mem: 6983\nEpoch: [2] Total time: 0:00:38 (0.6448 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:23  model_time: 0.2871 (0.2871)  evaluator_time: 0.0039 (0.0039)  time: 0.4719  data: 0.1796  max mem: 6983\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2536 (0.2540)  evaluator_time: 0.0017 (0.0021)  time: 0.2615  data: 0.0031  max mem: 6983\nTest: Total time: 0:00:13 (0.2672 s / it)\nAveraged stats: model_time: 0.2536 (0.2540)  evaluator_time: 0.0017 (0.0021)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.949\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.386\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.324\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.496\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.272\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.564\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.572\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.412\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.584\nEpoch: [3]  [ 0/60]  eta: 0:00:53  lr: 0.000500  loss: 0.1430 (0.1430)  loss_classifier: 0.0370 (0.0370)  loss_box_reg: 0.0875 (0.0875)  loss_objectness: 0.0089 (0.0089)  loss_rpn_box_reg: 0.0096 (0.0096)  time: 0.8995  data: 0.2071  max mem: 6983\nEpoch: [3]  [20/60]  eta: 0:00:26  lr: 0.000500  loss: 0.1757 (0.1837)  loss_classifier: 0.0432 (0.0492)  loss_box_reg: 0.1194 (0.1130)  loss_objectness: 0.0066 (0.0068)  loss_rpn_box_reg: 0.0141 (0.0147)  time: 0.6443  data: 0.0057  max mem: 6983\nEpoch: [3]  [40/60]  eta: 0:00:13  lr: 0.000500  loss: 0.1488 (0.1730)  loss_classifier: 0.0427 (0.0477)  loss_box_reg: 0.0932 (0.1054)  loss_objectness: 0.0046 (0.0062)  loss_rpn_box_reg: 0.0106 (0.0136)  time: 0.6576  data: 0.0057  max mem: 6983\nEpoch: [3]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1673 (0.1781)  loss_classifier: 0.0469 (0.0508)  loss_box_reg: 0.0972 (0.1080)  loss_objectness: 0.0045 (0.0059)  loss_rpn_box_reg: 0.0096 (0.0134)  time: 0.6530  data: 0.0053  max mem: 6983\nEpoch: [3] Total time: 0:00:39 (0.6593 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:23  model_time: 0.2835 (0.2835)  evaluator_time: 0.0039 (0.0039)  time: 0.4670  data: 0.1781  max mem: 6983\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2530 (0.2555)  evaluator_time: 0.0013 (0.0018)  time: 0.2578  data: 0.0032  max mem: 6983\nTest: Total time: 0:00:13 (0.2685 s / it)\nAveraged stats: model_time: 0.2530 (0.2555)  evaluator_time: 0.0013 (0.0018)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.493\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.942\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.419\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.362\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.285\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.571\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.574\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.585\nEpoch: [4]  [ 0/60]  eta: 0:00:54  lr: 0.000500  loss: 0.1610 (0.1610)  loss_classifier: 0.0388 (0.0388)  loss_box_reg: 0.1063 (0.1063)  loss_objectness: 0.0052 (0.0052)  loss_rpn_box_reg: 0.0107 (0.0107)  time: 0.9067  data: 0.2171  max mem: 6983\nEpoch: [4]  [20/60]  eta: 0:00:27  lr: 0.000500  loss: 0.1496 (0.1722)  loss_classifier: 0.0469 (0.0516)  loss_box_reg: 0.0883 (0.1041)  loss_objectness: 0.0025 (0.0046)  loss_rpn_box_reg: 0.0117 (0.0120)  time: 0.6716  data: 0.0062  max mem: 6983\nEpoch: [4]  [40/60]  eta: 0:00:13  lr: 0.000500  loss: 0.1522 (0.1744)  loss_classifier: 0.0459 (0.0516)  loss_box_reg: 0.1082 (0.1082)  loss_objectness: 0.0038 (0.0043)  loss_rpn_box_reg: 0.0068 (0.0103)  time: 0.6789  data: 0.0058  max mem: 6983\nEpoch: [4]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1792 (0.1787)  loss_classifier: 0.0513 (0.0527)  loss_box_reg: 0.1076 (0.1115)  loss_objectness: 0.0029 (0.0041)  loss_rpn_box_reg: 0.0100 (0.0105)  time: 0.6734  data: 0.0055  max mem: 6983\nEpoch: [4] Total time: 0:00:40 (0.6817 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:23  model_time: 0.2894 (0.2894)  evaluator_time: 0.0040 (0.0040)  time: 0.4672  data: 0.1722  max mem: 6983\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2520 (0.2579)  evaluator_time: 0.0014 (0.0018)  time: 0.2610  data: 0.0030  max mem: 6983\nTest: Total time: 0:00:13 (0.2706 s / it)\nAveraged stats: model_time: 0.2520 (0.2579)  evaluator_time: 0.0014 (0.0018)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.528\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.952\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.546\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.338\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.542\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.293\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.605\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.615\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.463\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.626\nEpoch: [5]  [ 0/60]  eta: 0:00:52  lr: 0.000500  loss: 0.1348 (0.1348)  loss_classifier: 0.0358 (0.0358)  loss_box_reg: 0.0862 (0.0862)  loss_objectness: 0.0018 (0.0018)  loss_rpn_box_reg: 0.0110 (0.0110)  time: 0.8712  data: 0.1820  max mem: 6983\nEpoch: [5]  [20/60]  eta: 0:00:27  lr: 0.000500  loss: 0.1626 (0.1758)  loss_classifier: 0.0444 (0.0485)  loss_box_reg: 0.0996 (0.1133)  loss_objectness: 0.0029 (0.0039)  loss_rpn_box_reg: 0.0098 (0.0101)  time: 0.6826  data: 0.0058  max mem: 6983\nEpoch: [5]  [40/60]  eta: 0:00:13  lr: 0.000500  loss: 0.1343 (0.1651)  loss_classifier: 0.0395 (0.0468)  loss_box_reg: 0.0837 (0.1052)  loss_objectness: 0.0023 (0.0037)  loss_rpn_box_reg: 0.0089 (0.0095)  time: 0.6910  data: 0.0059  max mem: 7287\nEpoch: [5]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1644 (0.1699)  loss_classifier: 0.0499 (0.0484)  loss_box_reg: 0.0954 (0.1076)  loss_objectness: 0.0026 (0.0038)  loss_rpn_box_reg: 0.0098 (0.0100)  time: 0.6861  data: 0.0062  max mem: 7287\nEpoch: [5] Total time: 0:00:41 (0.6929 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:24  model_time: 0.3069 (0.3069)  evaluator_time: 0.0041 (0.0041)  time: 0.4943  data: 0.1818  max mem: 7287\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2673 (0.2650)  evaluator_time: 0.0014 (0.0018)  time: 0.2721  data: 0.0030  max mem: 7287\nTest: Total time: 0:00:13 (0.2780 s / it)\nAveraged stats: model_time: 0.2673 (0.2650)  evaluator_time: 0.0014 (0.0018)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.560\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.957\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.602\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.451\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.570\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.311\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.644\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.647\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.550\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.655\nEpoch: [6]  [ 0/60]  eta: 0:00:55  lr: 0.000050  loss: 0.1775 (0.1775)  loss_classifier: 0.0405 (0.0405)  loss_box_reg: 0.1216 (0.1216)  loss_objectness: 0.0029 (0.0029)  loss_rpn_box_reg: 0.0125 (0.0125)  time: 0.9273  data: 0.2030  max mem: 7287\nEpoch: [6]  [20/60]  eta: 0:00:28  lr: 0.000050  loss: 0.1606 (0.1758)  loss_classifier: 0.0500 (0.0530)  loss_box_reg: 0.0945 (0.1087)  loss_objectness: 0.0032 (0.0039)  loss_rpn_box_reg: 0.0090 (0.0102)  time: 0.6964  data: 0.0055  max mem: 7287\nEpoch: [6]  [40/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1137 (0.1597)  loss_classifier: 0.0357 (0.0480)  loss_box_reg: 0.0660 (0.0997)  loss_objectness: 0.0018 (0.0034)  loss_rpn_box_reg: 0.0057 (0.0086)  time: 0.6934  data: 0.0057  max mem: 7287\nEpoch: [6]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1633 (0.1669)  loss_classifier: 0.0410 (0.0493)  loss_box_reg: 0.0954 (0.1050)  loss_objectness: 0.0024 (0.0035)  loss_rpn_box_reg: 0.0081 (0.0091)  time: 0.6901  data: 0.0058  max mem: 7287\nEpoch: [6] Total time: 0:00:41 (0.6998 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:24  model_time: 0.3050 (0.3050)  evaluator_time: 0.0035 (0.0035)  time: 0.4857  data: 0.1758  max mem: 7287\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2615 (0.2624)  evaluator_time: 0.0013 (0.0018)  time: 0.2673  data: 0.0031  max mem: 7287\nTest: Total time: 0:00:13 (0.2757 s / it)\nAveraged stats: model_time: 0.2615 (0.2624)  evaluator_time: 0.0013 (0.0018)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.537\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.948\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.534\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.547\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.294\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.628\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.642\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.550\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.649\nEpoch: [7]  [ 0/60]  eta: 0:00:55  lr: 0.000050  loss: 0.0931 (0.0931)  loss_classifier: 0.0252 (0.0252)  loss_box_reg: 0.0566 (0.0566)  loss_objectness: 0.0020 (0.0020)  loss_rpn_box_reg: 0.0093 (0.0093)  time: 0.9269  data: 0.2053  max mem: 7287\nEpoch: [7]  [20/60]  eta: 0:00:28  lr: 0.000050  loss: 0.1182 (0.1405)  loss_classifier: 0.0339 (0.0406)  loss_box_reg: 0.0713 (0.0877)  loss_objectness: 0.0017 (0.0043)  loss_rpn_box_reg: 0.0049 (0.0079)  time: 0.6926  data: 0.0066  max mem: 7462\nEpoch: [7]  [40/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1324 (0.1550)  loss_classifier: 0.0367 (0.0454)  loss_box_reg: 0.0773 (0.0970)  loss_objectness: 0.0020 (0.0038)  loss_rpn_box_reg: 0.0087 (0.0088)  time: 0.6966  data: 0.0055  max mem: 7462\nEpoch: [7]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1809 (0.1661)  loss_classifier: 0.0493 (0.0488)  loss_box_reg: 0.1123 (0.1040)  loss_objectness: 0.0038 (0.0042)  loss_rpn_box_reg: 0.0103 (0.0092)  time: 0.6901  data: 0.0055  max mem: 7462\nEpoch: [7] Total time: 0:00:42 (0.7001 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:24  model_time: 0.2994 (0.2994)  evaluator_time: 0.0039 (0.0039)  time: 0.4838  data: 0.1787  max mem: 7462\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2554 (0.2593)  evaluator_time: 0.0013 (0.0017)  time: 0.2629  data: 0.0029  max mem: 7462\nTest: Total time: 0:00:13 (0.2721 s / it)\nAveraged stats: model_time: 0.2554 (0.2593)  evaluator_time: 0.0013 (0.0017)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.530\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.951\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.491\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.451\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.540\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.283\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.616\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.618\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.562\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.623\nEpoch: [8]  [ 0/60]  eta: 0:00:58  lr: 0.000050  loss: 0.3004 (0.3004)  loss_classifier: 0.0767 (0.0767)  loss_box_reg: 0.2012 (0.2012)  loss_objectness: 0.0051 (0.0051)  loss_rpn_box_reg: 0.0173 (0.0173)  time: 0.9718  data: 0.2255  max mem: 7462\nEpoch: [8]  [20/60]  eta: 0:00:28  lr: 0.000050  loss: 0.1537 (0.1833)  loss_classifier: 0.0469 (0.0556)  loss_box_reg: 0.0927 (0.1127)  loss_objectness: 0.0018 (0.0034)  loss_rpn_box_reg: 0.0098 (0.0116)  time: 0.7003  data: 0.0058  max mem: 7462\nEpoch: [8]  [40/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1291 (0.1668)  loss_classifier: 0.0407 (0.0491)  loss_box_reg: 0.0915 (0.1050)  loss_objectness: 0.0029 (0.0032)  loss_rpn_box_reg: 0.0055 (0.0094)  time: 0.6929  data: 0.0056  max mem: 7462\nEpoch: [8]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1634 (0.1668)  loss_classifier: 0.0514 (0.0498)  loss_box_reg: 0.1034 (0.1045)  loss_objectness: 0.0024 (0.0032)  loss_rpn_box_reg: 0.0083 (0.0094)  time: 0.6999  data: 0.0056  max mem: 7462\nEpoch: [8] Total time: 0:00:42 (0.7050 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:24  model_time: 0.3030 (0.3030)  evaluator_time: 0.0037 (0.0037)  time: 0.4906  data: 0.1825  max mem: 7462\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2565 (0.2625)  evaluator_time: 0.0012 (0.0017)  time: 0.2623  data: 0.0030  max mem: 7462\nTest: Total time: 0:00:13 (0.2753 s / it)\nAveraged stats: model_time: 0.2565 (0.2625)  evaluator_time: 0.0012 (0.0017)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.534\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.951\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.526\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.293\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.550\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.289\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.618\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.619\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.450\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.632\nEpoch: [9]  [ 0/60]  eta: 0:00:57  lr: 0.000005  loss: 0.2058 (0.2058)  loss_classifier: 0.0578 (0.0578)  loss_box_reg: 0.1311 (0.1311)  loss_objectness: 0.0029 (0.0029)  loss_rpn_box_reg: 0.0140 (0.0140)  time: 0.9631  data: 0.2252  max mem: 7462\nEpoch: [9]  [20/60]  eta: 0:00:28  lr: 0.000005  loss: 0.1616 (0.1713)  loss_classifier: 0.0448 (0.0518)  loss_box_reg: 0.0901 (0.1049)  loss_objectness: 0.0042 (0.0042)  loss_rpn_box_reg: 0.0101 (0.0105)  time: 0.7072  data: 0.0055  max mem: 7462\nEpoch: [9]  [40/60]  eta: 0:00:14  lr: 0.000005  loss: 0.1449 (0.1669)  loss_classifier: 0.0415 (0.0500)  loss_box_reg: 0.1006 (0.1033)  loss_objectness: 0.0021 (0.0038)  loss_rpn_box_reg: 0.0069 (0.0098)  time: 0.7147  data: 0.0056  max mem: 7462\nEpoch: [9]  [59/60]  eta: 0:00:00  lr: 0.000005  loss: 0.1712 (0.1657)  loss_classifier: 0.0517 (0.0498)  loss_box_reg: 0.0998 (0.1030)  loss_objectness: 0.0019 (0.0034)  loss_rpn_box_reg: 0.0084 (0.0094)  time: 0.7149  data: 0.0065  max mem: 7462\nEpoch: [9] Total time: 0:00:43 (0.7191 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:24  model_time: 0.3057 (0.3057)  evaluator_time: 0.0038 (0.0038)  time: 0.4838  data: 0.1729  max mem: 7462\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2585 (0.2639)  evaluator_time: 0.0013 (0.0018)  time: 0.2670  data: 0.0030  max mem: 7462\nTest: Total time: 0:00:13 (0.2766 s / it)\nAveraged stats: model_time: 0.2585 (0.2639)  evaluator_time: 0.0013 (0.0018)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.535\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.950\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.550\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.550\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.297\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.624\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.634\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.475\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.646\n\n\nNow to convert and save the model. Make sure to put the model on CPU before conversion to avoid any errors. After conversion you should see quantized modules like QuantizedConvReLU2d.\nFasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPNQuantizeable(\n    (quant): Quantize(scale=tensor([0.0375]), zero_point=tensor([57]), dtype=torch.quint8)\n    (dequant): DeQuantize()\n    (body): IntermediateLayerGetter(\n      (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.2553767263889313, zero_point=0, padding=(3, 3))\n      (bn1): Identity()\n      (relu): Identity()\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): BottleneckQuantizeable(\n          (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.12423195689916611, zero_point=0)\n          (bn1): Identity()\n          (relu1): Identity()\n          ...\n\nfrom torch.ao.quantization import convert\n\n\nquant_rcnn_prepared.eval()\nquant_rcnn_prepared.to(torch.device('cpu'))\n\nquant_rcnn_converted = convert(quant_rcnn_prepared, inplace=False)\n\nquant_model_path = \"/content/quant_model.pth\"\ntorch.save(quant_rcnn_converted.state_dict(), quant_model_path)\n\nFor comparison I’ll generate the same network without any modifications made for quantization (including fusion). Then we can compare model sizes and latency. Note that this is just comparing latency on the CPU, if the float model was on GPU it could be significantly faster depending upon the hardware.\n\n%%capture\nfrom torchvision.models.detection.backbone_utils import BackboneWithFPN\nfrom torchvision.models.resnet import Bottleneck\n\n\nresnet_bb = resnet_101()\nrcnn = FasterRCNN(\n    BackboneWithFPN(\n        backbone=resnet_bb,\n        return_layers=return_layers,\n        in_channels_list=in_channels_list,\n        out_channels=256,\n        extra_blocks=None,\n        norm_layer=None,\n        ),\n    num_classes=2\n)\n\nrcnn.eval()\nrcnn.to(torch.device('cpu'))\nmodel_path = \"/content/float_model.pth\"\ntorch.save(rcnn.state_dict(), model_path)\n\n\nprint(f'size of quantized model: {round(os.path.getsize(\"/content/quant_model.pth\") / 1e6)} MB')\nprint(f'size of float model: {round(os.path.getsize(\"/content/float_model.pth\") / 1e6)} MB')\n\nsize of quantized model: 105 MB\nsize of float model: 242 MB\n\n\n\nfrom time import perf_counter\n\n\nquant_rcnn_converted.to(torch.device('cpu'))\n# just grab one test image/batch\nimages, targets = next(iter(data_loader_test))\nimages = list(img.to(torch.device('cpu')) for img in images)\nn = 10\n\nstart = perf_counter()\nfor _ in range(n):\n    __ = quant_rcnn_converted(images)\nprint(f\"quant model avg time: {(perf_counter() - start) / n:.2f}\")\n\nstart = perf_counter()\nfor _ in range(n):\n    __ = rcnn(images)\nprint(f\"float model avg time (cpu): {(perf_counter() - start) / n:.2f}\")\n\nquant model avg time: 1.42\nfloat model avg time (cpu): 2.20\n\n\nI believe a fully quantized model would be even smaller and faster by comparison. In this case, while we did quantize the backbone for the R-CNN, it only accounted for roughly 75% of model parameters. So a significant number of float operations still occur after the quantized backbone.\n\nnum_model_params = sum(p.numel() for p in rcnn.parameters() if p.requires_grad)\nnum_backbone_params = sum(p.numel() for p in rcnn.backbone.parameters() if p.requires_grad)\n\nprint(f\"total number of parameters in model: {num_model_params}\")\nprint(f\"total number of parameters in backbone: {num_backbone_params}\")\nprint(f\"ratio of quantized parameters: {num_backbone_params / num_model_params:.2f}\")\n\ntotal number of parameters in model: 60344409\ntotal number of parameters in backbone: 45844544\nratio of quantized parameters: 0.76\n\n\nWe can also profile each model to see where each spends the most time during a forward pass.\n\nfrom torch.profiler import profile, record_function, ProfilerActivity\n\n\nwith profile(activities=[ProfilerActivity.CPU], record_shapes=False) as prof:\n    with record_function(\"model_inference\"):\n        quant_rcnn_converted(images)\n\nprint(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n\nwith profile(activities=[ProfilerActivity.CPU], record_shapes=False) as prof:\n    with record_function(\"model_inference\"):\n        rcnn(images)\n\nprint(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n\n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                  model_inference         2.15%      29.904ms       100.00%        1.388s        1.388s             1  \n           quantized::conv2d_relu        26.61%     369.449ms        26.96%     374.209ms       5.585ms            67  \n                quantized::conv2d        23.33%     323.889ms        23.44%     325.335ms       7.230ms            45  \n           torchvision::roi_align        17.54%     243.538ms        20.78%     288.475ms      72.119ms             4  \n                     aten::conv2d         0.00%      64.000us        13.45%     186.652ms      12.443ms            15  \n                aten::convolution         0.03%     400.000us        13.44%     186.588ms      12.439ms            15  \n               aten::_convolution         0.02%     280.000us        13.41%     186.188ms      12.413ms            15  \n         aten::mkldnn_convolution        13.25%     183.875ms        13.29%     184.541ms      12.303ms            15  \n                     aten::linear         0.01%     115.000us         5.54%      76.905ms      19.226ms             4  \n                      aten::addmm         5.50%      76.300ms         5.52%      76.641ms      19.160ms             4  \n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 1.388s\n\n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                  model_inference         3.80%      79.209ms       100.00%        2.086s        2.086s             1  \n                     aten::conv2d         0.04%     759.000us        66.00%        1.377s      10.841ms           127  \n                aten::convolution         0.14%       3.015ms        65.96%        1.376s      10.835ms           127  \n               aten::_convolution         0.11%       2.234ms        65.82%        1.373s      10.811ms           127  \n         aten::mkldnn_convolution        65.44%        1.365s        65.71%        1.371s      10.793ms           127  \n           torchvision::roi_align        14.10%     294.166ms        14.37%     299.835ms      59.967ms             5  \n                 aten::batch_norm         0.02%     462.000us         3.95%      82.490ms     793.173us           104  \n     aten::_batch_norm_impl_index         0.05%     948.000us         3.93%      82.028ms     788.731us           104  \n          aten::native_batch_norm         3.73%      77.820ms         3.87%      80.832ms     777.231us           104  \n                     aten::linear         0.00%      41.000us         3.54%      73.933ms      18.483ms             4  \n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 2.086s\n\n\n\nThe following loads the saved quantized model. It’s important that the same process of fusing, preparing, and converting be done before loading weights since quantization significantly alters the network. For sake of completeness, we can look at a prediction from the partially quantized R-CNN.\n\n%%capture\n\nquant_model_loaded = FasterRCNN(\n    BackboneWithFPNQuantizeable(\n        backbone=resnet_101(),\n        return_layers=return_layers,\n        in_channels_list=in_channels_list,\n        out_channels=256,\n        extra_blocks=None,\n        norm_layer=None\n        ),\n    num_classes=2\n    )\n\nquant_model_loaded.eval()\nfuse_modules(quant_model_loaded.backbone.body, [['conv1', 'bn1', 'relu']], inplace=True)\nfor k1, m1 in quant_model_loaded.backbone.body.named_children():\n    if \"layer\" in k1:  # in sequential layer with blocks\n        for k2, m2 in m1.named_children():\n            fuse_modules(m2, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\", \"relu2\"], [\"conv3\", \"bn3\"]], inplace=True)\n            for k3, m3 in m2.named_children():\n                if \"downsample\" in k3:  # fuse downsample\n                    fuse_modules(m3, [[\"0\", \"1\"]], inplace=True)\n\nquant_model_loaded.train()\nquant_model_loaded.backbone.qconfig = torch.quantization.get_default_qconfig('fbgemm')\ntorch.quantization.prepare_qat(quant_model_loaded, inplace=True)\ntorch.quantization.convert(quant_model_loaded, inplace=True)\n\nquant_model_loaded.eval()\nquant_model_loaded.load_state_dict(torch.load(quant_model_path, map_location=torch.device('cpu')))\n\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\n\nimage = read_image(\"PennFudanPed/PNGImages/FudanPed00022.png\")  # 7, 22\neval_transform = get_transform(train=False)\n\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -&gt; RGB and move to device\n    x = x[:3, ...].to(torch.device('cpu'))\n    predictions = quant_model_loaded([x, ])\n    pred = predictions[0]\n\nthreshold = 0.50\nimage = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\nimage = image[:3, ...]\npred_labels = [f\"pedestrian: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"]) if score &gt; threshold]\npred_boxes = pred[\"boxes\"].long()[pred[\"scores\"] &gt; threshold]\n\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n\n# masks = (pred[\"masks\"] &gt; 0.7).squeeze(1)\n# output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))\n\n&lt;matplotlib.image.AxesImage at 0x781f0969da80&gt;\n\n\n\n\n\nAlthough this post looks polished and the process linear, early attemps were anything but. I ran into all sorts of issues during the model preparation phase, which appears to be a major downside of eager mode quantization. In another post, I plan to use FX graph mode quantization to compare the two methods."
  },
  {
    "objectID": "posts/gaussian-bayes-classifier/gaussian_bayes_classifier.html",
    "href": "posts/gaussian-bayes-classifier/gaussian_bayes_classifier.html",
    "title": "Gaussian Bayesian Classifiers",
    "section": "",
    "text": "Even though I studied (and revisited, and revisited..) Bayesian statistics several times over the years, I’ve always felt that it wasn’t an intuitive paradigm. Specifically, the bridge from conditional probability and Bayes theorem to Bayesian classifiers is something I’ve had to refresh. I created this post as a future reference but also as an excuse to do an in-depth review. Here I’ll derive the classifier, touch on maximum a posteriori and maximum likelihood, show what it means to make a naive assumption, and analyze decision boundaries under different conditions.\n\nmaximum a posteriori\nLet’s assume we have some data \\(X\\) that follows a Gaussian distribution\n\\[\nX \\sim N(\\mu, \\sigma)\n\\]\nand a variable \\(Y\\) which is discrete\n\\[\nY\\in\\{0, 1\\}\n\\]\nSuppose we know that the value of \\(Y\\) is dependent upon \\(X\\), but that the relationship is not deterministic. We can model this relationship using conditional probability\n\\[\nP(Y=y|X=x)\n\\]\nBut say we want to assign \\(Y\\) a definitive value (i.e., classify). In that case we can simply select the value of \\(Y\\) with the highest probability\n\\[\n\\arg\\max_ y P(Y|X)\n\\]\nAnd because we are selecting a value for \\(Y\\) when there is uncertainty, this means we are making an estimate. The above is known as the maximum a posteriori (MAP) estimate of \\(Y\\) given \\(X\\), and \\(P(Y|X)\\) is commonly referred to as the posterior.\nMost likely we won’t have knowledge of the posterior (\\(Y\\) is unknown afterall), so we use Bayes theorem to derive an equivalence\n\\[\nP(Y|X) = {P(Y \\cap X) \\over P(X)} = {P(X|Y) \\cdot P(Y) \\over P(X)}\n\\]\nwhere\n\n\\(P(X|Y)\\) is the likelhood (i.e., probability of the data given the class)\n\\(P(Y)\\) is the prior (i.e., probability of the class)\n\\(P(X)\\) is the marginal (i.e., probability of the data)\n\nWhen performing the MAP estimate, we are given some value of \\(X\\) and then calculate the posterior probability for each possible value of \\(Y\\). This means that the marginal is the same for all values of \\(Y\\) and is just a constant that can be factored out\n\\[\nP(Y|X) \\propto {P(X|Y) \\cdot P(Y)}\n\\]\nwhich simplifies the MAP classifier to\n\\[\n\\arg\\max_y {P(X|Y)  \\cdot P(Y)}\n\\]\nAs far as the likelihood function, we made an assumption on the distribution of \\(X\\) so we can use the Gaussian probability density function\n\\[\np(x|y) = \\frac{1}{\\sigma_y\\sqrt2\\pi} e ^ {- \\frac{1}{2} ( \\frac{x - \\mu_y}{\\sigma_y} ) ^2}\n\\]\nIf we don’t know the Gaussian parameters above, we just estimate them using the empirical mean and variance of the training data for each class which is a maximum likelihood estimate.\n\\[\n\\mu_y = \\frac{1}{n}\\sum_{i}^{n}x_i\n\\]\n\\[\n\\sigma_y^2 = \\frac{1}{n}\\sum_{i}^{n}(x_i - \\mu_y)^2\n\\]\nWe don’t know the distribution of the prior, so we have to estimate it. In practice, we simply use the prevalence of each class in the training data which is again a maximum likelihood estimate.\n\\[\np(y) = \\frac{1}{n}\\sum_{i}^{n} \\mathbb{1}(y_i = y)\n\\]\nIt’s worth noting that there is also a maximum likelihood estimate (MLE) that could be used for the classifier. As the name suggest we would just use the likelihood term and remove the prior\n\\[\n\\arg\\max_y {P(X|Y)}\n\\]\nbut this ignores the prior distribution if we have that information. So if the prior is available, then it’s advantageous to use it.\nWith some basic theory out of the way, let’s build a classifer.\n\n\nunivariate classifier\nLet’s simulate univariate Gaussian data for two classes. For simplicity, the data will have different means but the same variance.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\nsns.set()\n%matplotlib inline\n\n\nn = 1000\nmu_1 = 40\nmu_2 = 80\nstd = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std, size=n)\nx_2 = np.random.normal(loc=mu_2, scale=std, size=n)\n\ndf = pd.DataFrame({'x': np.concatenate([x_1, x_2]), 'y': [1] * n + [2] * n})\n\nsns.displot(df, kind='kde', x='x', hue='y',\n            fill=True, linewidth=0, palette='dark', alpha=0.5)\n\n\n\n\nTime to estimate priors, means, and standard deviations. This is trivial since we generated the data but let’s pretend that we didn’t :)\n\npriors = {k: df[df.y == k].size / df.size for k in df.y.unique()}\npriors\n\n{1: 0.5, 2: 0.5}\n\n\n\nmeans = {k: df[df.y == k].x.mean() for k in df.y.unique()}\nmeans\n\n{1: 39.81007946653686, 2: 79.64703348340272}\n\n\n\nstdevs = {k: df[df.y == k].x.std() for k in df.y.unique()}\n# .std(ddof=0) if not sample\nstdevs\n\n{1: 9.829502308649815, 2: 9.922409640186507}\n\n\nNow that the data is fit, we can build a classifier and predict new instances.\n\n# note: scipy actually has gaussian pdf: from scipy.stats import norm\n\ndef uni_gaussian_pdf(x, mean, stdev):\n    scalar = 1.0 / (stdev * np.sqrt(2 * np.pi))\n    exponential = np.exp(-0.5 * ((x - mean) / stdev) ** 2)\n    return scalar * exponential\n\nclasses = df.y.unique().tolist()\n\ndef gbayes_uni_classify(x):\n    probas = []\n    for c in classes:\n        likelihood = uni_gaussian_pdf(x, means[c], stdevs[c])\n        # likelihood = norm.pdf(x, means[c], stdevs[c])\n        probas.append(likelihood * priors[c])\n    return classes[np.argmax(probas)]\n\nIt’s important to mention here that the priors are the same since we generated equal amounts of data for both classes. Mathematically this means that the prior is a constant and can be factored out in the original MAP equation (for this case) giving\n\\[\n\\arg\\max_y {P(X|Y)}\n\\]\nSo in the case where priors are the same, the MAP is equivalent to the MLE.\nAnd now to visualize the decision boundary.\n\nsim_data = np.arange(0, 150, 1)  # uniform sequence\nsim_class_preds = [gbayes_uni_classify(x) for x in sim_data]\n\ndecision_boundary = np.where(np.array(sim_class_preds[:-1]) - np.array(sim_class_preds[1:]) != 0)[0]\nprint(decision_boundary)\n\nsns.displot(df, kind='kde', x='x', hue='y', fill=True, linewidth=0, palette='dark', alpha=0.5)\nfor v in sim_data[decision_boundary]:\n    plt.axvline(v, color='black', linestyle='--')\n\n[59]\n\n\n\n\n\nNo surprises here, the decision boundary is roughly halfway between means, as expected. Let’s make things more interesting and make the variances unequal…\n\ndef gen_uni_dists_df(n1, n2, mu1, mu2, std1, std2):\n    x1 = np.random.normal(loc=mu1, scale=std1, size=n1)\n    x2 = np.random.normal(loc=mu2, scale=std2, size=n2)\n    return pd.DataFrame({'x': np.concatenate([x1, x2]), 'y': [1] * n1 + [2] * n2})\n\ndf = gen_uni_dists_df(n1=1000, n2=1000, mu1=40, mu2=80, std1=20, std2=10)\n\nsns.displot(df, kind='kde', x='x', hue='y', fill=True, linewidth=0, palette='dark', alpha=0.5)\n\n\n\n\n\nclass GBUniClf:\n\n    def __init__(self):\n        self.classes = None\n        self.priors = None\n        self.means = None\n        self.stdevs = None\n\n    def fit(self, df):\n        self.classes = df.y.unique().tolist()\n        self.priors = {k: df[df.y == k].size / df.size for k in self.classes}\n        self.means = {k: df[df.y == k].x.mean() for k in self.classes}\n        self.stdevs = {k: df[df.y == k].x.std() for k in self.classes}\n\n    def likelihood(self, x, mean, stdev):\n        scalar = 1.0 / (stdev * np.sqrt(2 * np.pi))\n        exponential = np.exp(-0.5 * ((x - mean) / stdev) ** 2)\n        return scalar * exponential\n\n    def predict(self, x):\n        probas = []\n        for c in self.classes:\n            likelihood = self.likelihood(x, self.means[c], self.stdevs[c])\n            probas.append(likelihood * self.priors[c])\n        return self.classes[np.argmax(probas)]\n\n\ndef uni_decision_boundary(clf, df, sim_range):\n\n    sim_class_preds = [clf.predict(x) for x in sim_range]\n\n    decision_boundary = np.where(np.array(sim_class_preds[:-1]) - np.array(sim_class_preds[1:]) != 0)[0]\n    print(decision_boundary)\n\n    df_preds = pd.DataFrame({'x': sim_range, 'y': sim_class_preds})\n\n    sns.displot(df, kind='kde', x='x', hue='y', fill=True, linewidth=0, palette='dark', alpha=0.3)\n    rug_plot = sns.rugplot(df_preds, x=\"x\", hue=\"y\", palette='dark', alpha=0.7)\n    rug_plot.get_legend().remove()\n    for v in sim_range[decision_boundary]:\n        plt.axvline(v, color='black', linestyle='--')\n\n\nclf = GBUniClf()\nclf.fit(df)\n\nsim_range = np.arange(-50, 200, 1)\nuni_decision_boundary(clf, df, sim_range)\n\n[113 175]\n\n\n\n\n\nBecause class 1 has a larger variance, there is now a second decision boundary. Instances with high values of \\(x\\) (far right) are less likely to belong to class 2 even though they are closer to its’ mean. Instead they get classified as 1.\nWhat if the priors are different?\n\ndf = gen_uni_dists_df(n1=2000, n2=500, mu1=40, mu2=80, std1=20, std2=10)\n\nclf = GBUniClf()\nclf.fit(df)\n\nsim_range = np.arange(-50, 200, 1)\nuni_decision_boundary(clf, df, sim_range)\n\n[120 163]\n\n\n\n\n\nIt simply makes the more prevalent class more likely, not unexpected.\n\n\nmultivariate\nNow we can look at bivariate data where covariance between features come into play. The naive assumption ignores covariance so we can compare classifiers that do and do not make that assumption.\nMathematically, the posterior is now conditioned on multiple features\n\\[\nP(Y|X) = P(Y|x_1, x_2, \\ldots , x_i)\n\\]\nand the MAP classifier in the multivariate case is\n\\[\n\\arg\\max_y {P(Y) \\cdot P(x_1, x_2, \\ldots , x_i|Y)}\n\\]\nTherefore we use the multivariate likelihood function which makes use of covariance\n\\[\np(x|y) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma_y|}} e^{ - \\frac{1}{2} (x - \\mu_y)^T \\Sigma_y^{-1} (x - \\mu_y)}\n\\]\nThis is a drop-in replacement though, and the rest of the classifier is the same.\n\ndef gen_bi_dists_df(n1, n2, mu1, mu2, std1, std2):\n    x1 = np.random.normal(loc=mu1, scale=std1, size=(n1, 2))\n    x2 = np.random.normal(loc=mu2, scale=std2, size=(n2, 2))\n    data = np.concatenate([x1, x2])\n    return pd.DataFrame({'x1': data[:, 0], 'x2': data[:, 1], 'y': [1] * n1 + [2] * n2})\n\ndf = gen_bi_dists_df(n1=1000, n2=1000, mu1=40, mu2=80, std1=10, std2=10)\n\n# s = sns.scatterplot(df, x='x1', y='x2', hue='y', hue_order=classes, palette='dark', alpha=0.25)\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\n\nclass GBBiClf:\n\n    def __init__(self):\n        self.classes = None\n        self.priors = None\n        self.means = None\n        self.covars = None\n        self.covar_dets = None\n        self.covar_invs = None\n\n    def fit(self, df):\n        self.classes = df.y.unique().tolist()\n        self.priors = {k: df[df.y == k].shape[0] / df.shape[0] for k in self.classes}\n        self.means = {k: df[['x1', 'x2']][df.y == k].mean(axis=0) for k in self.classes}\n        self.covars = {k: np.cov(df[['x1', 'x2']][df.y == k], rowvar=False, bias=True) for k in self.classes}\n        self.covar_dets = {k: np.linalg.det(self.covars[k]) for k in self.classes}\n        self.covar_invs = {k: np.linalg.inv(self.covars[k]) for k in self.classes}\n\n    def likelihood(self, x, c):\n        dims = 2\n        scalar = 1.0 / np.sqrt(((2 * np.pi) ** dims) * self.covar_dets[c])\n        exponential = np.exp(-0.5 * (x - self.means[c]).T @ self.covar_invs[c] @ (x - self.means[c]))\n        return scalar * exponential\n\n    def predict(self, x):\n        probas = []\n        for c in self.classes:\n            likelihood = self.likelihood(x, c)\n            probas.append(likelihood * self.priors[c])\n        return self.classes[np.argmax(probas)]\n\n\ndef bi_decision_boundary(clf, df, sim_range):\n\n    sim_data = np.array([np.array([x1, x2]) for x1 in sim_range for x2 in sim_range])\n    sim_classes = [clf.predict(x) for x in sim_data]\n\n    plot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\n    # sns.scatterplot(plot_df, x='x1', y=\"x2\", hue=\"y\", hue_order=classes, palette='dark', marker=\".\", alpha=0.15)\n    plt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\n    plt_points._legend.remove()\n    s = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\n    sns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\n    return s\n\n\nclf = GBBiClf()\nclf.fit(df)\n\nsim_range = np.arange(0, 140, 1)\nbi_decision_boundary(clf, df, sim_range)\n\n&lt;Axes: xlabel='x1', ylabel='x2'&gt;\n\n\n\n\n\nThe variance was same for both distributions and the features were sampled independently, so the decision boundary isn’t complex. Slight curvature is due to the estimate of covariance which is different from the true value.\n\nclf.covars\n\n{1: array([[100.87705238,  -3.73052293],\n        [ -3.73052293,  99.42549259]]),\n 2: array([[106.57505726,  -3.75129152],\n        [ -3.75129152, 107.04125472]])}\n\n\nEven though this data is uninteresting, let’s compare the decision boundary of a naive classifier. The naive assumption is that all features are independent, so we can use the chain rule of probability for a simpler calculation of the likelihood.\n\\[\nP(X|Y) = P(x_1, x_2, \\ldots , x_i|Y) = \\prod\\limits_{i}P(x_i|Y)\n\\]\nThe MAP classifier under the naive assumption then becomes\n\\[\n\\arg\\max_y {P(Y) \\cdot P(x_1|Y) \\cdot P(x_2|Y) \\cdot \\ldots \\cdot P(x_m|Y)}\n\\]\nFor this case though, since the features were generated independently, the decision boundary should be roughly the same.\n\nclass GNBBiClf:\n\n    def __init__(self):\n        self.classes = None\n        self.priors = None\n        self.means = None\n\n    def fit(self, df):\n        self.classes = df.y.unique().tolist()\n        self.priors = {k: df[df.y == k].shape[0] / df.shape[0] for k in self.classes}\n        self.means = {k: df[['x1', 'x2']][df.y == k].mean(axis=0) for k in self.classes}\n        self.stdevs = {k: np.std(df[['x1', 'x2']][df.y == k], axis=0) for k in self.classes}\n\n    def likelihood(self, x, mean, stdev):\n        scalar = 1.0 / (stdev * np.sqrt(2 * np.pi))\n        exponential = np.exp(-0.5 * ((x - mean) / stdev) ** 2)\n        return scalar * exponential\n\n    def predict(self, x):\n        probas = []\n        for c in self.classes:\n            joint_likelihood = 1\n            for i, v in enumerate(x):\n                likelihood = self.likelihood(v, self.means[c][i], self.stdevs[c][i])\n                joint_likelihood *= likelihood\n            probas.append(joint_likelihood * self.priors[c])\n        return self.classes[np.argmax(probas)]\n\n\nclf = GNBBiClf()\nclf.fit(df)\n\nsim_range = np.arange(0, 140, 1)\nbi_decision_boundary(clf, df, sim_range)\n\n&lt;Axes: xlabel='x1', ylabel='x2'&gt;\n\n\n\n\n\nWhat if just the covariance are different? Let’s draw random data were the features are still independent (i.e. covariance matrix is symmetic) but the variance of features is different for each class.\n\ndf = gen_bi_dists_df(n1=1000, n2=1000, mu1=40, mu2=100, std1=20, std2=10)\n\nclf = GBBiClf()\nclf.fit(df)\n\nsim_range = np.arange(0, 140, 1)\nplot = bi_decision_boundary(clf, df, sim_range)\nplot.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\nAs expected, the decision boundary favors class 1 since it had larger variance. Without any correlation between features, I would again expect the decision boundary to be the same for the naive classifier.\n\nclf = GNBBiClf()\nclf.fit(df)\n# sanity check\n# from sklearn.naive_bayes import GaussianNB\n# clf = GaussianNB()\n\nsim_range = np.arange(-10, 200, 1)\nplot = bi_decision_boundary(clf, df, sim_range)\nplot.set(xlim=(-10, 200), ylim=(-10, 200))\n\n[(-10.0, 200.0), (-10.0, 200.0)]\n\n\n\n\n\nThe decision boundary for the naive classifier is roughly identical. Zooming out, we can see the classifier has similar behavior as the univariate case for different variance.\nLet’s finally simulate data with correlation between features. There should be a noticeable difference in the decision boundary for the naive classifier.\n\nn = 1000\nmu_1 = 40\nmu_2 = 100\n\nx_1 = np.random.multivariate_normal(mean=[mu_1, mu_1], cov=[[50, 70], [70, 200]], size=n)\nx_2 = np.random.multivariate_normal(mean=[mu_2, mu_2], cov=[[100, 1], [1, 100]], size=n)  # no correlation\ndata = np.concatenate([x_1, x_2])\n\ndf = pd.DataFrame({'x1': data[:, 0], 'x2': data[:, 1], 'y': [1] * n + [2] * n})\n\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\n\nclf = GBBiClf()\nclf.fit(df)\n\nsim_range = np.arange(-10, 140, 1)\nplot = bi_decision_boundary(clf, df, sim_range)\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\n\n# naive classifier\nclf = GNBBiClf()\nclf.fit(df)\n\nsim_range = np.arange(-10, 140, 1)\nplot = bi_decision_boundary(clf, df, sim_range)\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\nThe difference between the naive and non-naive classifier is more noticeable when there is correlation between features (class 1). The naive classifier clearly ignores covariance and the decision boundary is much smoother.\nOne advantage of the naive assumption is computational efficiency. Each feature is treated independently and there is no need to do matrix multiplication with the covariance matrix. As a result, predictions for the naive classifier ran in faster time compared to non-naive by an order of magnitude.\n\nfrom time import perf_counter\n\n# TODO memory reqs\n\nm = 100\n\nstart_time = perf_counter()\nfor i in range(m):\n    clf = GBBiClf()\n    clf.fit(df)\nprint(f\"GB avg fit time: {(perf_counter() - start_time) / m:.6f}\")\nstart_time = perf_counter()\nfor i in range(m):\n    clf.predict(sim_data[i])\nprint(f\"GB avg predict time: {(perf_counter() - start_time) / m:.6f}\")\n\nstart_time = perf_counter()\nfor i in range(m):\n    clf = GNBBiClf()\n    clf.fit(df)\nprint(f\"GNB avg fit time: {(perf_counter() - start_time) / m:.6f}\")\nstart_time = perf_counter()\nfor i in range(m):\n    clf.predict(sim_data[i])\nprint(f\"GNB avg predict time: {(perf_counter() - start_time) / m:.6f}\")\n\nGB avg fit time: 0.004047\nGB avg predict time: 0.000654\nGNB avg fit time: 0.005097\nGNB avg predict time: 0.000047"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cbhyphen",
    "section": "",
    "text": "Speeding up Inference with TensorRT\n\n\n\n\n\n\n\n\n\n\n\n\nMay 27, 2024\n\n\nchris\n\n\n\n\n\n\n  \n\n\n\n\nSample Size for A/B Hypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2024\n\n\nchris\n\n\n\n\n\n\n  \n\n\n\n\nMatrix Factorization for Recommender Systems\n\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nchris\n\n\n\n\n\n\n  \n\n\n\n\nFX Graph Mode Quantization in PyTorch\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nchris\n\n\n\n\n\n\n  \n\n\n\n\nEager Mode Quantization in PyTorch: Quantizing the Backbone of an Object Detection Model\n\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nchris\n\n\n\n\n\n\n  \n\n\n\n\nGaussian Bayesian Classifiers\n\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\nchris\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/matrix-factorization/matrix_factorization.html",
    "href": "posts/matrix-factorization/matrix_factorization.html",
    "title": "Matrix Factorization for Recommender Systems",
    "section": "",
    "text": "It’s pretty difficult to escape recommender systems in 2024. These days, we regularly get recommended things such as content from streaming services like Netflix and products from online shopping platforms like Amazon. Even if you don’t know the term recommender systems, there’s a good chance you understand what it does.\nI first heard about recommender systems in the context of the Netflix prize which was an open competition in 2009 for best collaborative filtering (CF) algorithm. The challenge was to improve upon Netflix’s existing recommender system by at least 0.10 RMSE and the winner was awarded $1MM. Around that time, nearest neighbor techniques were popular CF methods however, the winners of the Netflix prize proved that matrix factorization (MF) models were superior. MF models have some similarity to singular value decomposition (SVD) but can handle sparsity often seen with recommender system datasets whereas SVD can’t.\nTo understand more about MF and recommender systems, in this post I’ll be creating an algorithm from scratch and evaluating it’s performance on the MovieLens 20M dataset (see here for more info). In particular, I’ll be implementing probabilistic matrix factorization (PMF) which was a seminal improvement over previous MF techniques because of it’s ability to handle sparsity and scale linearly with data (see paper here).\n\nIntuition\nBefore diving into a mathematical derivation, let me provide some intuition about how MF works.\nIn a ratings scenario we have some number of n users and m movies. This can be represented with an n x m matrix where values of the matrix are the ranking a person gave to a movie. For simplicity, let’s say the value is a 1 if they watched and liked the movie otherwise it’s a 0. Most people will only watch a handful of movies so there’ll be many missing values in the matrix.\nIn the ratings matrix, each user can be described by the sparse vector of 1’s (mostly 0’s) that indicate the movies they liked (or the users that liked them in the case of movie vectors). But we can use a more compact representation similar to how one-hot encoded words are condensed via word embeddings. This allows for better comparison if we want to use something like cosine similarity to compare users, for example.\nThe dense representations of users and movies will form two smaller matrices, an n x d user matrix and an m x d movie matrix where d &lt;&lt; n and d &lt;&lt; m. You can see from a quick dimensionality analysis that the product of these two smaller matrices could be an approximation for the original ratings matrix. In fact, we could minimize the difference between the approximation and the true rating in an optimization process. Once we’ve done that, we would even have an estimate of the rating a user might have given a movie even if they didn’t watch it. And the movie with the highest likelihood of a positive rating, well we can recommend that to a user. So at a high level, that’s exactly what I’m going to do here with MF.\n\n\nMAP and Objective Function\nTime for more formal mathematical definitions. Like I mentioned above, we have \\(n\\) users and \\(m\\) movies as well as a ratings matrix \\(R \\in \\mathbb{R}^{n \\times m}\\) such that \\(R_{i,j}\\) is the rating by user \\(i\\) for movie \\(j\\). The user latent matrix is \\(U \\in \\mathbb{R}^{d \\times n}\\) and movie latent matrix \\(V \\in \\mathbb{R}^{d \\times m}\\) where \\(d\\) is the latent space dimension and a tunable hyperparameter.\nThe author’s of the paper make an assumption on the distribution of \\(R_{i,j}\\), which is the product of latent vectors \\(U_i\\) and \\(V_j\\).\n\\[R_{i,j} \\sim N(U_i^{\\intercal} V_j, \\sigma ^ 2)\\]\nAnd the latent vectors \\(U_i\\) and \\(V_j\\) are also assumed to be normally distributed with zero mean and spherical covariance (dimensionality \\(d\\)). You don’t always hear this term “spherical covariance” but it essentially means all dimensions are independent with same mean and variance (i.e. IID). The variance is also tunable via the \\(\\lambda\\) parameter (defined further below).\n\\[U_i \\sim N(0, \\sigma_u ^ 2 \\mathbf{I})\\]\n\\[V_j \\sim N(0, \\sigma_v ^ 2 \\mathbf{I})\\]\nNow we can define the likelihood function for the entire ratings matrix\n\\[\np(R|U,V,\\sigma^2) = \\prod\\limits_{i}^{n}\\prod\\limits_{j}^{m} p(R_{i,j} | U_i, V_j)\n\\]\nas well as prior probabilities on the latent matrices.\n\\[\np(U|\\sigma_u^2) = \\prod\\limits_{i}^{n} p(U_i) \\; , \\quad p(V|\\sigma_v^2) = \\prod\\limits_j^m p(V_i)\n\\]\nIn a previous post I went through a process to derive a MAP estimate. This is similar. Here we can also approximate the posterior by the product of the likelihood and priors\n\\[\np(U,V|R,\\sigma^2, \\sigma_u^2, \\sigma_v^2) ≈ p(R|U,V,\\sigma^2) \\cdot p(U|\\sigma_u^2) \\cdot p(V|\\sigma_v^2)\n\\]\nand because summations are easier to work with than products, we take the log of the posterior (see paper for details) and maximize that to derive the MAP solution. After some re-arranging for a minimization objective, we would arrive at the following objective function as described in the paper.\n\\[\nE = \\frac{1}{2} \\sum_i^n \\sum_j^m I_{i,j}(R_{i,j} - U_i^\\intercal V_j)^2 + \\frac{\\lambda_u}{2} \\sum_i^n \\Vert U_i \\Vert ^ 2 +  \\frac{\\lambda_v}{2} \\sum_j^m \\Vert V_j \\Vert ^ 2\n\\]\nwhere \\(I_{i,j}\\) is an indicator function if user \\(i\\) has rated movie \\(j\\), and \\(\\lambda_u = \\frac{\\sigma^2}{\\sigma_u^2}\\) and similarly \\(\\lambda_v = \\frac{\\sigma^2}{\\sigma_v^2}\\). The indicator function is an important addition in the objective function as it allows us to ignore the vast number of unrated movies for each user.\nAn interesting aside here is that the priors are essentially adding L2 regularization terms in the objective function. If we were to maximize the likelihood only (i.e. MLE) we wouldn’t have these terms.\nOk, so the paper gets us to this objective function but leaves out any description of a learning algorithm. So now we have to derive update rules in order to learn values for \\(U\\) and \\(V\\).\n\n\nDerivation of Learning Algorithm\nIf you look at the objective function, you’ll see we have two unknown latent matrices \\(U\\) and \\(V\\). Because of this, the objective function is not convex. However, if one of the matrices is fixed, then the other turns into a convex optimization problem. So we can take turns with updates to \\(U\\) and \\(V\\) by performing an alternating coordinate descent. Note that in this context it is also referred to least squares optimization (ALS). These links by Facebook and Google were helpful for confirming this important optimization detail.\nTo find the update rules, we need to derive partial derivatives for \\(U_i\\) and \\(V_j\\) with respect to the objective function. So, time to dive into a little matrix calculus with a derivation for \\(\\frac{\\partial E}{\\partial U_i}\\).\n\\[\n\\frac{\\partial E}{\\partial U_i} = \\frac{\\partial}{\\partial U_i} \\biggl[ \\frac{1}{2}\\sum_i^n \\sum_j^m I_{i,j}(R_{i,j} - U_i^\\intercal V_j)^2 \\biggr] + \\frac{\\partial}{\\partial U_i} \\biggl[ \\frac{\\lambda_u}{2} \\sum_i^n \\Vert U_i \\Vert ^ 2 \\biggr] + \\frac{\\partial}{\\partial U_i} \\biggl[ \\frac{\\lambda_v}{2} \\sum_j^m \\Vert V_j \\Vert ^ 2 \\biggr]\n\\]\n\\[\n\\frac{\\partial E}{\\partial U_i} = \\frac{1}{2}\\sum_i^n \\sum_j^m I_{i,j} \\frac{\\partial}{\\partial U_i} \\biggl[ (R_{i,j} - U_i^\\intercal V_j)^2 \\biggr] + \\frac{\\lambda_u}{2} \\sum_i^n \\frac{\\partial}{\\partial U_i} \\biggl[ \\Vert U_i \\Vert ^ 2 \\biggr]\n\\]\nSummation over \\(i\\) dissappears as there is only one term that is not a constant when differentiating with respect to \\(U_i\\).\n\\[\n\\frac{\\partial E}{\\partial U_i} = \\lambda_u U_i - \\sum_j^m I_{i,j}(R_{i,j} - U_i^\\intercal V_j) V_j\n\\]\nNow we can solve for this in closed form by setting the gradient to zero which gives direct update rules (gradient descent isn’t necessary).\n\\[\n\\lambda_u U_i - \\sum_j^m I_{i,j}(R_{i,j} - U_i^\\intercal V_j) V_j = 0\n\\]\n\\[\n\\lambda_u U_i - \\sum_j^m I_{i,j} R_{i,j} V_j + U_i \\sum_j^m I_{i,j} V_j V_j^\\intercal = 0\n\\]\nAnd solving for \\(U_i\\) we have\n\\[\nU_i \\biggl( \\lambda_u \\mathbf{I} + \\sum_j^m I_{i,j} V_j V_j^\\intercal \\biggr) = \\sum_j^m I_{i,j} R_{i,j} V_j\n\\]\n\\[\nU_i = \\sum_j^m I_{i,j} R_{i,j} V_j \\biggl( \\lambda_u \\mathbf{I} + \\sum_j^m I_{i,j} V_j V_j^\\intercal \\biggr)^{-1}\n\\]\nand it follows similarly for \\(V_j\\)\n\\[\nV_j = \\sum_i^n I_{i,j} R_{i,j} U_i \\biggl(\\lambda_v \\mathbf{I} + \\sum_i^n I_{i,j} U_i U_i^\\intercal \\biggr)^{-1}\n\\]\nThere is another interesting aside here about the priors. If we were to do MLE instead of MAP we’d get a similar update rule except for the \\(\\lambda_v \\mathbf{I}\\) term. If you’ve ever tried to implement your own least squares solution in closed form, you may remember that regression coefficients require inverting the matrix \\((X^\\intercal X)^{-1}\\). Sometimes your data doesn’t behave well and you end up with a non-invertible matrix (because your determinant is zero). The \\(\\lambda_v \\mathbf{I}\\) term is essentially adding some values to the diagonal of the matrix which makes it less likely to have a zero determinant (i.e. more stable).\nOk, so armed with these update rules now we can dive into some code.\n\n\nCode\n\n# !wget https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n!wget https://files.grouplens.org/datasets/movielens/ml-10m.zip\n\n# https://files.grouplens.org/datasets/movielens/ml-latest-small-README.html\n# https://files.grouplens.org/datasets/movielens/ml-10m-README.html\n\n--2024-02-14 04:07:27--  https://files.grouplens.org/datasets/movielens/ml-10m.zip\nResolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\nConnecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 65566137 (63M) [application/zip]\nSaving to: ‘ml-10m.zip’\n\nml-10m.zip          100%[===================&gt;]  62.53M  22.4MB/s    in 2.8s    \n\n2024-02-14 04:07:31 (22.4 MB/s) - ‘ml-10m.zip’ saved [65566137/65566137]\n\n\n\n\n# !unzip ml-latest-small.zip -d ./\n!unzip ml-10m.zip -d ./\n\nArchive:  ml-10m.zip\n   creating: ./ml-10M100K/\n  inflating: ./ml-10M100K/allbut.pl  \n  inflating: ./ml-10M100K/movies.dat  \n  inflating: ./ml-10M100K/ratings.dat  \n  inflating: ./ml-10M100K/README.html  \n  inflating: ./ml-10M100K/split_ratings.sh  \n  inflating: ./ml-10M100K/tags.dat   \n\n\n\nimport pandas as pd\n\n# ratings_df = pd.read_csv(\"./ml-latest-small/ratings.csv\")\nratings_df = pd.read_csv(\n    \"./ml-10M100K/ratings.dat\",\n    sep=\"::\",\n    header=None,\n    names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"]\n)\nprint(ratings_df.shape)\nratings_df.head()\n\nParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators &gt; 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n  ratings_df = pd.read_csv(\n\n\n(10000054, 4)\n\n\n\n  \n    \n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\n\n\n\n\n0\n1\n122\n5.0\n838985046\n\n\n1\n1\n185\n5.0\n838983525\n\n\n2\n1\n231\n5.0\n838983392\n\n\n3\n1\n292\n5.0\n838983421\n\n\n4\n1\n316\n5.0\n838983392\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# ratings are in increments of 0.5\nratings_df.rating.value_counts()\n\n4.0    2875850\n3.0    2356676\n5.0    1544812\n3.5     879764\n2.0     790306\n4.5     585022\n1.0     384180\n2.5     370178\n1.5     118278\n0.5      94988\nName: rating, dtype: int64\n\n\n\n# remove timestamp and create ratings matrix\nratings_df.drop(\"timestamp\", axis=1, inplace=True)\nR = ratings_df.pivot(index='userId', columns='movieId', values='rating')\nprint(R.shape)\nR.head()\n\n(69878, 10677)\n\n\n\n  \n    \n\n\n\n\n\nmovieId\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n65006\n65011\n65025\n65027\n65037\n65088\n65091\n65126\n65130\n65133\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\n3.0\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n5 rows × 10677 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nimport numpy as np\n\nR = R.to_numpy()\n\n\n# sanity check - all movies have rating and all users have rated as expected\nusers_not_rated = np.all(np.isnan(R), axis=1).sum()\nmovies_not_rated = np.all(np.isnan(R), axis=0).sum()\nassert 0 == users_not_rated == movies_not_rated\n\n\n# ratio missing values\nn, m = R.shape\nnans = np.isnan(R).sum()\nnum_ratings = m * n - nans\n\nprint(f\"total number of users: {n}\")\nprint(f\"total number of movies: {m}\")\nprint(f\"total number of movie ratings: {num_ratings}\")\nprint(f\"total number of possible ratings: {n * m}\")\nprint(f\"missing values in ratings matrix: {100 * round(nans / (m * n), 3)}%\")\n\ntotal number of users: 69878\ntotal number of movies: 10677\ntotal number of movie ratings: 10000054\ntotal number of possible ratings: 746087406\nmissing values in ratings matrix: 98.7%\n\n\n\n# indices of ratings\nrating_idxs = np.argwhere(np.isfinite(R))  # 2D array [[i ,j], ...]\n\n# randomly select 15% of rating indices to keep as test\ntrain_slice = np.random.choice(num_ratings, size=int(num_ratings * 0.85), replace=False)\ntest_slice = np.setdiff1d(np.arange(num_ratings), train_slice)\n\ntrain_idxs = rating_idxs[train_slice, :]\ntest_idxs = rating_idxs[test_slice, :]\n\ntrain_idxs.shape, test_idxs.shape\n\n((8500045, 2), (1500009, 2))\n\n\n\n# fill test indices with nan in training data\nR_train = R.copy()\n\nfor i, j in test_idxs:\n    R_train[i, j] = np.nan\n\nassert np.isnan(R_train).sum() == nans + test_idxs.shape[0]\n\n\n# normalizing the training data to zero mean and sigma of 1 for simplicity\nii_train = train_idxs[:, 0].flatten()\njj_train = train_idxs[:, 1].flatten()\n\nR_train_mu = R_train[ii_train, jj_train].mean()\nR_train_sigma = R_train[ii_train, jj_train].std()\nR_train = (R_train - R_train_mu) / R_train_sigma\n\nprint(f\"R train mu: {round(R_train_mu, 2)}\\t R train sigma: {round(R_train_sigma, 2)}\")\n\nR train mu: 3.51     R train sigma: 1.06\n\n\n\n# paper uses:\n#  d=30 with original netflix dataset 100M+ ratings, ~500k users, ~20k movies\n#  adaptive prior w/ spherical covariances lambda U = 0.01, lambda V = 0.001\n#  constrained PMF lambda = 0.002\nd = 30\nlambda_u = 20  # higher better, just some random experimentation here\nlambda_v = 20\nsigma_r = 1  # np.std(R[~np.isnan(R)])\nsigma_u = np.sqrt(sigma_r**2 / lambda_u)  # assume sigma=1 for ratings matrix\nsigma_v = np.sqrt(sigma_r**2 / lambda_v)\n\n# randomly generate values for user and movie latent matrices\n# U can be empty since updated first\nU_mu = np.zeros(d)\nU_cov = np.identity(d) * sigma_u**2  # spherical\nU = np.random.multivariate_normal(U_mu, U_cov, size=n).T\n\nV_mu = np.zeros(d)\nV_cov = np.identity(d) * sigma_v**2\nV = np.random.multivariate_normal(V_mu, V_cov, size=m).T\n\n# D x M/N\nU.shape, V.shape\n\n((30, 69878), (30, 10677))\n\n\n\n# function to update U_i latent vector\n\ndef update_U_i(i, ratings_matrix, user_matrix, movie_matrix, lambda_u):\n    # valid j (i.e. movies this user rated)\n    j_idxs = np.argwhere(np.isfinite(ratings_matrix[i, :])).flatten()\n    # skip if no movies rated in train set\n    if len(j_idxs) == 0:\n        return\n    # running summation\n    sum_rv = np.zeros(d)  # vector: weighted V_j by R_ij\n    sum_vv = np.zeros((d, d))  # matrix: outer product V_j\n    for j in j_idxs:\n        V_j = movie_matrix[:, j]\n        sum_rv += V_j * ratings_matrix[i, j]\n        sum_vv += np.outer(V_j, V_j)\n    user_matrix[:, i] = sum_rv @ np.linalg.inv(sum_vv + np.identity(d) * lambda_u)\n\n\ndef update_V_j(j, ratings_matrix, user_matrix, movie_matrix, lambda_v):\n    # valid i (i.e. users that rated this movie)\n    i_idxs = np.argwhere(np.isfinite(ratings_matrix[:, j])).flatten()\n    # skip if no users rated in train set\n    if len(i_idxs) == 0:\n        return\n    sum_ru = np.zeros(d)  # vector: weighted U_i by R_ij\n    sum_uu = np.zeros((d, d))  # matrix: outer product U_i\n    for i in i_idxs:\n        U_i = user_matrix[:, i]\n        sum_ru += U_i * ratings_matrix[i, j]\n        sum_uu += np.outer(U_i, U_i)\n    movie_matrix[:, j] = sum_ru @ np.linalg.inv(sum_uu + np.identity(d) * lambda_v)\n\n\n# use logistic function since ratings prediction is zero mean (same as paper)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# map sigmoid to range [1, 5] (same as paper)\n# there are 0.5 ratings so this isn't perfect\n\ndef sigmoid_to_rating(x, k=5):\n    return (k - 1) * x + 1\n\n\n# run optimization and report RMSE\n\nepochs = 20\ntrain_rmse = []\nvalid_rmse = []\n\n# lambda_u_hat = lambda_u\n# lambda_v_hat = lambda_v\n\nfor e in range(epochs):\n\n    for i in range(n):\n        update_U_i(i, R_train, U, V, lambda_u)\n\n    for j in range(m):\n        update_V_j(j, R_train, U, V, lambda_v)\n\n    # adaptive...  single sigma -&gt; update lambda_uv = 1 / (sigma_uv ** 2)\n    # if (e + 1) % 10 == 0:\n    #     sigma_u_hat = np.std(U, axis=1)\n    #     sigma_v_hat = np.std(V, axis=1)\n    #     lambda_u_hat = 1 / (sigma_u_hat ** 2)\n    #     lambda_v_hat = 1 / (sigma_v_hat ** 2)\n    # print(f\"sigma U: {sigma_u_hat}\")\n    # print(f\"sigma V: {sigma_v_hat}\")\n    # print(f\"lambda u: {lambda_u_hat}\")\n    # print(f\"lambda v: {lambda_v_hat}\")\n\n    # training metrics\n    sse = 0\n    sse_norm = 0\n    for i, j in train_idxs:  # skips NA\n        r_ij_hat = np.dot(U[:, i], V[:, j])  # pred\n        rating = sigmoid_to_rating(sigmoid(r_ij_hat))\n        # rating_half_inc = round(rating * 2) / 2  # rounds to nearest 0.5\n        sse += (R[i, j] - rating) ** 2\n        sse_norm += (R_train[i, j] - r_ij_hat) ** 2\n    rmse_train = np.sqrt(sse / train_idxs.shape[0])\n    rmse_norm = np.sqrt(sse_norm / train_idxs.shape[0])\n    print(f\"epoch {e + 1}  |  training RMSE: {round(rmse_train, 4)} (normalized: {round(rmse_norm, 4)})\")\n    train_rmse.append(rmse_train)\n\n    # validation RMSE\n    sse = 0\n    for i, j in test_idxs:\n        r_ij_hat = np.dot(U[:, i], V[:, j])\n        rating = sigmoid_to_rating(sigmoid(r_ij_hat))\n        # rating_half_inc = round(rating * 2) / 2  # rounds to nearest 0.5\n        sse += (R[i, j] - rating) ** 2\n    rmse_valid = np.sqrt(sse / test_idxs.shape[0])\n    print(f\"epoch {e + 1}  |  validation RMSE: {round(rmse_valid, 4)}\")\n    valid_rmse.append(rmse_valid)\n\nepoch 1  |  training RMSE: 1.0294 (normalized: 0.8594)\nepoch 1  |  validation RMSE: 1.1035\nepoch 2  |  training RMSE: 0.8958 (normalized: 0.6913)\nepoch 2  |  validation RMSE: 0.9757\nepoch 3  |  training RMSE: 0.8798 (normalized: 0.6679)\nepoch 3  |  validation RMSE: 0.9624\nepoch 4  |  training RMSE: 0.8735 (normalized: 0.6588)\nepoch 4  |  validation RMSE: 0.9573\nepoch 5  |  training RMSE: 0.8704 (normalized: 0.6542)\nepoch 5  |  validation RMSE: 0.9546\nepoch 6  |  training RMSE: 0.8685 (normalized: 0.6515)\nepoch 6  |  validation RMSE: 0.9529\nepoch 7  |  training RMSE: 0.8672 (normalized: 0.6498)\nepoch 7  |  validation RMSE: 0.9517\nepoch 8  |  training RMSE: 0.8663 (normalized: 0.6486)\nepoch 8  |  validation RMSE: 0.9508\nepoch 9  |  training RMSE: 0.8657 (normalized: 0.6477)\nepoch 9  |  validation RMSE: 0.9501\nepoch 10  |  training RMSE: 0.8652 (normalized: 0.6471)\nepoch 10  |  validation RMSE: 0.9496\nepoch 11  |  training RMSE: 0.8648 (normalized: 0.6466)\nepoch 11  |  validation RMSE: 0.9492\nepoch 12  |  training RMSE: 0.8645 (normalized: 0.6462)\nepoch 12  |  validation RMSE: 0.9488\nepoch 13  |  training RMSE: 0.8642 (normalized: 0.6458)\nepoch 13  |  validation RMSE: 0.9485\nepoch 14  |  training RMSE: 0.864 (normalized: 0.6456)\nepoch 14  |  validation RMSE: 0.9483\nepoch 15  |  training RMSE: 0.8639 (normalized: 0.6454)\nepoch 15  |  validation RMSE: 0.9481\nepoch 16  |  training RMSE: 0.8637 (normalized: 0.6452)\nepoch 16  |  validation RMSE: 0.9479\nepoch 17  |  training RMSE: 0.8636 (normalized: 0.6451)\nepoch 17  |  validation RMSE: 0.9477\nepoch 18  |  training RMSE: 0.8635 (normalized: 0.6449)\nepoch 18  |  validation RMSE: 0.9476\nepoch 19  |  training RMSE: 0.8634 (normalized: 0.6448)\nepoch 19  |  validation RMSE: 0.9475\nepoch 20  |  training RMSE: 0.8634 (normalized: 0.6447)\nepoch 20  |  validation RMSE: 0.9474\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\n\nsns.set()\niterations = np.arange(1, epochs + 1, 1)\n\nax = plt.figure().gca()\nplt.xlabel(\"epochs\")\nplt.ylabel(\"RMSE\")\nplt.plot(iterations, train_rmse, \"-o\", color=\"dodgerblue\", label=\"training\")\nplt.plot(iterations, valid_rmse, \"-o\", color=\"salmon\", label=\"validation\")\nplt.legend(loc=\"best\")\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\n\n\n\n\nThe baseline RMSE that Netflix had achieved before the competition was 0.95. This beats that by a small margin but the dataset is different so it’s not an apples to apples comparison. Also, the range of ratings goes down to 0.5 in the MovieLens dataset whereas prediction mapping was consistent with the paper and Netflix dataset which had a range of [1, 5]. This was mostly an exercise in understanding PMF and implementing an algorithm based off an objective function in a research paper. But, if we wanted to squeeze out more performance there are several things we could do:\n\nmap predictions to [0.5, 5]\nuse adaptive covariance (biggest gains)\ntune the lambda regularization coefficients (I started with 1 and only went up/down by an order of magnitude)\nseparate covariances for users and movies\ndiagonal covariance matrix (instead of spherical) to account for different variation between latent features"
  },
  {
    "objectID": "posts/pytorch-fxgraph-qat/pytorch_fxgraph_qat.html",
    "href": "posts/pytorch-fxgraph-qat/pytorch_fxgraph_qat.html",
    "title": "FX Graph Mode Quantization in PyTorch",
    "section": "",
    "text": "Following up on the last post I did about eager mode quantization in pytorch, in this post I’ll be using pytorch’s FX graph mode quantization to quantize the same R-CNN. There are significant differences between the two quantization methods and here I will touch on those as well as demonstrate how to quantize using FX graph mode.\nAt the time of writing this, FX graph mode quantization is still a prototype feature. It’s not quite as mature as eager mode which is currently a beta feature. Although there does appear to be more effort on FX graph mode and it’s even encouraged over eager mode for first time users.\n\nSymbolic Tracing\nFX graph mode quantization requires the network to be symbolically traceable. Under the hood, PyTorch converts the nn.Module network to an alternative format they often refer to as an internal representation (IR). You can imagine that this IR would need to be an accurate and consistent representation of the network regardless of the data flowing through it. So, any parts of the network that have data dependent control flow aren’t supported. Note that the approach taken with FX graph mode can apparently result in hacky modifications to code in the network that would otherwise be unnecessary. Users have complained about it complained about this and it’s something to keep in mind when considering this method.\n\n\nAutomation\nOnce your network is symbolically traceable, you’ll be in for a treat compared to eager mode. The biggest advantages of FX graph mode quantization are:\n\nmodule fusion occurs automatically, something that could otherwise be tedious or error prone depending upon the complexity and size of your network\nfunctionals and torch ops also get converted automagically. In this case that means no need to modify the bottleneck block to use float functional as done in the previous post\nno requirement to insert quant/dequant stubs in the network which means you can avoid creating those additional wrapper classes\n\nSignificant time and effort was invested in doing the above with eager mode. Assuming that getting your network to a symbolically traceable state isn’t more of a time sink, FX graph mode can be a better choice.\n\n\nVerification and Model Preparation\nWith that out of the way, let’s dive into FX graph and QAT. As before, we’ll start with creating the resnet backbone but without having to modify the bottleneck to use float functional operator.\n\nimport torch\nfrom torchvision.models.resnet import ResNet, Bottleneck, ResNet101_Weights\n\n\ndef resnet_101():\n    resnet = ResNet(block=Bottleneck, layers=[3, 4, 23, 3])\n    resnet.load_state_dict(ResNet101_Weights.DEFAULT.get_state_dict(progress=True))\n    return resnet\n\n\nresnet = resnet_101()\n\nAt this point the resnet is fully traceable. Tracing it with an example input will return a ScriptModule which can be used to get a representation of graph’s forward method.\n\ntraced_module = torch.jit.trace(resnet, torch.rand(1, 3, 200, 200))\nprint(traced_module.code)\n\ndef forward(self,\n    x: Tensor) -&gt; Tensor:\n  fc = self.fc\n  avgpool = self.avgpool\n  layer4 = self.layer4\n  layer3 = self.layer3\n  layer2 = self.layer2\n  layer1 = self.layer1\n  maxpool = self.maxpool\n  relu = self.relu\n  bn1 = self.bn1\n  conv1 = self.conv1\n  _0 = (relu).forward((bn1).forward((conv1).forward(x, ), ), )\n  _1 = (layer1).forward((maxpool).forward(_0, ), )\n  _2 = (layer3).forward((layer2).forward(_1, ), )\n  _3 = (avgpool).forward((layer4).forward(_2, ), )\n  input = torch.flatten(_3, 1)\n  return (fc).forward(input, )\n\n\n\nJust as was done during eager mode preparation, the next step is to use torchvision’s helper method IntermediateLayerGetter to extract layer outputs from the resnet to feed to the FPN.\n\nfrom torchvision.models._utils import IntermediateLayerGetter\n\n\nreturned_layers = [1, 2, 3, 4]\nreturn_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}\nresnet_layers = IntermediateLayerGetter(resnet, return_layers=return_layers)\n\nAs we saw before, the result of the layer getter is a module dict which returns an ordered dict from its forward method. If we attempt to trace using strict mode, JIT will complain because of the mutable output type:\nAttributeError\n...\nAttributeError: expected a dictionary of (method_name, input) pairs\nThis can be ignored if we “are sure that the container you are using in your problem is a constant structure and does not get used as control flow (if, for) conditions.” Since we know the output won’t change, we can safely ignore this and set strict=False. Note that this isn’t necessary for QAT preparation but it’s helpful to know apriori if the parts of the model that we intend to quantize are indeed traceable.\n\ntraced_module = torch.jit.trace(resnet_layers, torch.rand(1, 3, 200, 200), strict=False)\nprint(traced_module.code)\n\ndef forward(self,\n    x: Tensor) -&gt; Dict[str, Tensor]:\n  layer4 = self.layer4\n  layer3 = self.layer3\n  layer2 = self.layer2\n  layer1 = self.layer1\n  maxpool = self.maxpool\n  relu = self.relu\n  bn1 = self.bn1\n  conv1 = self.conv1\n  _0 = (relu).forward((bn1).forward((conv1).forward(x, ), ), )\n  _1 = (layer1).forward((maxpool).forward(_0, ), )\n  _2 = (layer2).forward(_1, )\n  _3 = (layer3).forward(_2, )\n  _4 = {\"0\": _1, \"1\": _2, \"2\": _3, \"3\": (layer4).forward(_3, )}\n  return _4\n\n\n\nNow to create the backbone with FPN, however this time without any modifications. After this, the module can be traced. Because the output is a mutable type (ordered dict) and it’s structure will not change, strict mode needs to be set to false. If you don’t, there is a slightly different error (shown below) but the reason is the same.\nRuntimeError: Encountering a dict at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior.\n\nfrom torchvision.models.detection.backbone_utils import BackboneWithFPN\n\n\nin_channels_stage2 = resnet.inplanes // 8\nin_channels_list = [in_channels_stage2 * 2 ** (i - 1) for i in returned_layers]\nout_channels = 256\nreturned_layers = [1, 2, 3, 4]\nreturn_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}\n\nbb_fpn = BackboneWithFPN(\n    backbone=resnet,\n    return_layers=return_layers,\n    in_channels_list=in_channels_list,\n    out_channels=out_channels\n)\n\n\ntraced_module = torch.jit.trace(bb_fpn, torch.rand(1, 3, 200, 200), strict=False)\nprint(traced_module.code)\n\ndef forward(self,\n    x: Tensor) -&gt; Dict[str, Tensor]:\n  fpn = self.fpn\n  body = self.body\n  _0, _1, _2, _3, = (body).forward(x, )\n  _4, _5, _6, _7, _8, = (fpn).forward(_0, _1, _2, _3, )\n  _9 = {\"0\": _4, \"1\": _5, \"2\": _6, \"3\": _7, \"pool\": _8}\n  return _9\n\n\n\nWe’ve verified the backbone with FPN is indeed traceable. So now we can create the R-CNN and prepare the model for QAT. During preparation, FX graph mode will automatically insert observers and fuse modules. The returned model is also now graph module and looks something like:\nGraphModule(\n  (activation_post_process_0): HistogramObserver(min_val=inf, max_val=-inf)\n  (body): Module(\n    (conv1): ConvBnReLU2d(\n      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n    )\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Module(\n      (0): Module(\n        (conv1): ConvBnReLU2d(\n          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n        )\n        (conv2): ConvBnReLU2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n          ...\nNote that preparation now requires an example input to determine the output types. As before, I’ll freeze the first layer along with batch norm stats.\n\n%%capture\nimport re\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN\nfrom torch.ao.quantization import quantize_fx\nfrom torch.ao.quantization.qconfig_mapping import get_default_qconfig_mapping\n\n\nquant_rcnn = FasterRCNN(bb_fpn, num_classes=2)\n\nexample_input = torch.randn(1, 3, 200, 200)\nquant_rcnn.train()\nqconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\nquant_rcnn.backbone = quantize_fx.prepare_qat_fx(quant_rcnn.backbone, qconfig_mapping, example_input)\n\nquant_rcnn = quant_rcnn.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n\nfor name, parameter in quant_rcnn.named_parameters():\n    if re.search(r\"body.conv1\", name) or re.search(r\"body.layer1\", name):\n        parameter.requires_grad = False\n\n\n\nQAT and Model Conversion\nAs in the previous post, I’ll use the PennFudan dataset from the Torchvision object detection finetuning tutorial.\n\nimport os\n\nfrom torchvision.io import read_image\nfrom torchvision.ops.boxes import masks_to_boxes\nfrom torchvision import tv_tensors\nfrom torchvision.transforms.v2 import functional as F\nfrom torchvision.transforms import v2 as T\n\n\nclass PennFudanDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = read_image(img_path)\n        mask = read_image(mask_path)\n        # instances are encoded as different colors\n        obj_ids = torch.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n        num_objs = len(obj_ids)\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n\n        # get bounding box coordinates for each mask\n        boxes = masks_to_boxes(masks)\n\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n\n        image_id = idx\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        # Wrap sample and targets into torchvision tv_tensors:\n        img = tv_tensors.Image(img)\n\n        target = {}\n        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n        target[\"masks\"] = tv_tensors.Mask(masks)\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)\n\n\ndef get_transform(train):\n    transforms = []\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    transforms.append(T.ToDtype(torch.float, scale=True))\n    transforms.append(T.ToPureTensor())\n    return T.Compose(transforms)\n\n\n%%capture\n\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")\n\n!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n!unzip PennFudanPed.zip -d ./\n\n\nimport utils\nfrom engine import train_one_epoch, evaluate\n\n\n# train on the GPU or on the CPU, if a GPU is not available\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n# use our dataset and defined transformations\ndataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\ndataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n\n# split the dataset in train and test set\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=1,\n    shuffle=True,\n    num_workers=1,\n    collate_fn=utils.collate_fn\n)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test,\n    batch_size=1,\n    shuffle=False,\n    num_workers=1,\n    collate_fn=utils.collate_fn\n)\n\n\n# move model to the right device\nquant_rcnn.to(device)\n\n# construct an optimizer\nparams = [p for p in quant_rcnn.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(\n    params,\n    lr=0.005,\n    momentum=0.9,\n    weight_decay=0.0005\n)\n\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=3,\n    gamma=0.1\n)\n\n# let's train it for 10 epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(quant_rcnn, optimizer, data_loader, device, epoch, print_freq=20)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(quant_rcnn, data_loader_test, device=device)\n\nEpoch: [0]  [  0/120]  eta: 0:00:51  lr: 0.000047  loss: 1.4356 (1.4356)  loss_classifier: 0.6798 (0.6798)  loss_box_reg: 0.0042 (0.0042)  loss_objectness: 0.6744 (0.6744)  loss_rpn_box_reg: 0.0773 (0.0773)  time: 0.4267  data: 0.1246  max mem: 4118\nEpoch: [0]  [ 20/120]  eta: 0:00:34  lr: 0.000886  loss: 0.7461 (0.8032)  loss_classifier: 0.1333 (0.2406)  loss_box_reg: 0.0412 (0.0626)  loss_objectness: 0.5052 (0.4778)  loss_rpn_box_reg: 0.0116 (0.0222)  time: 0.3406  data: 0.0036  max mem: 4118\nEpoch: [0]  [ 40/120]  eta: 0:00:27  lr: 0.001726  loss: 0.3244 (0.5867)  loss_classifier: 0.1122 (0.1902)  loss_box_reg: 0.0930 (0.0879)  loss_objectness: 0.0811 (0.2863)  loss_rpn_box_reg: 0.0191 (0.0224)  time: 0.3351  data: 0.0035  max mem: 4118\nEpoch: [0]  [ 60/120]  eta: 0:00:20  lr: 0.002565  loss: 0.3139 (0.5107)  loss_classifier: 0.1092 (0.1722)  loss_box_reg: 0.1229 (0.1013)  loss_objectness: 0.0516 (0.2141)  loss_rpn_box_reg: 0.0137 (0.0231)  time: 0.3380  data: 0.0034  max mem: 4118\nEpoch: [0]  [ 80/120]  eta: 0:00:13  lr: 0.003405  loss: 0.2165 (0.4596)  loss_classifier: 0.0779 (0.1560)  loss_box_reg: 0.0868 (0.1079)  loss_objectness: 0.0359 (0.1730)  loss_rpn_box_reg: 0.0122 (0.0226)  time: 0.3359  data: 0.0038  max mem: 4118\nEpoch: [0]  [100/120]  eta: 0:00:06  lr: 0.004244  loss: 0.1732 (0.4241)  loss_classifier: 0.0546 (0.1431)  loss_box_reg: 0.0924 (0.1116)  loss_objectness: 0.0328 (0.1473)  loss_rpn_box_reg: 0.0105 (0.0221)  time: 0.3306  data: 0.0034  max mem: 4118\nEpoch: [0]  [119/120]  eta: 0:00:00  lr: 0.005000  loss: 0.1507 (0.4001)  loss_classifier: 0.0599 (0.1343)  loss_box_reg: 0.0839 (0.1141)  loss_objectness: 0.0171 (0.1304)  loss_rpn_box_reg: 0.0104 (0.0213)  time: 0.3301  data: 0.0033  max mem: 4118\nEpoch: [0] Total time: 0:00:40 (0.3369 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:18  model_time: 0.2536 (0.2536)  evaluator_time: 0.0039 (0.0039)  time: 0.3654  data: 0.1060  max mem: 4118\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2215 (0.2262)  evaluator_time: 0.0017 (0.0025)  time: 0.2302  data: 0.0030  max mem: 4118\nTest: Total time: 0:00:11 (0.2365 s / it)\nAveraged stats: model_time: 0.2215 (0.2262)  evaluator_time: 0.0017 (0.0025)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.251\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.661\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.067\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.024\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.274\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.116\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.387\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.400\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.022\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.438\nEpoch: [1]  [  0/120]  eta: 0:00:54  lr: 0.005000  loss: 0.1070 (0.1070)  loss_classifier: 0.0222 (0.0222)  loss_box_reg: 0.0459 (0.0459)  loss_objectness: 0.0226 (0.0226)  loss_rpn_box_reg: 0.0164 (0.0164)  time: 0.4527  data: 0.0874  max mem: 4118\nEpoch: [1]  [ 20/120]  eta: 0:00:34  lr: 0.005000  loss: 0.2121 (0.2348)  loss_classifier: 0.0590 (0.0804)  loss_box_reg: 0.0915 (0.1130)  loss_objectness: 0.0149 (0.0175)  loss_rpn_box_reg: 0.0153 (0.0238)  time: 0.3356  data: 0.0033  max mem: 4118\nEpoch: [1]  [ 40/120]  eta: 0:00:27  lr: 0.005000  loss: 0.2197 (0.2434)  loss_classifier: 0.0628 (0.0799)  loss_box_reg: 0.1179 (0.1204)  loss_objectness: 0.0132 (0.0157)  loss_rpn_box_reg: 0.0287 (0.0274)  time: 0.3409  data: 0.0036  max mem: 4118\nEpoch: [1]  [ 60/120]  eta: 0:00:20  lr: 0.005000  loss: 0.2105 (0.2374)  loss_classifier: 0.0591 (0.0756)  loss_box_reg: 0.1001 (0.1178)  loss_objectness: 0.0117 (0.0164)  loss_rpn_box_reg: 0.0176 (0.0276)  time: 0.3342  data: 0.0034  max mem: 4118\nEpoch: [1]  [ 80/120]  eta: 0:00:13  lr: 0.005000  loss: 0.1133 (0.2263)  loss_classifier: 0.0351 (0.0707)  loss_box_reg: 0.0642 (0.1158)  loss_objectness: 0.0089 (0.0152)  loss_rpn_box_reg: 0.0112 (0.0246)  time: 0.3325  data: 0.0034  max mem: 4118\nEpoch: [1]  [100/120]  eta: 0:00:06  lr: 0.005000  loss: 0.1559 (0.2291)  loss_classifier: 0.0573 (0.0720)  loss_box_reg: 0.0860 (0.1200)  loss_objectness: 0.0060 (0.0138)  loss_rpn_box_reg: 0.0135 (0.0233)  time: 0.3367  data: 0.0034  max mem: 4118\nEpoch: [1]  [119/120]  eta: 0:00:00  lr: 0.005000  loss: 0.1403 (0.2247)  loss_classifier: 0.0440 (0.0706)  loss_box_reg: 0.0860 (0.1195)  loss_objectness: 0.0046 (0.0124)  loss_rpn_box_reg: 0.0103 (0.0221)  time: 0.3353  data: 0.0034  max mem: 4118\nEpoch: [1] Total time: 0:00:40 (0.3375 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:19  model_time: 0.2691 (0.2691)  evaluator_time: 0.0068 (0.0068)  time: 0.3851  data: 0.1073  max mem: 4118\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2217 (0.2271)  evaluator_time: 0.0014 (0.0024)  time: 0.2323  data: 0.0040  max mem: 4118\nTest: Total time: 0:00:11 (0.2382 s / it)\nAveraged stats: model_time: 0.2217 (0.2271)  evaluator_time: 0.0014 (0.0024)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.452\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.912\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.297\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.006\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.186\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.485\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.223\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.556\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.565\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.250\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.378\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.586\nEpoch: [2]  [  0/120]  eta: 0:00:58  lr: 0.005000  loss: 0.3480 (0.3480)  loss_classifier: 0.0969 (0.0969)  loss_box_reg: 0.2045 (0.2045)  loss_objectness: 0.0084 (0.0084)  loss_rpn_box_reg: 0.0382 (0.0382)  time: 0.4848  data: 0.0990  max mem: 4118\nEpoch: [2]  [ 20/120]  eta: 0:00:34  lr: 0.005000  loss: 0.1341 (0.1999)  loss_classifier: 0.0486 (0.0573)  loss_box_reg: 0.0765 (0.1212)  loss_objectness: 0.0035 (0.0042)  loss_rpn_box_reg: 0.0100 (0.0172)  time: 0.3420  data: 0.0034  max mem: 4118\nEpoch: [2]  [ 40/120]  eta: 0:00:27  lr: 0.005000  loss: 0.0984 (0.1881)  loss_classifier: 0.0389 (0.0531)  loss_box_reg: 0.0616 (0.1145)  loss_objectness: 0.0021 (0.0049)  loss_rpn_box_reg: 0.0110 (0.0156)  time: 0.3282  data: 0.0035  max mem: 4118\nEpoch: [2]  [ 60/120]  eta: 0:00:20  lr: 0.005000  loss: 0.1335 (0.1769)  loss_classifier: 0.0371 (0.0497)  loss_box_reg: 0.0836 (0.1082)  loss_objectness: 0.0018 (0.0047)  loss_rpn_box_reg: 0.0074 (0.0143)  time: 0.3309  data: 0.0036  max mem: 4118\nEpoch: [2]  [ 80/120]  eta: 0:00:13  lr: 0.005000  loss: 0.1453 (0.1743)  loss_classifier: 0.0383 (0.0483)  loss_box_reg: 0.0852 (0.1070)  loss_objectness: 0.0028 (0.0044)  loss_rpn_box_reg: 0.0136 (0.0147)  time: 0.3397  data: 0.0034  max mem: 4118\nEpoch: [2]  [100/120]  eta: 0:00:06  lr: 0.005000  loss: 0.1703 (0.1798)  loss_classifier: 0.0410 (0.0487)  loss_box_reg: 0.1154 (0.1093)  loss_objectness: 0.0050 (0.0048)  loss_rpn_box_reg: 0.0190 (0.0170)  time: 0.3397  data: 0.0036  max mem: 4118\nEpoch: [2]  [119/120]  eta: 0:00:00  lr: 0.005000  loss: 0.1078 (0.1789)  loss_classifier: 0.0296 (0.0482)  loss_box_reg: 0.0621 (0.1084)  loss_objectness: 0.0037 (0.0050)  loss_rpn_box_reg: 0.0111 (0.0174)  time: 0.3413  data: 0.0035  max mem: 4118\nEpoch: [2] Total time: 0:00:40 (0.3392 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:20  model_time: 0.2938 (0.2938)  evaluator_time: 0.0044 (0.0044)  time: 0.4081  data: 0.1080  max mem: 4118\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2222 (0.2301)  evaluator_time: 0.0012 (0.0019)  time: 0.2304  data: 0.0031  max mem: 4118\nTest: Total time: 0:00:11 (0.2398 s / it)\nAveraged stats: model_time: 0.2222 (0.2301)  evaluator_time: 0.0012 (0.0019)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.459\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.935\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.320\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.050\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.225\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.481\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.236\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.548\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.563\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.050\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.511\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.576\nEpoch: [3]  [  0/120]  eta: 0:00:53  lr: 0.000500  loss: 0.1714 (0.1714)  loss_classifier: 0.0475 (0.0475)  loss_box_reg: 0.1006 (0.1006)  loss_objectness: 0.0148 (0.0148)  loss_rpn_box_reg: 0.0085 (0.0085)  time: 0.4497  data: 0.0858  max mem: 4118\nEpoch: [3]  [ 20/120]  eta: 0:00:33  lr: 0.000500  loss: 0.1223 (0.1611)  loss_classifier: 0.0343 (0.0433)  loss_box_reg: 0.0736 (0.0934)  loss_objectness: 0.0054 (0.0082)  loss_rpn_box_reg: 0.0094 (0.0161)  time: 0.3345  data: 0.0034  max mem: 4118\nEpoch: [3]  [ 40/120]  eta: 0:00:27  lr: 0.000500  loss: 0.1039 (0.1498)  loss_classifier: 0.0250 (0.0406)  loss_box_reg: 0.0655 (0.0887)  loss_objectness: 0.0042 (0.0069)  loss_rpn_box_reg: 0.0079 (0.0136)  time: 0.3398  data: 0.0043  max mem: 4118\nEpoch: [3]  [ 60/120]  eta: 0:00:20  lr: 0.000500  loss: 0.1237 (0.1445)  loss_classifier: 0.0331 (0.0395)  loss_box_reg: 0.0722 (0.0850)  loss_objectness: 0.0045 (0.0065)  loss_rpn_box_reg: 0.0057 (0.0135)  time: 0.3353  data: 0.0036  max mem: 4118\nEpoch: [3]  [ 80/120]  eta: 0:00:13  lr: 0.000500  loss: 0.1128 (0.1440)  loss_classifier: 0.0337 (0.0392)  loss_box_reg: 0.0734 (0.0857)  loss_objectness: 0.0039 (0.0061)  loss_rpn_box_reg: 0.0069 (0.0130)  time: 0.3368  data: 0.0034  max mem: 4118\nEpoch: [3]  [100/120]  eta: 0:00:06  lr: 0.000500  loss: 0.1441 (0.1482)  loss_classifier: 0.0359 (0.0405)  loss_box_reg: 0.0901 (0.0893)  loss_objectness: 0.0033 (0.0058)  loss_rpn_box_reg: 0.0099 (0.0126)  time: 0.3267  data: 0.0035  max mem: 4118\nEpoch: [3]  [119/120]  eta: 0:00:00  lr: 0.000500  loss: 0.1051 (0.1469)  loss_classifier: 0.0276 (0.0398)  loss_box_reg: 0.0678 (0.0895)  loss_objectness: 0.0020 (0.0054)  loss_rpn_box_reg: 0.0067 (0.0122)  time: 0.3282  data: 0.0040  max mem: 4118\nEpoch: [3] Total time: 0:00:40 (0.3354 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:18  model_time: 0.2583 (0.2583)  evaluator_time: 0.0039 (0.0039)  time: 0.3704  data: 0.1063  max mem: 4118\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2236 (0.2284)  evaluator_time: 0.0012 (0.0017)  time: 0.2295  data: 0.0030  max mem: 4118\nTest: Total time: 0:00:11 (0.2384 s / it)\nAveraged stats: model_time: 0.2236 (0.2284)  evaluator_time: 0.0012 (0.0017)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.538\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.950\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.583\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.327\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.562\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.269\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.604\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.606\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.511\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.625\nEpoch: [4]  [  0/120]  eta: 0:00:57  lr: 0.000500  loss: 0.0602 (0.0602)  loss_classifier: 0.0157 (0.0157)  loss_box_reg: 0.0431 (0.0431)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0012 (0.0012)  time: 0.4826  data: 0.0982  max mem: 4118\nEpoch: [4]  [ 20/120]  eta: 0:00:34  lr: 0.000500  loss: 0.1051 (0.1215)  loss_classifier: 0.0246 (0.0325)  loss_box_reg: 0.0664 (0.0777)  loss_objectness: 0.0020 (0.0024)  loss_rpn_box_reg: 0.0048 (0.0089)  time: 0.3355  data: 0.0034  max mem: 4118\nEpoch: [4]  [ 40/120]  eta: 0:00:27  lr: 0.000500  loss: 0.1095 (0.1227)  loss_classifier: 0.0317 (0.0330)  loss_box_reg: 0.0637 (0.0783)  loss_objectness: 0.0026 (0.0028)  loss_rpn_box_reg: 0.0056 (0.0085)  time: 0.3335  data: 0.0035  max mem: 4118\nEpoch: [4]  [ 60/120]  eta: 0:00:20  lr: 0.000500  loss: 0.1261 (0.1309)  loss_classifier: 0.0346 (0.0349)  loss_box_reg: 0.0821 (0.0832)  loss_objectness: 0.0018 (0.0029)  loss_rpn_box_reg: 0.0095 (0.0098)  time: 0.3379  data: 0.0034  max mem: 4118\nEpoch: [4]  [ 80/120]  eta: 0:00:13  lr: 0.000500  loss: 0.1560 (0.1396)  loss_classifier: 0.0446 (0.0380)  loss_box_reg: 0.0914 (0.0881)  loss_objectness: 0.0025 (0.0034)  loss_rpn_box_reg: 0.0076 (0.0101)  time: 0.3355  data: 0.0035  max mem: 4118\nEpoch: [4]  [100/120]  eta: 0:00:06  lr: 0.000500  loss: 0.1295 (0.1440)  loss_classifier: 0.0324 (0.0394)  loss_box_reg: 0.0877 (0.0905)  loss_objectness: 0.0026 (0.0038)  loss_rpn_box_reg: 0.0076 (0.0103)  time: 0.3268  data: 0.0034  max mem: 4118\nEpoch: [4]  [119/120]  eta: 0:00:00  lr: 0.000500  loss: 0.1028 (0.1440)  loss_classifier: 0.0250 (0.0395)  loss_box_reg: 0.0653 (0.0907)  loss_objectness: 0.0020 (0.0036)  loss_rpn_box_reg: 0.0063 (0.0103)  time: 0.3404  data: 0.0035  max mem: 4118\nEpoch: [4] Total time: 0:00:40 (0.3367 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:18  model_time: 0.2581 (0.2581)  evaluator_time: 0.0042 (0.0042)  time: 0.3692  data: 0.1050  max mem: 4118\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2333 (0.2339)  evaluator_time: 0.0013 (0.0018)  time: 0.2418  data: 0.0033  max mem: 4118\nTest: Total time: 0:00:12 (0.2438 s / it)\nAveraged stats: model_time: 0.2333 (0.2339)  evaluator_time: 0.0013 (0.0018)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.583\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.971\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.633\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.025\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.325\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.609\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.286\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.646\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.649\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.511\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.670\nEpoch: [5]  [  0/120]  eta: 0:00:56  lr: 0.000500  loss: 0.0435 (0.0435)  loss_classifier: 0.0182 (0.0182)  loss_box_reg: 0.0232 (0.0232)  loss_objectness: 0.0003 (0.0003)  loss_rpn_box_reg: 0.0019 (0.0019)  time: 0.4734  data: 0.0969  max mem: 4118\nEpoch: [5]  [ 20/120]  eta: 0:00:35  lr: 0.000500  loss: 0.1120 (0.1410)  loss_classifier: 0.0310 (0.0395)  loss_box_reg: 0.0774 (0.0894)  loss_objectness: 0.0016 (0.0022)  loss_rpn_box_reg: 0.0084 (0.0098)  time: 0.3440  data: 0.0039  max mem: 4118\nEpoch: [5]  [ 40/120]  eta: 0:00:27  lr: 0.000500  loss: 0.1002 (0.1318)  loss_classifier: 0.0299 (0.0363)  loss_box_reg: 0.0614 (0.0834)  loss_objectness: 0.0017 (0.0030)  loss_rpn_box_reg: 0.0032 (0.0092)  time: 0.3488  data: 0.0035  max mem: 4118\nEpoch: [5]  [ 60/120]  eta: 0:00:20  lr: 0.000500  loss: 0.1339 (0.1381)  loss_classifier: 0.0293 (0.0373)  loss_box_reg: 0.0939 (0.0883)  loss_objectness: 0.0016 (0.0031)  loss_rpn_box_reg: 0.0062 (0.0094)  time: 0.3475  data: 0.0038  max mem: 4118\nEpoch: [5]  [ 80/120]  eta: 0:00:13  lr: 0.000500  loss: 0.1281 (0.1397)  loss_classifier: 0.0369 (0.0377)  loss_box_reg: 0.0801 (0.0898)  loss_objectness: 0.0013 (0.0029)  loss_rpn_box_reg: 0.0071 (0.0093)  time: 0.3423  data: 0.0034  max mem: 4118\nEpoch: [5]  [100/120]  eta: 0:00:06  lr: 0.000500  loss: 0.1085 (0.1398)  loss_classifier: 0.0381 (0.0377)  loss_box_reg: 0.0637 (0.0892)  loss_objectness: 0.0011 (0.0029)  loss_rpn_box_reg: 0.0064 (0.0100)  time: 0.3382  data: 0.0033  max mem: 4118\nEpoch: [5]  [119/120]  eta: 0:00:00  lr: 0.000500  loss: 0.1180 (0.1382)  loss_classifier: 0.0275 (0.0378)  loss_box_reg: 0.0697 (0.0880)  loss_objectness: 0.0012 (0.0027)  loss_rpn_box_reg: 0.0055 (0.0096)  time: 0.3403  data: 0.0034  max mem: 4118\nEpoch: [5] Total time: 0:00:41 (0.3456 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:18  model_time: 0.2561 (0.2561)  evaluator_time: 0.0037 (0.0037)  time: 0.3699  data: 0.1082  max mem: 4118\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2226 (0.2282)  evaluator_time: 0.0011 (0.0016)  time: 0.2313  data: 0.0031  max mem: 4118\nTest: Total time: 0:00:11 (0.2378 s / it)\nAveraged stats: model_time: 0.2226 (0.2282)  evaluator_time: 0.0011 (0.0016)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.585\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.955\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.682\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.282\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.615\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.285\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.645\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.651\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.500\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.675\nEpoch: [6]  [  0/120]  eta: 0:00:54  lr: 0.000050  loss: 0.0497 (0.0497)  loss_classifier: 0.0101 (0.0101)  loss_box_reg: 0.0377 (0.0377)  loss_objectness: 0.0006 (0.0006)  loss_rpn_box_reg: 0.0013 (0.0013)  time: 0.4527  data: 0.0854  max mem: 4118\nEpoch: [6]  [ 20/120]  eta: 0:00:35  lr: 0.000050  loss: 0.1043 (0.1187)  loss_classifier: 0.0288 (0.0338)  loss_box_reg: 0.0714 (0.0760)  loss_objectness: 0.0013 (0.0016)  loss_rpn_box_reg: 0.0054 (0.0073)  time: 0.3507  data: 0.0036  max mem: 4118\nEpoch: [6]  [ 40/120]  eta: 0:00:27  lr: 0.000050  loss: 0.1110 (0.1172)  loss_classifier: 0.0277 (0.0320)  loss_box_reg: 0.0755 (0.0755)  loss_objectness: 0.0018 (0.0021)  loss_rpn_box_reg: 0.0057 (0.0076)  time: 0.3412  data: 0.0037  max mem: 4118\nEpoch: [6]  [ 60/120]  eta: 0:00:20  lr: 0.000050  loss: 0.1067 (0.1196)  loss_classifier: 0.0275 (0.0325)  loss_box_reg: 0.0619 (0.0772)  loss_objectness: 0.0012 (0.0025)  loss_rpn_box_reg: 0.0031 (0.0074)  time: 0.3431  data: 0.0041  max mem: 4118\nEpoch: [6]  [ 80/120]  eta: 0:00:13  lr: 0.000050  loss: 0.1232 (0.1236)  loss_classifier: 0.0333 (0.0336)  loss_box_reg: 0.0801 (0.0791)  loss_objectness: 0.0015 (0.0026)  loss_rpn_box_reg: 0.0056 (0.0083)  time: 0.3325  data: 0.0034  max mem: 4118\nEpoch: [6]  [100/120]  eta: 0:00:06  lr: 0.000050  loss: 0.1273 (0.1281)  loss_classifier: 0.0300 (0.0351)  loss_box_reg: 0.0806 (0.0819)  loss_objectness: 0.0015 (0.0029)  loss_rpn_box_reg: 0.0062 (0.0082)  time: 0.3556  data: 0.0042  max mem: 4118\nEpoch: [6]  [119/120]  eta: 0:00:00  lr: 0.000050  loss: 0.0951 (0.1297)  loss_classifier: 0.0295 (0.0357)  loss_box_reg: 0.0680 (0.0830)  loss_objectness: 0.0015 (0.0027)  loss_rpn_box_reg: 0.0039 (0.0083)  time: 0.3447  data: 0.0040  max mem: 4118\nEpoch: [6] Total time: 0:00:41 (0.3464 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:18  model_time: 0.2564 (0.2564)  evaluator_time: 0.0036 (0.0036)  time: 0.3700  data: 0.1081  max mem: 4118\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2383 (0.2413)  evaluator_time: 0.0013 (0.0017)  time: 0.2628  data: 0.0039  max mem: 4118\nTest: Total time: 0:00:12 (0.2515 s / it)\nAveraged stats: model_time: 0.2383 (0.2413)  evaluator_time: 0.0013 (0.0017)\nAccumulating evaluation results...\nDONE (t=0.01s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.602\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.961\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.693\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.301\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.634\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.288\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.662\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.667\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.478\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.695\nEpoch: [7]  [  0/120]  eta: 0:00:54  lr: 0.000050  loss: 0.0397 (0.0397)  loss_classifier: 0.0133 (0.0133)  loss_box_reg: 0.0253 (0.0253)  loss_objectness: 0.0010 (0.0010)  loss_rpn_box_reg: 0.0001 (0.0001)  time: 0.4533  data: 0.0919  max mem: 4118\nEpoch: [7]  [ 20/120]  eta: 0:00:35  lr: 0.000050  loss: 0.1022 (0.1146)  loss_classifier: 0.0279 (0.0318)  loss_box_reg: 0.0659 (0.0724)  loss_objectness: 0.0010 (0.0017)  loss_rpn_box_reg: 0.0057 (0.0087)  time: 0.3465  data: 0.0035  max mem: 4118\nEpoch: [7]  [ 40/120]  eta: 0:00:27  lr: 0.000050  loss: 0.1042 (0.1220)  loss_classifier: 0.0261 (0.0332)  loss_box_reg: 0.0658 (0.0781)  loss_objectness: 0.0013 (0.0020)  loss_rpn_box_reg: 0.0059 (0.0088)  time: 0.3374  data: 0.0036  max mem: 4118\nEpoch: [7]  [ 60/120]  eta: 0:00:20  lr: 0.000050  loss: 0.1504 (0.1340)  loss_classifier: 0.0459 (0.0371)  loss_box_reg: 0.0936 (0.0850)  loss_objectness: 0.0019 (0.0022)  loss_rpn_box_reg: 0.0067 (0.0097)  time: 0.3477  data: 0.0037  max mem: 4118\nEpoch: [7]  [ 80/120]  eta: 0:00:13  lr: 0.000050  loss: 0.0815 (0.1321)  loss_classifier: 0.0239 (0.0366)  loss_box_reg: 0.0501 (0.0832)  loss_objectness: 0.0021 (0.0031)  loss_rpn_box_reg: 0.0062 (0.0091)  time: 0.3354  data: 0.0037  max mem: 4118\nEpoch: [7]  [100/120]  eta: 0:00:06  lr: 0.000050  loss: 0.0972 (0.1313)  loss_classifier: 0.0236 (0.0359)  loss_box_reg: 0.0704 (0.0836)  loss_objectness: 0.0016 (0.0029)  loss_rpn_box_reg: 0.0051 (0.0089)  time: 0.3420  data: 0.0037  max mem: 4118\nEpoch: [7]  [119/120]  eta: 0:00:00  lr: 0.000050  loss: 0.0925 (0.1308)  loss_classifier: 0.0259 (0.0360)  loss_box_reg: 0.0534 (0.0835)  loss_objectness: 0.0010 (0.0026)  loss_rpn_box_reg: 0.0040 (0.0087)  time: 0.3312  data: 0.0035  max mem: 4118\nEpoch: [7] Total time: 0:00:41 (0.3420 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:19  model_time: 0.2790 (0.2790)  evaluator_time: 0.0040 (0.0040)  time: 0.3950  data: 0.1099  max mem: 4118\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2328 (0.2311)  evaluator_time: 0.0012 (0.0017)  time: 0.2403  data: 0.0032  max mem: 4118\nTest: Total time: 0:00:12 (0.2411 s / it)\nAveraged stats: model_time: 0.2328 (0.2311)  evaluator_time: 0.0012 (0.0017)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.606\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.965\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.691\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.309\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.637\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.290\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.660\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.664\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.478\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.692\nEpoch: [8]  [  0/120]  eta: 0:01:06  lr: 0.000050  loss: 0.3302 (0.3302)  loss_classifier: 0.0762 (0.0762)  loss_box_reg: 0.2121 (0.2121)  loss_objectness: 0.0069 (0.0069)  loss_rpn_box_reg: 0.0350 (0.0350)  time: 0.5563  data: 0.1245  max mem: 4118\nEpoch: [8]  [ 20/120]  eta: 0:00:34  lr: 0.000050  loss: 0.0942 (0.1334)  loss_classifier: 0.0245 (0.0366)  loss_box_reg: 0.0559 (0.0851)  loss_objectness: 0.0008 (0.0020)  loss_rpn_box_reg: 0.0059 (0.0096)  time: 0.3338  data: 0.0036  max mem: 4118\nEpoch: [8]  [ 40/120]  eta: 0:00:27  lr: 0.000050  loss: 0.0989 (0.1257)  loss_classifier: 0.0283 (0.0342)  loss_box_reg: 0.0635 (0.0807)  loss_objectness: 0.0007 (0.0023)  loss_rpn_box_reg: 0.0052 (0.0084)  time: 0.3320  data: 0.0034  max mem: 4118\nEpoch: [8]  [ 60/120]  eta: 0:00:20  lr: 0.000050  loss: 0.1265 (0.1325)  loss_classifier: 0.0325 (0.0360)  loss_box_reg: 0.0893 (0.0857)  loss_objectness: 0.0014 (0.0027)  loss_rpn_box_reg: 0.0044 (0.0082)  time: 0.3368  data: 0.0035  max mem: 4118\nEpoch: [8]  [ 80/120]  eta: 0:00:13  lr: 0.000050  loss: 0.0977 (0.1320)  loss_classifier: 0.0235 (0.0357)  loss_box_reg: 0.0558 (0.0858)  loss_objectness: 0.0011 (0.0025)  loss_rpn_box_reg: 0.0047 (0.0080)  time: 0.3376  data: 0.0035  max mem: 4118\nEpoch: [8]  [100/120]  eta: 0:00:06  lr: 0.000050  loss: 0.1137 (0.1329)  loss_classifier: 0.0276 (0.0362)  loss_box_reg: 0.0748 (0.0859)  loss_objectness: 0.0012 (0.0024)  loss_rpn_box_reg: 0.0038 (0.0084)  time: 0.3383  data: 0.0036  max mem: 4118\nEpoch: [8]  [119/120]  eta: 0:00:00  lr: 0.000050  loss: 0.1221 (0.1300)  loss_classifier: 0.0341 (0.0358)  loss_box_reg: 0.0712 (0.0832)  loss_objectness: 0.0021 (0.0026)  loss_rpn_box_reg: 0.0057 (0.0084)  time: 0.3293  data: 0.0034  max mem: 4118\nEpoch: [8] Total time: 0:00:40 (0.3374 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:18  model_time: 0.2610 (0.2610)  evaluator_time: 0.0036 (0.0036)  time: 0.3756  data: 0.1088  max mem: 4118\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2253 (0.2335)  evaluator_time: 0.0012 (0.0017)  time: 0.2361  data: 0.0032  max mem: 4118\nTest: Total time: 0:00:12 (0.2433 s / it)\nAveraged stats: model_time: 0.2253 (0.2335)  evaluator_time: 0.0012 (0.0017)\nAccumulating evaluation results...\nDONE (t=0.01s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.597\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.958\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.666\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.294\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.629\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.289\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.655\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.660\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.478\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.686\nEpoch: [9]  [  0/120]  eta: 0:01:02  lr: 0.000005  loss: 0.2162 (0.2162)  loss_classifier: 0.0581 (0.0581)  loss_box_reg: 0.1423 (0.1423)  loss_objectness: 0.0041 (0.0041)  loss_rpn_box_reg: 0.0117 (0.0117)  time: 0.5208  data: 0.1063  max mem: 4118\nEpoch: [9]  [ 20/120]  eta: 0:00:35  lr: 0.000005  loss: 0.0883 (0.1108)  loss_classifier: 0.0271 (0.0284)  loss_box_reg: 0.0535 (0.0702)  loss_objectness: 0.0010 (0.0027)  loss_rpn_box_reg: 0.0041 (0.0094)  time: 0.3453  data: 0.0035  max mem: 4118\nEpoch: [9]  [ 40/120]  eta: 0:00:28  lr: 0.000005  loss: 0.1093 (0.1252)  loss_classifier: 0.0293 (0.0329)  loss_box_reg: 0.0642 (0.0813)  loss_objectness: 0.0010 (0.0022)  loss_rpn_box_reg: 0.0046 (0.0089)  time: 0.3466  data: 0.0036  max mem: 4118\nEpoch: [9]  [ 60/120]  eta: 0:00:20  lr: 0.000005  loss: 0.0878 (0.1183)  loss_classifier: 0.0182 (0.0316)  loss_box_reg: 0.0387 (0.0765)  loss_objectness: 0.0009 (0.0023)  loss_rpn_box_reg: 0.0036 (0.0079)  time: 0.3363  data: 0.0036  max mem: 4118\nEpoch: [9]  [ 80/120]  eta: 0:00:13  lr: 0.000005  loss: 0.1186 (0.1250)  loss_classifier: 0.0355 (0.0338)  loss_box_reg: 0.0691 (0.0806)  loss_objectness: 0.0009 (0.0022)  loss_rpn_box_reg: 0.0057 (0.0085)  time: 0.3427  data: 0.0037  max mem: 4118\nEpoch: [9]  [100/120]  eta: 0:00:06  lr: 0.000005  loss: 0.1115 (0.1301)  loss_classifier: 0.0311 (0.0347)  loss_box_reg: 0.0739 (0.0850)  loss_objectness: 0.0017 (0.0021)  loss_rpn_box_reg: 0.0062 (0.0084)  time: 0.3343  data: 0.0035  max mem: 4118\nEpoch: [9]  [119/120]  eta: 0:00:00  lr: 0.000005  loss: 0.0857 (0.1269)  loss_classifier: 0.0257 (0.0340)  loss_box_reg: 0.0551 (0.0826)  loss_objectness: 0.0013 (0.0021)  loss_rpn_box_reg: 0.0066 (0.0081)  time: 0.3332  data: 0.0038  max mem: 4118\nEpoch: [9] Total time: 0:00:41 (0.3417 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:18  model_time: 0.2536 (0.2536)  evaluator_time: 0.0036 (0.0036)  time: 0.3713  data: 0.1122  max mem: 4118\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2274 (0.2330)  evaluator_time: 0.0012 (0.0017)  time: 0.2359  data: 0.0033  max mem: 4118\nTest: Total time: 0:00:12 (0.2430 s / it)\nAveraged stats: model_time: 0.2274 (0.2330)  evaluator_time: 0.0012 (0.0017)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.602\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.964\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.679\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.315\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.633\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.288\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.660\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.664\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.478\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.691\n\n\n\n# convert to quantized\nquant_rcnn.to(torch.device('cpu'))\nquant_rcnn.eval()\nquant_rcnn.backbone = quantize_fx.convert_fx(quant_rcnn.backbone)\n\nIn the previous post, I saved the state dict of the model. This was a bit cumbersome when re-loading as I had to reinstantiate the model and perform conversion again before I could load the state dict. So here I’ll save it as a torch script which doesn’t have a class definition dependency nor does it require the same conversion steps. This is great because it decouples model creation with usage as long as input/output signatures are known. Once the model is saved, it becomes a recursive script module:\nRecursiveScriptModule(\n  original_name=FasterRCNN\n  (transform): RecursiveScriptModule(original_name=GeneralizedRCNNTransform)\n  (backbone): RecursiveScriptModule(\n    original_name=GraphModule\n    (body): RecursiveScriptModule(\n      original_name=Module\n      (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n      (maxpool): RecursiveScriptModule(original_name=MaxPool2d)\n      (layer1): RecursiveScriptModule(\n        original_name=Module\n        (0): RecursiveScriptModule(\n          original_name=Module\n          (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n          (conv2): RecursiveScriptModule(original_name=ConvReLU2d)\n          (conv3): RecursiveScriptModule(original_name=Conv2d)\n          (downsample): RecursiveScriptModule(\n            original_name=Module\n            (0): RecursiveScriptModule(original_name=Conv2d)\n          )\n        )\n        ...\n\nscript_module = torch.jit.script(quant_rcnn)\nscript_module.save(\"./quant_rcnn_torchscript.pt\")\n\nquant_rcnn_jit = torch.jit.load(\"./quant_rcnn_torchscript.pt\", map_location=torch.device('cpu'))\n\n\nfrom time import perf_counter\n\n\nimages, targets = next(iter(data_loader_test))\nimages = list(img.to(torch.device('cpu')) for img in images)\nn = 10\n\n# warmup\nfor _ in range(n * 3):\n    __ = quant_rcnn_jit(images)\n\nstart = perf_counter()\n\nfor _ in range(n):\n    __ = quant_rcnn_jit(images)\nprint(f\"quant jit model avg time: {(perf_counter() - start) / n:.2f}\")\n\ncode/__torch__/torchvision/models/detection/faster_rcnn.py:103: UserWarning: RCNN always returns a (Losses, Detections) tuple in scripting\n\n\nquant jit model avg time: 1.44\n\n\nAs expected, FX graph mode quantization has about the same inference time as eager. The eager mode model had an average inference time of 1.42.\nNote the UserWarning above. Scripting changes the return signature to includes losses in addition to the normal prediction output. I’m not sure if this applies to other models but at least there is a warning so we know to modify any post processing etc. to handle this change.\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\n\nimage = read_image(\"PennFudanPed/PNGImages/FudanPed00007.png\")\neval_transform = get_transform(train=False)\n\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -&gt; RGB and move to device\n    x = x[:3, ...].to(torch.device('cpu'))\n    predictions = quant_rcnn_jit([x, ])\n    pred = predictions[1][0]  # JIT model returns tuple ({losses}, [pred_dicts])\n\nthreshold = 0.50\nimage = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\nimage = image[:3, ...]\npred_labels = [f\"pedestrian: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"]) if score &gt; threshold]\npred_boxes = pred[\"boxes\"].long()[pred[\"scores\"] &gt; threshold]\n\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))\n\ncode/__torch__/torchvision/models/detection/faster_rcnn/___torch_mangle_17.py:103: UserWarning: RCNN always returns a (Losses, Detections) tuple in scripting\n\n\n&lt;matplotlib.image.AxesImage at 0x7db2e61c3220&gt;\n\n\n\n\n\nFX graph mode quantization made for much easier model preparation compared to eager mode. We did not have to modify network, insert stubs, or even fuse modules. However, because of the symbolic tracing requirement, more complex networks with data dependent control flow may not be able to use it."
  },
  {
    "objectID": "posts/pytorch-tensorrt/onnx_tensorrt.html",
    "href": "posts/pytorch-tensorrt/onnx_tensorrt.html",
    "title": "Speeding up Inference with TensorRT",
    "section": "",
    "text": "follow up post to pytorch quantization … can we make it faster with GPU and TensorRT\n\n# random colab error: \"A UTF-8 locale is required. Got ANSI_X3.4-1968\"\n# https://github.com/googlecolab/colabtools/issues/3409\n\nimport locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"\n\nget FasterRCNN with a resnet101 backbone same as previous post …\n\n\nCode\n%%capture\n\nimport torch\nfrom torchvision.models.resnet import ResNet, Bottleneck, ResNet101_Weights\nfrom torchvision.models._utils import IntermediateLayerGetter\nfrom torchvision.models.detection.backbone_utils import BackboneWithFPN\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN\n\n\ndef resnet_101():\n    resnet = ResNet(block=Bottleneck, layers=[3, 4, 23, 3])\n    resnet.load_state_dict(ResNet101_Weights.DEFAULT.get_state_dict(progress=True))\n    return resnet\n\n\nresnet = resnet_101()\n\n# same as before, get intermediate layers and their output dimensions\nreturned_layers = [1, 2, 3, 4]\nreturn_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}\nin_channels_list = []\nfor k1, m1 in resnet.named_children():\n    if 'layer' in k1:\n        in_channels_list.append((m1[-1].bn3.num_features))\n\nrcnn = FasterRCNN(\n    BackboneWithFPN(\n        backbone=resnet,\n        return_layers=return_layers,\n        in_channels_list=in_channels_list,\n        out_channels=256,\n        extra_blocks=None,\n        norm_layer=None,\n        ),\n    num_classes=2\n)\n\nrcnn.eval()\n\n\ntime the RCNN on both CPU and GPU. I don’t recall what the specs were the last time I used colab to profile the inference time so I’ll document that here as well. I’m using a T4 GPU and the following CPU\n\n# !cat /proc/cpuinfo  | grep 'name' | uniq\n!lscpu | grep 'name'\n\nModel name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n\n\n\n!nvidia-smi -L\n\nGPU 0: NVIDIA L4 (UUID: GPU-393b8fe1-1ca8-7aaf-94b9-04eef8e2fda5)\n\n\n\n# random image\nimage = torch.rand(3, 200, 200)\n# put on CPU\nrcnn.to(torch.device('cpu'))\nimage_cpu = image.to(torch.device('cpu'))\n\nwith torch.no_grad():\n    cpu_time = %timeit -o rcnn([image_cpu])\n\n1.47 s ± 137 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nfrom copy import deepcopy\n\n# on GPU\nrcnn_gpu = deepcopy(rcnn).to(torch.device('cuda'))\n# rcnn.to(torch.device('cuda'))\nimage_gpu = image.to(torch.device('cuda'))\n\nwith torch.no_grad():\n    gpu_time = %timeit -o rcnn_gpu([image_gpu])\n\n37.9 ms ± 235 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nwe can also test with half precision…\n\nrcnn_gpu_half = rcnn_gpu.half().to(torch.device('cuda'))\ninput_half = image_gpu.half()\n\nwith torch.no_grad():\n    gpu_half_time = %timeit -o rcnn_gpu_half([input_half])\n\n29.1 ms ± 1.13 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nalso re-clock the quantized model using FX Graph Mode since it’s performance is also CPU specific\n\n\nCode\n%%capture\n\nfrom torch.ao.quantization import quantize_fx\nfrom torch.ao.quantization.qconfig_mapping import get_default_qconfig_mapping\n\n\nquant_rcnn = deepcopy(rcnn)\n\nqconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")  # \"qnnpack\"\n# assume calibrated already\nquant_rcnn.eval()\nquant_rcnn.to(torch.device('cpu'))\n# prepare and quantize\nexample_input = torch.randn(1, 3, 200, 200)\nquant_rcnn.backbone = quantize_fx.prepare_fx(quant_rcnn.backbone, qconfig_mapping, example_input)\nquant_rcnn.backbone = quantize_fx.convert_fx(quant_rcnn.backbone)\n\nscript_module = torch.jit.script(quant_rcnn)\nscript_module.save(\"./quant_rcnn.pt\")\nquant_rcnn_jit = torch.jit.load(\"./quant_rcnn.pt\", map_location=torch.device('cpu'))\n\n#| code-fold: true\n\n\n\nimport warnings\n\n# warmup\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    for _ in range(3):\n        __ = quant_rcnn_jit([image_cpu])\n\nwith torch.no_grad():\n    quant_time = %timeit -o quant_rcnn_jit([image_cpu])\n\n652 ms ± 81 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nBelow I convert the float model to onnx. I went through onnx because that used to be the preferred way of converting to TensorRT. However, the onnx conversion didn’t play well with the trtexec command line utility for TensorRT regardless of the torch to onnx exporter used. Below the old torch script onnx converter is used but the newer ‘dynamo’ converter also had issues. Thankfully PyTorch has a very easy TensorRT API now, but I keep the ONNX model and evaluate it to see if a simple conversion offers any benefits.\n\n%%capture\n\n!pip install onnx\n!pip install onnxruntime\n\n\nimport onnx\n\ntorch.onnx.export(\n    deepcopy(rcnn),\n    # onnx wants a tuple of 2 or bombs:  https://github.com/zhiqwang/yolort/issues/485\n    ([torch.randn(3, 200, 200)], ),\n    \"rcnn.onnx\",\n    # do_constant_folding=True,\n    opset_version = 11,\n    verbose=False\n    )\n# make sure the onnx proto is valid\nrcnn_onnx = onnx.load(\"rcnn.onnx\")\nonnx.checker.check_model(rcnn_onnx)\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4009: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  (torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i], dtype=torch.float32)).float()))\n/usr/local/lib/python3.10/dist-packages/torchvision/ops/boxes.py:166: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n/usr/local/lib/python3.10/dist-packages/torchvision/ops/boxes.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))\n/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1559: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n  assert condition, message\n/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/transform.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  torch.tensor(s, dtype=torch.float32, device=boxes.device)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/transform.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  / torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset9.py:5858: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n  warnings.warn(\n\n\nrun inference on onnx model, make sure outputs are as expected, then clock-it…\n\nimport onnxruntime\nimport numpy as np\n\nort_session = onnxruntime.InferenceSession(\"rcnn.onnx\", providers=[\"CPUExecutionProvider\"])\n# good to make sure inputs are as expected: '[i.name for i in ort_session.get_inputs()]'\n\n# onnx wants numpy tensor not torch tensor\ndef to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n\n# get a prediction.  onnx doesn't need a list input like torch model does\nort_inputs = {ort_session.get_inputs()[0].name: to_numpy(image)}\nort_outs = ort_session.run(None, ort_inputs)\n\n\n# onxx outputs are list of three arrays corresponding to 'boxes', 'labels', and 'scores'\nprint(\"onnx out shapes: \", [arr.shape for arr in ort_outs])\n# quant model out is tuple of (losses, outputs)\ntorch_outs = __[1][0]\nprint(\"torch out shapes: \", [torch_outs[k].shape for k in torch_outs])\n\nonnx out shapes:  [(100, 4), (100,), (100,)]\ntorch out shapes:  [torch.Size([100, 4]), torch.Size([100]), torch.Size([100])]\n\n\n\nonnx_time = %timeit -o ort_session.run(None, ort_inputs)\n\n# sess = onnxruntime.InferenceSession('rcnn.onnx', providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider'])\n# onnx_trt_time = %timeit -o sess.run(None, ort_inputs)\n\n1.05 s ± 114 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n# more steps for using trtexec which has issues with rcnn input shape\n# !sudo apt-get install tensorrt\n# !pip install tensorrt\n# !ls /usr/src/tensorrt/bin  # make sure trtexec is there\n# !/usr/src/tensorrt/bin/trtexec --onnx=rcnn.onnx --saveEngine=rcnn_engine_pytorch.trt\n\nuse the handy torch-tensorrt package…\n\n%%capture\n\n!python -m pip install torch-tensorrt\n\n\n%%capture\n\ndevice = torch.device(\"cuda\")\nrcnn.to(device)\n\n\nimport torch_tensorrt\n\n# need to wrap rcnn inputs in list\ninputs = [[torch.randn(3, 200, 200).to(\"cuda\")]]  # .half()]\n\ntrt_model = torch_tensorrt.compile(\n    deepcopy(rcnn),\n    ir=\"torch_compile\",\n    # frontend api below complains about input shape\n    # backend=\"torch_tensorrt\",\n    inputs=inputs,\n    enabled_precisions={torch.float32},  #  {torch.half}\n    debug=True,\n    workspace_size=20 &lt;&lt; 30,\n    min_block_size=7,\n    torch_executed_ops={},\n)\n\n\n%%capture\n\n# contrary to docs, first run actually compiles model\n# https://pytorch.org/TensorRT/tutorials/_rendered_examples/dynamo/torch_compile_resnet_example.html#torch-compile-resnet\noutputs = trt_model(*inputs)\n\n\ntrt_time = %timeit -o trt_model(*inputs)\n\n26.1 ms ± 207 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nruntime = [\n    'cpu',\n    'quant',\n    'onnx',\n    'gpu',\n    'gpu half',\n    'tensorrt'\n    ]\nlatency = [\n    cpu_time.average,\n    quant_time.average,\n    onnx_time.average,\n    gpu_time.average,\n    gpu_half_time.average,\n    trt_time.average\n    ]\nlatency = [round(n, 3) for n in latency]\n\nax.bar(runtime, latency)\n\nax.set_ylabel('latency (ms)')\nax.set_yscale('log')\n\nplt.show()\n\n\n\n\n… half precision on the GPU is nearly as fast as TensorRT.. with TensorRT can also use half-precision to improve latency even more …"
  }
]