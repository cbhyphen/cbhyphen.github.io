[
  {
    "objectID": "posts/rcnn-quantization/rcnn_quantization.html",
    "href": "posts/rcnn-quantization/rcnn_quantization.html",
    "title": "Quantizing an Object Detection Model in PyTorch",
    "section": "",
    "text": "At the time of writing this, quantization in PyTorch was relatively new to me. After reading about the advantages of quantization aware training, I wanted a deep dive on eager mode quantization and for something non-trivial. In my research I came across multiple discussions online requesting either help or a tutorial for quantizing the backbone of an object detection model (faster R-CNN in this case). As far as I could tell there was nothing out there and it got me wondering. Are there no tutorials because it’s too difficult or maybe not worth it? Surely there is some advantage with respect to memory requirements or latency (although maybe not better than GPU). In any case, this was the perfect excuse for a deep dive.\nIt’s worth mentioning that I ran into all sorts of issues on my early attempts. This post is polished and makes the whole process look linear but it really wasn’t. There were many attempts, breaks to research bugs, figuring out how to do a thing, reverting to simpler models, etc. I failed and failed, then finally began to understand things, and only then did it work.\n\nNetwork Modifications\nThe first step in setting up the network for quantization is to create a modified bottleneck block. This isn’t obvious until you try to quantize the ResNet without it. You will get an error .. out += identity .. Could not run 'aten::add.out' .. which means that PyTorch isn’t able to quantize the skip connection using the += operator in eager mode. This discussion on the pytorch forums was helpful for describing the error as well as how to fix it. The modified bottleneck block just uses FloatFunctional which has a quantized addition operator. I’m using ResNet 101 here but for much smaller networks you would want to modify the basic block. Also, the original bottleneck class reuses the ReLU layer which won’t work when fusing. Finding this blog post about quantizing ResNet was helpful for realizing and avoiding that pitfall.\n\nfrom functools import partial\nfrom typing import Any, Callable, List, Optional, Type, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\nfrom torchvision.models.resnet import conv1x1, conv3x3\nfrom torch.ao.nn.quantized import FloatFunctional\n\n\nclass BottleneckFloatFunctional(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.0)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.f_add = nn.quantized.FloatFunctional()\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu2(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = self.f_add.add(out, identity)\n        out = self.relu3(out)\n\n        return out\n\nNow we can simply plug in that class to generate the ResNet. Even though the float functional operator was added, we can still load pretrained imagenet weights since the weights/submodules didn’t change. Note that the number of classes for the ResNet don’t matter here because we will extract intermediate layers and not the final fully connected layer (more on that ahead).\n\nfrom torchvision.models.resnet import ResNet, ResNet50_Weights, ResNet101_Weights\n\n\ndef resnet101_ff():\n    return ResNet(block=BottleneckFloatFunctional, layers=[3, 4, 23, 3])\n\n\nres101_backbone_ff = resnet101_ff()\nres101_backbone_ff.load_state_dict(ResNet101_Weights.DEFAULT.get_state_dict(progress=True))\n\n&lt;All keys matched successfully&gt;\n\n\nI’m assuming some familiarity with the R-CNN architecture but to refresh, the feature pyramid network uses output from consecutive layers of the ResNet backbone to extract semantically rich information at different resolutions. Torchvision has a convenience class for the FPN that takes as input an OrderedDict containing the output of backbone layers (no final fully connected layer).\nTo get the layer outputs from the backbone, we can use a utility class IntermediateLayerGetter (type nn.ModuleDict) that returns an OrderedDict of layer outputs given the layer names. There is also a convenience class BackboneWithFPN which uses the intermediate layer getter but creating the FPN that way doesn’t allow the backbone to be quantized.\nAs an aside, it looks like the control flow (for loop) in IntermediateLayerGetter is one reason that the network is not symbolically traceable and why graph FX mode quantization can’t be used here.\n\nfrom torchvision.models._utils import IntermediateLayerGetter\n\n\nreturned_layers = [1, 2, 3, 4]  # get all 4 layers\nreturn_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}  # {'layer1': 0, 'layer2': 1, ...}\n\nres101_backbone_layers = IntermediateLayerGetter(res101_backbone_ff, return_layers=return_layers)\n\nHere’s an example of what the intermediate output looks like for a toy image.\n\nout = res101_backbone_layers(torch.rand(1, 3, 200, 200))  # e.g. 200 x 200 image with 3 channels\n\n[(k, v.shape) for k, v in out.items()]\n\n[('0', torch.Size([1, 256, 50, 50])),\n ('1', torch.Size([1, 512, 25, 25])),\n ('2', torch.Size([1, 1024, 13, 13])),\n ('3', torch.Size([1, 2048, 7, 7]))]\n\n\nWrapping the module dict backbone in sequential layer with quant/dequant stubs causes issues downstream when quantizing, so we need to create an nn.Module wrapper class manually.\n\nfrom typing import Callable, Dict, List, Optional, Union\nfrom torchvision.ops.feature_pyramid_network import ExtraFPNBlock, FeaturePyramidNetwork, LastLevelMaxPool\n\n\nclass QuantLayers(torch.nn.Module):\n\n    def __init__(self, layers_module_dict: torch.nn.ModuleDict):\n        super().__init__()\n        self.quant = torch.ao.quantization.QuantStub()\n        self.dequant = torch.ao.quantization.DeQuantStub()\n        self.layers = layers_module_dict\n\n    def forward(self, x):\n        x = self.quant(x)\n        out = self.layers(x)\n        for k, v in out.items():\n            out[k] = self.dequant(v)\n        return out\n\nNext step is to create a modified backbone with FPN. This follows the BackboneWithFPN class but just uses the wrapped module dict above with quantized inputs and dequantized outputs so they can be fed to the FPN. The feature pyramid network also needs to know the exact dimensions of each output from the resnet backbone so we’ll get that below.\n\nclass QuantBackboneWithFPN(nn.Module):\n\n    def __init__(\n        self,\n        backbone: nn.Module,\n        return_layers: Dict[str, str],\n        in_channels_list: List[int],\n        out_channels: int,\n        extra_blocks: Optional[ExtraFPNBlock] = None,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -&gt; None:\n        super().__init__()\n\n        if extra_blocks is None:\n            extra_blocks = LastLevelMaxPool()\n\n        self.body = QuantLayers(\n            IntermediateLayerGetter(backbone, return_layers=return_layers)\n        )\n        self.fpn = FeaturePyramidNetwork(\n            in_channels_list=in_channels_list,\n            out_channels=out_channels,\n            extra_blocks=extra_blocks,\n            norm_layer=norm_layer,\n        )\n        self.out_channels = out_channels\n\n    def forward(self, x: Tensor) -&gt; Dict[str, Tensor]:\n        x = self.body(x)\n        x = self.fpn(x)\n        return x\n\n\n#  there are several ways to get these dimensions\n\n# from backbone_utils.py: https://github.com/pytorch/vision/blob/main/torchvision/models/detection/backbone_utils.py#L145\n# in_channels_stage2 = res101_backbone_layers.inplanes // 8\n# in_channels_list = [in_channels_stage2 * 2 ** (i - 1) for i in returned_layers]\n\nin_channels_list = []\nfor k1, m1 in res101_backbone_layers.named_children():\n  if 'layer' in k1:\n    in_channels_list.append((m1[-1].bn3.num_features))\n\nin_channels_list\n\n[256, 512, 1024, 2048]\n\n\nAnother important note is that regular BatchNorm2d is used here instead of FrozenBatchNorm2d because the latter isn’t quantizeable. Frozen batch norm is the recommended layer because batches are generally too small for good estimates of mean and variance statistics. So using regular batch norm could be unstable and less performant if those layers aren’t frozen.\n\nres101_bb_fpn = QuantBackboneWithFPN(\n    backbone=res101_backbone_layers,\n    return_layers=return_layers,\n    in_channels_list=in_channels_list,\n    out_channels=256,  # defined\n    extra_blocks=None,  # defaults to LastLevelMaxPool, see above\n    norm_layer=None,  # defaults to torch.nn.BatchNorm2d, see FeaturePyramidNetwork\n)\n# res101_bb_fpn\n\nAt this point there should be quant/dequant stubs in the network similar to below.\nQuantBackboneWithFPN(\n  (body): QuantLayers(\n    (quant): QuantStub()\n    (dequant): DeQuantStub()\n    (layers): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      ...\nOnce the backbone with FPN is created, it can be plugged into the FasterRCNN torchvision class. The number of classes is set to 2 for object or background (specific to the dataset used here).\n\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN\n\n\nquant_rcnn_res101 = FasterRCNN(res101_bb_fpn, num_classes=2)\n# quant_rcnn_res101\n\n\n\nLayer Fusion and Quantization Config\nBefore training and subsequently converting the model, we can fuse specific sequences of modules in the backbone. Fusing compresses the model making it smaller and run faster. Below sequences of Conv2d-BatchNorm2d-ReLU and Conv2d-BatchNorm2d are fused. After fusing you should see new fused modules in the network (shown below) like ConvReLU2d as well as Identity.\n(backbone): QuantBackboneWithFPN(\n    (body): QuantLayers(\n      (quant): QuantStub()\n      (dequant): DeQuantStub()\n      (layers): IntermediateLayerGetter(\n        (conv1): ConvReLU2d(\n          (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n          (1): ReLU(inplace=True)\n        )\n        (bn1): Identity()\n        (relu): Identity()\n        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n        (layer1): Sequential(\n          (0): BottleneckFloatFunctional(\n            (conv1): ConvReLU2d(\n              (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n              (1): ReLU(inplace=True)\n            )\n            (bn1): Identity()\n            (relu1): Identity()\n            ...\n\nfrom torch.ao.quantization import fuse_modules\n\n\nquant_rcnn_res101.eval()\n# fuse stem\nfuse_modules(quant_rcnn_res101.backbone.body.layers, [['conv1', 'bn1', 'relu']], inplace=True)\n# fuse blocks\nfor k1, m1 in quant_rcnn_res101.backbone.body.layers.named_children():\n    if \"layer\" in k1:  # in sequential layer with blocks\n        for k2, m2 in m1.named_children():\n            fuse_modules(m2, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\", \"relu2\"], [\"conv3\", \"bn3\"]], inplace=True)\n            for k3, m3 in m2.named_children():\n                if \"downsample\" in k3:  # fuse downsample\n                    fuse_modules(m3, [[\"0\", \"1\"]], inplace=True)\n\nBefore training the qconfig needs to be set but on the resnet backbone only. And again, because the batches are so small, batch norm gets frozen using the handy API found in this PyTorch tutorial. Last, I’ll freeze the stem and the first layer in the backbone since the pretrained imagenet weights were loaded. After preparation you should be able to see the observers in the network.\n(backbone): QuantBackboneWithFPN(\n    (body): QuantLayers(\n      (quant): QuantStub(\n        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n        )\n      )\n      (dequant): DeQuantStub()\n      (layers): IntermediateLayerGetter(\n        (conv1): ConvReLU2d(\n          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)\n          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n          )\n          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n          )\n        )\n        (bn1): Identity()\n        (relu): Identity()\n        ...\n\nimport re\n\n\n# for backend in torch.backends.quantized.supported_engines:\n#     print(backend)\n\nquant_rcnn_res101.train()\nquant_rcnn_res101.backbone.body.qconfig = torch.ao.quantization.get_default_qat_qconfig('fbgemm')\nquant_rcnn_res101_prepared = torch.ao.quantization.prepare_qat(quant_rcnn_res101, inplace=False)\n\nquant_rcnn_res101_prepared = quant_rcnn_res101_prepared.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n\nquant_rcnn_res101_prepared.backbone.body.layers.conv1.weight.requires_grad = False\nfor name, parameter in quant_rcnn_res101_prepared.backbone.body.named_parameters():\n    if re.search(r\".layer1\", name):\n        parameter.requires_grad = False\n\n\n\nDataset, Training, and Conversion\nI’ll be using the PennFudan dataset from the Torchvision object detection finetuning tutorial for QAT. Most of the code below is borrowed from that tutorial with slight modifications since I don’t perform segmentation.\n\nimport os\nimport torch\n\nfrom torchvision.io import read_image\nfrom torchvision.ops.boxes import masks_to_boxes\nfrom torchvision import tv_tensors\nfrom torchvision.transforms.v2 import functional as F\nfrom torchvision.transforms import v2 as T\n\n\nclass PennFudanDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = read_image(img_path)\n        mask = read_image(mask_path)\n        # instances are encoded as different colors\n        obj_ids = torch.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n        num_objs = len(obj_ids)\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n\n        # get bounding box coordinates for each mask\n        boxes = masks_to_boxes(masks)\n\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n\n        image_id = idx\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        # Wrap sample and targets into torchvision tv_tensors:\n        img = tv_tensors.Image(img)\n\n        target = {}\n        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n        target[\"masks\"] = tv_tensors.Mask(masks)\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)\n\n\ndef get_transform(train):\n    transforms = []\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    transforms.append(T.ToDtype(torch.float, scale=True))\n    transforms.append(T.ToPureTensor())\n    return T.Compose(transforms)\n\n\n%%capture\n\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")\n\n!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n!unzip PennFudanPed.zip -d ./\n\n\nimport utils\nfrom engine import train_one_epoch, evaluate\n\n\n# train on the GPU or on the CPU, if a GPU is not available\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n# use our dataset and defined transformations\ndataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\ndataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n\n# split the dataset in train and test set\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=2,\n    shuffle=True,\n    num_workers=1,\n    collate_fn=utils.collate_fn\n)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test,\n    batch_size=1,\n    shuffle=False,\n    num_workers=1,\n    collate_fn=utils.collate_fn\n)\n\n\n# move model to the right device\nquant_rcnn_res101_prepared.to(device)\n\n# construct an optimizer\nparams = [p for p in quant_rcnn_res101_prepared.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(\n    params,\n    lr=0.005,\n    momentum=0.9,\n    weight_decay=0.0005\n)\n\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=3,\n    gamma=0.1\n)\n\n# let's train it for 10 epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(quant_rcnn_res101_prepared, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(quant_rcnn_res101_prepared, data_loader_test, device=device)\n\nEpoch: [0]  [ 0/60]  eta: 0:01:05  lr: 0.000090  loss: 1.6602 (1.6602)  loss_classifier: 0.9224 (0.9224)  loss_box_reg: 0.0023 (0.0023)  loss_objectness: 0.7214 (0.7214)  loss_rpn_box_reg: 0.0140 (0.0140)  time: 1.0922  data: 0.2625  max mem: 9188\nEpoch: [0]  [10/60]  eta: 0:00:40  lr: 0.000936  loss: 0.7143 (0.9616)  loss_classifier: 0.0847 (0.3113)  loss_box_reg: 0.0019 (0.0041)  loss_objectness: 0.6638 (0.6250)  loss_rpn_box_reg: 0.0206 (0.0212)  time: 0.8087  data: 0.0288  max mem: 9188\nEpoch: [0]  [20/60]  eta: 0:00:31  lr: 0.001783  loss: 0.5990 (0.7357)  loss_classifier: 0.1087 (0.2302)  loss_box_reg: 0.0221 (0.0527)  loss_objectness: 0.3577 (0.4293)  loss_rpn_box_reg: 0.0206 (0.0235)  time: 0.7719  data: 0.0054  max mem: 9188\nEpoch: [0]  [30/60]  eta: 0:00:23  lr: 0.002629  loss: 0.3487 (0.5963)  loss_classifier: 0.1135 (0.1929)  loss_box_reg: 0.0837 (0.0640)  loss_objectness: 0.0951 (0.3145)  loss_rpn_box_reg: 0.0212 (0.0250)  time: 0.7637  data: 0.0054  max mem: 9188\nEpoch: [0]  [40/60]  eta: 0:00:15  lr: 0.003476  loss: 0.3345 (0.5256)  loss_classifier: 0.1140 (0.1790)  loss_box_reg: 0.0874 (0.0739)  loss_objectness: 0.0602 (0.2496)  loss_rpn_box_reg: 0.0201 (0.0230)  time: 0.7614  data: 0.0054  max mem: 9188\nEpoch: [0]  [50/60]  eta: 0:00:07  lr: 0.004323  loss: 0.2655 (0.4923)  loss_classifier: 0.1164 (0.1727)  loss_box_reg: 0.0991 (0.0861)  loss_objectness: 0.0439 (0.2110)  loss_rpn_box_reg: 0.0160 (0.0226)  time: 0.7539  data: 0.0054  max mem: 9188\nEpoch: [0]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2582 (0.4716)  loss_classifier: 0.1061 (0.1660)  loss_box_reg: 0.1086 (0.0956)  loss_objectness: 0.0400 (0.1876)  loss_rpn_box_reg: 0.0109 (0.0223)  time: 0.7506  data: 0.0055  max mem: 9188\nEpoch: [0] Total time: 0:00:46 (0.7705 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:26  model_time: 0.2997 (0.2997)  evaluator_time: 0.0087 (0.0087)  time: 0.5225  data: 0.2121  max mem: 9188\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2671 (0.2676)  evaluator_time: 0.0043 (0.0045)  time: 0.2761  data: 0.0031  max mem: 9188\nTest: Total time: 0:00:14 (0.2843 s / it)\nAveraged stats: model_time: 0.2671 (0.2676)  evaluator_time: 0.0043 (0.0045)\nAccumulating evaluation results...\nDONE (t=0.04s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.021\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.089\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.022\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.002\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.077\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.303\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.325\nEpoch: [1]  [ 0/60]  eta: 0:01:00  lr: 0.005000  loss: 0.2305 (0.2305)  loss_classifier: 0.0670 (0.0670)  loss_box_reg: 0.1148 (0.1148)  loss_objectness: 0.0354 (0.0354)  loss_rpn_box_reg: 0.0133 (0.0133)  time: 1.0080  data: 0.2090  max mem: 9188\nEpoch: [1]  [10/60]  eta: 0:00:38  lr: 0.005000  loss: 0.3300 (0.3228)  loss_classifier: 0.1011 (0.1051)  loss_box_reg: 0.1396 (0.1461)  loss_objectness: 0.0468 (0.0494)  loss_rpn_box_reg: 0.0231 (0.0222)  time: 0.7770  data: 0.0242  max mem: 9188\nEpoch: [1]  [20/60]  eta: 0:00:30  lr: 0.005000  loss: 0.2922 (0.2905)  loss_classifier: 0.0966 (0.0966)  loss_box_reg: 0.1377 (0.1319)  loss_objectness: 0.0367 (0.0438)  loss_rpn_box_reg: 0.0156 (0.0183)  time: 0.7568  data: 0.0057  max mem: 9188\nEpoch: [1]  [30/60]  eta: 0:00:23  lr: 0.005000  loss: 0.2693 (0.2873)  loss_classifier: 0.0876 (0.0962)  loss_box_reg: 0.1191 (0.1302)  loss_objectness: 0.0337 (0.0430)  loss_rpn_box_reg: 0.0101 (0.0179)  time: 0.7612  data: 0.0057  max mem: 9188\nEpoch: [1]  [40/60]  eta: 0:00:15  lr: 0.005000  loss: 0.2052 (0.2968)  loss_classifier: 0.0669 (0.1035)  loss_box_reg: 0.1018 (0.1287)  loss_objectness: 0.0299 (0.0464)  loss_rpn_box_reg: 0.0101 (0.0182)  time: 0.7637  data: 0.0057  max mem: 9188\nEpoch: [1]  [50/60]  eta: 0:00:07  lr: 0.005000  loss: 0.1873 (0.2863)  loss_classifier: 0.0641 (0.0982)  loss_box_reg: 0.0945 (0.1282)  loss_objectness: 0.0212 (0.0416)  loss_rpn_box_reg: 0.0164 (0.0183)  time: 0.7661  data: 0.0057  max mem: 9188\nEpoch: [1]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2432 (0.2833)  loss_classifier: 0.0731 (0.0959)  loss_box_reg: 0.1244 (0.1310)  loss_objectness: 0.0187 (0.0376)  loss_rpn_box_reg: 0.0170 (0.0188)  time: 0.7676  data: 0.0057  max mem: 9188\nEpoch: [1] Total time: 0:00:46 (0.7698 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:24  model_time: 0.2893 (0.2893)  evaluator_time: 0.0060 (0.0060)  time: 0.4938  data: 0.1962  max mem: 9188\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2744 (0.2684)  evaluator_time: 0.0021 (0.0024)  time: 0.2816  data: 0.0031  max mem: 9188\nTest: Total time: 0:00:14 (0.2826 s / it)\nAveraged stats: model_time: 0.2744 (0.2684)  evaluator_time: 0.0021 (0.0024)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.696\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.040\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.113\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.219\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.131\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.389\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.408\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.360\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.421\nEpoch: [2]  [ 0/60]  eta: 0:01:02  lr: 0.005000  loss: 0.1664 (0.1664)  loss_classifier: 0.0622 (0.0622)  loss_box_reg: 0.0806 (0.0806)  loss_objectness: 0.0133 (0.0133)  loss_rpn_box_reg: 0.0103 (0.0103)  time: 1.0334  data: 0.2054  max mem: 9188\nEpoch: [2]  [10/60]  eta: 0:00:40  lr: 0.005000  loss: 0.1824 (0.2101)  loss_classifier: 0.0578 (0.0609)  loss_box_reg: 0.1048 (0.1166)  loss_objectness: 0.0102 (0.0110)  loss_rpn_box_reg: 0.0180 (0.0216)  time: 0.8014  data: 0.0237  max mem: 9188\nEpoch: [2]  [20/60]  eta: 0:00:31  lr: 0.005000  loss: 0.1885 (0.2088)  loss_classifier: 0.0615 (0.0680)  loss_box_reg: 0.1048 (0.1126)  loss_objectness: 0.0076 (0.0093)  loss_rpn_box_reg: 0.0180 (0.0190)  time: 0.7785  data: 0.0057  max mem: 9188\nEpoch: [2]  [30/60]  eta: 0:00:23  lr: 0.005000  loss: 0.1895 (0.2282)  loss_classifier: 0.0627 (0.0734)  loss_box_reg: 0.1084 (0.1228)  loss_objectness: 0.0067 (0.0110)  loss_rpn_box_reg: 0.0160 (0.0211)  time: 0.7754  data: 0.0058  max mem: 9188\nEpoch: [2]  [40/60]  eta: 0:00:15  lr: 0.005000  loss: 0.2135 (0.2191)  loss_classifier: 0.0580 (0.0679)  loss_box_reg: 0.1133 (0.1171)  loss_objectness: 0.0069 (0.0131)  loss_rpn_box_reg: 0.0182 (0.0210)  time: 0.7747  data: 0.0058  max mem: 9188\nEpoch: [2]  [50/60]  eta: 0:00:07  lr: 0.005000  loss: 0.1913 (0.2131)  loss_classifier: 0.0502 (0.0647)  loss_box_reg: 0.1150 (0.1163)  loss_objectness: 0.0074 (0.0126)  loss_rpn_box_reg: 0.0145 (0.0195)  time: 0.7779  data: 0.0066  max mem: 9188\nEpoch: [2]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2049 (0.2168)  loss_classifier: 0.0544 (0.0649)  loss_box_reg: 0.1379 (0.1196)  loss_objectness: 0.0077 (0.0120)  loss_rpn_box_reg: 0.0157 (0.0202)  time: 0.7743  data: 0.0071  max mem: 9188\nEpoch: [2] Total time: 0:00:46 (0.7830 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:26  model_time: 0.3114 (0.3114)  evaluator_time: 0.0047 (0.0047)  time: 0.5266  data: 0.2082  max mem: 9188\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2667 (0.2690)  evaluator_time: 0.0020 (0.0022)  time: 0.2730  data: 0.0034  max mem: 9188\nTest: Total time: 0:00:14 (0.2832 s / it)\nAveraged stats: model_time: 0.2667 (0.2690)  evaluator_time: 0.0020 (0.0022)\nAccumulating evaluation results...\nDONE (t=0.03s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.426\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.941\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.275\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.003\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.249\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.446\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.216\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.535\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.540\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.380\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.561\nEpoch: [3]  [ 0/60]  eta: 0:00:59  lr: 0.000500  loss: 0.1269 (0.1269)  loss_classifier: 0.0302 (0.0302)  loss_box_reg: 0.0771 (0.0771)  loss_objectness: 0.0064 (0.0064)  loss_rpn_box_reg: 0.0132 (0.0132)  time: 0.9940  data: 0.2165  max mem: 9188\nEpoch: [3]  [10/60]  eta: 0:00:40  lr: 0.000500  loss: 0.1716 (0.2328)  loss_classifier: 0.0517 (0.0684)  loss_box_reg: 0.1076 (0.1371)  loss_objectness: 0.0037 (0.0082)  loss_rpn_box_reg: 0.0132 (0.0190)  time: 0.8007  data: 0.0249  max mem: 9188\nEpoch: [3]  [20/60]  eta: 0:00:31  lr: 0.000500  loss: 0.1716 (0.2058)  loss_classifier: 0.0479 (0.0585)  loss_box_reg: 0.1076 (0.1229)  loss_objectness: 0.0041 (0.0069)  loss_rpn_box_reg: 0.0143 (0.0175)  time: 0.7782  data: 0.0058  max mem: 9188\nEpoch: [3]  [30/60]  eta: 0:00:23  lr: 0.000500  loss: 0.1613 (0.1940)  loss_classifier: 0.0434 (0.0556)  loss_box_reg: 0.1042 (0.1161)  loss_objectness: 0.0051 (0.0067)  loss_rpn_box_reg: 0.0131 (0.0157)  time: 0.7731  data: 0.0059  max mem: 9188\nEpoch: [3]  [40/60]  eta: 0:00:15  lr: 0.000500  loss: 0.1840 (0.1919)  loss_classifier: 0.0563 (0.0555)  loss_box_reg: 0.1063 (0.1140)  loss_objectness: 0.0051 (0.0064)  loss_rpn_box_reg: 0.0132 (0.0160)  time: 0.7706  data: 0.0062  max mem: 9188\nEpoch: [3]  [50/60]  eta: 0:00:07  lr: 0.000500  loss: 0.1402 (0.1820)  loss_classifier: 0.0444 (0.0527)  loss_box_reg: 0.0820 (0.1080)  loss_objectness: 0.0044 (0.0060)  loss_rpn_box_reg: 0.0132 (0.0153)  time: 0.7652  data: 0.0061  max mem: 9188\nEpoch: [3]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1260 (0.1743)  loss_classifier: 0.0367 (0.0501)  loss_box_reg: 0.0732 (0.1045)  loss_objectness: 0.0022 (0.0055)  loss_rpn_box_reg: 0.0080 (0.0143)  time: 0.7578  data: 0.0055  max mem: 9188\nEpoch: [3] Total time: 0:00:46 (0.7759 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:26  model_time: 0.3235 (0.3235)  evaluator_time: 0.0042 (0.0042)  time: 0.5317  data: 0.2016  max mem: 9188\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2647 (0.2682)  evaluator_time: 0.0016 (0.0018)  time: 0.2752  data: 0.0030  max mem: 9188\nTest: Total time: 0:00:14 (0.2819 s / it)\nAveraged stats: model_time: 0.2647 (0.2682)  evaluator_time: 0.0016 (0.0018)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.501\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.938\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.453\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.314\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.523\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.245\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.575\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.575\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.380\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.599\nEpoch: [4]  [ 0/60]  eta: 0:00:58  lr: 0.000500  loss: 0.1874 (0.1874)  loss_classifier: 0.0379 (0.0379)  loss_box_reg: 0.1311 (0.1311)  loss_objectness: 0.0055 (0.0055)  loss_rpn_box_reg: 0.0129 (0.0129)  time: 0.9828  data: 0.2060  max mem: 9188\nEpoch: [4]  [10/60]  eta: 0:00:39  lr: 0.000500  loss: 0.1874 (0.1985)  loss_classifier: 0.0483 (0.0520)  loss_box_reg: 0.1261 (0.1253)  loss_objectness: 0.0042 (0.0041)  loss_rpn_box_reg: 0.0143 (0.0171)  time: 0.7883  data: 0.0238  max mem: 9188\nEpoch: [4]  [20/60]  eta: 0:00:31  lr: 0.000500  loss: 0.1580 (0.1767)  loss_classifier: 0.0424 (0.0474)  loss_box_reg: 0.0927 (0.1102)  loss_objectness: 0.0040 (0.0045)  loss_rpn_box_reg: 0.0133 (0.0147)  time: 0.7684  data: 0.0057  max mem: 9188\nEpoch: [4]  [30/60]  eta: 0:00:23  lr: 0.000500  loss: 0.1519 (0.1718)  loss_classifier: 0.0421 (0.0475)  loss_box_reg: 0.0991 (0.1055)  loss_objectness: 0.0038 (0.0048)  loss_rpn_box_reg: 0.0093 (0.0140)  time: 0.7741  data: 0.0059  max mem: 9188\nEpoch: [4]  [40/60]  eta: 0:00:15  lr: 0.000500  loss: 0.1401 (0.1661)  loss_classifier: 0.0371 (0.0455)  loss_box_reg: 0.0776 (0.1031)  loss_objectness: 0.0038 (0.0046)  loss_rpn_box_reg: 0.0092 (0.0129)  time: 0.7815  data: 0.0058  max mem: 9188\nEpoch: [4]  [50/60]  eta: 0:00:07  lr: 0.000500  loss: 0.1408 (0.1695)  loss_classifier: 0.0360 (0.0473)  loss_box_reg: 0.0843 (0.1031)  loss_objectness: 0.0038 (0.0054)  loss_rpn_box_reg: 0.0112 (0.0136)  time: 0.7709  data: 0.0057  max mem: 9188\nEpoch: [4]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1481 (0.1657)  loss_classifier: 0.0366 (0.0461)  loss_box_reg: 0.0888 (0.1014)  loss_objectness: 0.0026 (0.0052)  loss_rpn_box_reg: 0.0119 (0.0130)  time: 0.7635  data: 0.0058  max mem: 9188\nEpoch: [4] Total time: 0:00:46 (0.7779 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:25  model_time: 0.3073 (0.3073)  evaluator_time: 0.0039 (0.0039)  time: 0.5077  data: 0.1946  max mem: 9188\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2605 (0.2693)  evaluator_time: 0.0016 (0.0019)  time: 0.2714  data: 0.0030  max mem: 9188\nTest: Total time: 0:00:14 (0.2829 s / it)\nAveraged stats: model_time: 0.2605 (0.2693)  evaluator_time: 0.0016 (0.0019)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.939\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.367\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.017\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.242\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.242\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.570\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.571\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.320\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.595\nEpoch: [5]  [ 0/60]  eta: 0:00:59  lr: 0.000500  loss: 0.1179 (0.1179)  loss_classifier: 0.0285 (0.0285)  loss_box_reg: 0.0738 (0.0738)  loss_objectness: 0.0035 (0.0035)  loss_rpn_box_reg: 0.0120 (0.0120)  time: 0.9861  data: 0.2058  max mem: 9188\nEpoch: [5]  [10/60]  eta: 0:00:39  lr: 0.000500  loss: 0.1691 (0.1664)  loss_classifier: 0.0493 (0.0478)  loss_box_reg: 0.1028 (0.1006)  loss_objectness: 0.0039 (0.0043)  loss_rpn_box_reg: 0.0109 (0.0137)  time: 0.7915  data: 0.0246  max mem: 9188\nEpoch: [5]  [20/60]  eta: 0:00:31  lr: 0.000500  loss: 0.1691 (0.1752)  loss_classifier: 0.0499 (0.0500)  loss_box_reg: 0.1028 (0.1053)  loss_objectness: 0.0042 (0.0052)  loss_rpn_box_reg: 0.0109 (0.0149)  time: 0.7731  data: 0.0062  max mem: 9188\nEpoch: [5]  [30/60]  eta: 0:00:23  lr: 0.000500  loss: 0.1550 (0.1697)  loss_classifier: 0.0483 (0.0486)  loss_box_reg: 0.0918 (0.1023)  loss_objectness: 0.0046 (0.0052)  loss_rpn_box_reg: 0.0104 (0.0136)  time: 0.7696  data: 0.0057  max mem: 9188\nEpoch: [5]  [40/60]  eta: 0:00:15  lr: 0.000500  loss: 0.1439 (0.1698)  loss_classifier: 0.0439 (0.0489)  loss_box_reg: 0.0912 (0.1030)  loss_objectness: 0.0028 (0.0046)  loss_rpn_box_reg: 0.0097 (0.0133)  time: 0.7672  data: 0.0054  max mem: 9188\nEpoch: [5]  [50/60]  eta: 0:00:07  lr: 0.000500  loss: 0.1373 (0.1627)  loss_classifier: 0.0387 (0.0469)  loss_box_reg: 0.0841 (0.0988)  loss_objectness: 0.0029 (0.0045)  loss_rpn_box_reg: 0.0078 (0.0125)  time: 0.7658  data: 0.0054  max mem: 9188\nEpoch: [5]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1196 (0.1647)  loss_classifier: 0.0331 (0.0475)  loss_box_reg: 0.0802 (0.1003)  loss_objectness: 0.0030 (0.0047)  loss_rpn_box_reg: 0.0071 (0.0121)  time: 0.7703  data: 0.0056  max mem: 9188\nEpoch: [5] Total time: 0:00:46 (0.7769 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:25  model_time: 0.3016 (0.3016)  evaluator_time: 0.0036 (0.0036)  time: 0.5064  data: 0.1989  max mem: 9188\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2680 (0.2723)  evaluator_time: 0.0015 (0.0018)  time: 0.2758  data: 0.0029  max mem: 9188\nTest: Total time: 0:00:14 (0.2859 s / it)\nAveraged stats: model_time: 0.2680 (0.2723)  evaluator_time: 0.0015 (0.0018)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.517\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.964\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.484\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.006\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.315\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.539\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.256\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.583\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.586\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.067\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.608\nEpoch: [6]  [ 0/60]  eta: 0:00:59  lr: 0.000050  loss: 0.1945 (0.1945)  loss_classifier: 0.0693 (0.0693)  loss_box_reg: 0.1114 (0.1114)  loss_objectness: 0.0028 (0.0028)  loss_rpn_box_reg: 0.0110 (0.0110)  time: 0.9962  data: 0.2050  max mem: 9188\nEpoch: [6]  [10/60]  eta: 0:00:39  lr: 0.000050  loss: 0.1817 (0.1884)  loss_classifier: 0.0551 (0.0548)  loss_box_reg: 0.1114 (0.1192)  loss_objectness: 0.0029 (0.0025)  loss_rpn_box_reg: 0.0103 (0.0117)  time: 0.7963  data: 0.0238  max mem: 9188\nEpoch: [6]  [20/60]  eta: 0:00:31  lr: 0.000050  loss: 0.1631 (0.1723)  loss_classifier: 0.0446 (0.0494)  loss_box_reg: 0.1005 (0.1094)  loss_objectness: 0.0026 (0.0025)  loss_rpn_box_reg: 0.0089 (0.0110)  time: 0.7784  data: 0.0056  max mem: 9188\nEpoch: [6]  [30/60]  eta: 0:00:23  lr: 0.000050  loss: 0.1194 (0.1552)  loss_classifier: 0.0393 (0.0450)  loss_box_reg: 0.0682 (0.0973)  loss_objectness: 0.0021 (0.0027)  loss_rpn_box_reg: 0.0071 (0.0102)  time: 0.7767  data: 0.0055  max mem: 9188\nEpoch: [6]  [40/60]  eta: 0:00:15  lr: 0.000050  loss: 0.1135 (0.1558)  loss_classifier: 0.0380 (0.0451)  loss_box_reg: 0.0677 (0.0974)  loss_objectness: 0.0027 (0.0030)  loss_rpn_box_reg: 0.0074 (0.0103)  time: 0.7690  data: 0.0055  max mem: 9188\nEpoch: [6]  [50/60]  eta: 0:00:07  lr: 0.000050  loss: 0.1423 (0.1599)  loss_classifier: 0.0434 (0.0465)  loss_box_reg: 0.0839 (0.0993)  loss_objectness: 0.0035 (0.0035)  loss_rpn_box_reg: 0.0110 (0.0106)  time: 0.7677  data: 0.0057  max mem: 9188\nEpoch: [6]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1605 (0.1649)  loss_classifier: 0.0487 (0.0483)  loss_box_reg: 0.0996 (0.1019)  loss_objectness: 0.0027 (0.0040)  loss_rpn_box_reg: 0.0074 (0.0107)  time: 0.7675  data: 0.0056  max mem: 9188\nEpoch: [6] Total time: 0:00:46 (0.7791 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:25  model_time: 0.3030 (0.3030)  evaluator_time: 0.0038 (0.0038)  time: 0.5109  data: 0.2020  max mem: 9188\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2680 (0.2678)  evaluator_time: 0.0015 (0.0018)  time: 0.2732  data: 0.0031  max mem: 9188\nTest: Total time: 0:00:14 (0.2814 s / it)\nAveraged stats: model_time: 0.2680 (0.2678)  evaluator_time: 0.0015 (0.0018)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.521\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.948\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.470\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.306\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.544\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.269\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.591\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.596\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.617\nEpoch: [7]  [ 0/60]  eta: 0:00:58  lr: 0.000050  loss: 0.0945 (0.0945)  loss_classifier: 0.0253 (0.0253)  loss_box_reg: 0.0563 (0.0563)  loss_objectness: 0.0073 (0.0073)  loss_rpn_box_reg: 0.0056 (0.0056)  time: 0.9705  data: 0.1853  max mem: 9188\nEpoch: [7]  [10/60]  eta: 0:00:39  lr: 0.000050  loss: 0.1117 (0.1174)  loss_classifier: 0.0318 (0.0328)  loss_box_reg: 0.0728 (0.0726)  loss_objectness: 0.0033 (0.0046)  loss_rpn_box_reg: 0.0056 (0.0075)  time: 0.7892  data: 0.0219  max mem: 9188\nEpoch: [7]  [20/60]  eta: 0:00:31  lr: 0.000050  loss: 0.1672 (0.1506)  loss_classifier: 0.0427 (0.0432)  loss_box_reg: 0.1006 (0.0924)  loss_objectness: 0.0033 (0.0049)  loss_rpn_box_reg: 0.0100 (0.0101)  time: 0.7759  data: 0.0056  max mem: 9188\nEpoch: [7]  [30/60]  eta: 0:00:23  lr: 0.000050  loss: 0.1998 (0.1628)  loss_classifier: 0.0521 (0.0470)  loss_box_reg: 0.1177 (0.1005)  loss_objectness: 0.0039 (0.0048)  loss_rpn_box_reg: 0.0111 (0.0105)  time: 0.7782  data: 0.0055  max mem: 9188\nEpoch: [7]  [40/60]  eta: 0:00:15  lr: 0.000050  loss: 0.1580 (0.1646)  loss_classifier: 0.0507 (0.0471)  loss_box_reg: 0.1028 (0.1019)  loss_objectness: 0.0040 (0.0047)  loss_rpn_box_reg: 0.0098 (0.0109)  time: 0.7750  data: 0.0056  max mem: 9188\nEpoch: [7]  [50/60]  eta: 0:00:07  lr: 0.000050  loss: 0.1286 (0.1654)  loss_classifier: 0.0416 (0.0473)  loss_box_reg: 0.0761 (0.1023)  loss_objectness: 0.0022 (0.0050)  loss_rpn_box_reg: 0.0091 (0.0108)  time: 0.7680  data: 0.0062  max mem: 9188\nEpoch: [7]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1352 (0.1616)  loss_classifier: 0.0416 (0.0464)  loss_box_reg: 0.0761 (0.0996)  loss_objectness: 0.0024 (0.0047)  loss_rpn_box_reg: 0.0087 (0.0108)  time: 0.7615  data: 0.0061  max mem: 9188\nEpoch: [7] Total time: 0:00:46 (0.7777 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:25  model_time: 0.2944 (0.2944)  evaluator_time: 0.0038 (0.0038)  time: 0.5057  data: 0.2053  max mem: 9188\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2690 (0.2697)  evaluator_time: 0.0015 (0.0017)  time: 0.2784  data: 0.0030  max mem: 9188\nTest: Total time: 0:00:14 (0.2834 s / it)\nAveraged stats: model_time: 0.2690 (0.2697)  evaluator_time: 0.0015 (0.0017)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.511\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.960\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.464\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.020\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.168\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.536\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.267\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.581\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.585\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.360\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.608\nEpoch: [8]  [ 0/60]  eta: 0:01:02  lr: 0.000050  loss: 0.1875 (0.1875)  loss_classifier: 0.0613 (0.0613)  loss_box_reg: 0.1094 (0.1094)  loss_objectness: 0.0076 (0.0076)  loss_rpn_box_reg: 0.0092 (0.0092)  time: 1.0381  data: 0.2319  max mem: 9188\nEpoch: [8]  [10/60]  eta: 0:00:39  lr: 0.000050  loss: 0.1625 (0.1392)  loss_classifier: 0.0334 (0.0383)  loss_box_reg: 0.1082 (0.0888)  loss_objectness: 0.0020 (0.0028)  loss_rpn_box_reg: 0.0103 (0.0093)  time: 0.7846  data: 0.0262  max mem: 9188\nEpoch: [8]  [20/60]  eta: 0:00:31  lr: 0.000050  loss: 0.1625 (0.1732)  loss_classifier: 0.0377 (0.0486)  loss_box_reg: 0.1059 (0.1091)  loss_objectness: 0.0022 (0.0040)  loss_rpn_box_reg: 0.0105 (0.0114)  time: 0.7634  data: 0.0056  max mem: 9188\nEpoch: [8]  [30/60]  eta: 0:00:23  lr: 0.000050  loss: 0.1233 (0.1589)  loss_classifier: 0.0372 (0.0447)  loss_box_reg: 0.0807 (0.0999)  loss_objectness: 0.0024 (0.0035)  loss_rpn_box_reg: 0.0093 (0.0107)  time: 0.7660  data: 0.0055  max mem: 9188\nEpoch: [8]  [40/60]  eta: 0:00:15  lr: 0.000050  loss: 0.1437 (0.1624)  loss_classifier: 0.0351 (0.0458)  loss_box_reg: 0.0866 (0.1019)  loss_objectness: 0.0024 (0.0037)  loss_rpn_box_reg: 0.0096 (0.0110)  time: 0.7712  data: 0.0056  max mem: 9188\nEpoch: [8]  [50/60]  eta: 0:00:07  lr: 0.000050  loss: 0.1505 (0.1609)  loss_classifier: 0.0417 (0.0462)  loss_box_reg: 0.1007 (0.1003)  loss_objectness: 0.0022 (0.0037)  loss_rpn_box_reg: 0.0095 (0.0108)  time: 0.7742  data: 0.0058  max mem: 9188\nEpoch: [8]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1266 (0.1577)  loss_classifier: 0.0403 (0.0455)  loss_box_reg: 0.0737 (0.0980)  loss_objectness: 0.0022 (0.0036)  loss_rpn_box_reg: 0.0070 (0.0106)  time: 0.7697  data: 0.0057  max mem: 9188\nEpoch: [8] Total time: 0:00:46 (0.7761 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:24  model_time: 0.2885 (0.2885)  evaluator_time: 0.0034 (0.0034)  time: 0.4929  data: 0.1990  max mem: 9188\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2648 (0.2672)  evaluator_time: 0.0016 (0.0017)  time: 0.2731  data: 0.0031  max mem: 9188\nTest: Total time: 0:00:14 (0.2808 s / it)\nAveraged stats: model_time: 0.2648 (0.2672)  evaluator_time: 0.0016 (0.0017)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.529\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.962\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.499\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.004\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.330\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.553\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.264\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.597\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.603\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.067\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.480\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.623\nEpoch: [9]  [ 0/60]  eta: 0:01:02  lr: 0.000005  loss: 0.1457 (0.1457)  loss_classifier: 0.0373 (0.0373)  loss_box_reg: 0.0974 (0.0974)  loss_objectness: 0.0019 (0.0019)  loss_rpn_box_reg: 0.0091 (0.0091)  time: 1.0399  data: 0.2021  max mem: 9188\nEpoch: [9]  [10/60]  eta: 0:00:39  lr: 0.000005  loss: 0.1672 (0.1505)  loss_classifier: 0.0467 (0.0432)  loss_box_reg: 0.0974 (0.0931)  loss_objectness: 0.0023 (0.0036)  loss_rpn_box_reg: 0.0091 (0.0107)  time: 0.7953  data: 0.0234  max mem: 9188\nEpoch: [9]  [20/60]  eta: 0:00:31  lr: 0.000005  loss: 0.1563 (0.1567)  loss_classifier: 0.0475 (0.0462)  loss_box_reg: 0.0904 (0.0967)  loss_objectness: 0.0026 (0.0035)  loss_rpn_box_reg: 0.0070 (0.0103)  time: 0.7706  data: 0.0062  max mem: 9188\nEpoch: [9]  [30/60]  eta: 0:00:23  lr: 0.000005  loss: 0.1367 (0.1544)  loss_classifier: 0.0420 (0.0449)  loss_box_reg: 0.0846 (0.0953)  loss_objectness: 0.0027 (0.0040)  loss_rpn_box_reg: 0.0086 (0.0103)  time: 0.7776  data: 0.0071  max mem: 9188\nEpoch: [9]  [40/60]  eta: 0:00:15  lr: 0.000005  loss: 0.1362 (0.1518)  loss_classifier: 0.0395 (0.0437)  loss_box_reg: 0.0834 (0.0941)  loss_objectness: 0.0028 (0.0038)  loss_rpn_box_reg: 0.0109 (0.0102)  time: 0.7749  data: 0.0063  max mem: 9188\nEpoch: [9]  [50/60]  eta: 0:00:07  lr: 0.000005  loss: 0.1520 (0.1632)  loss_classifier: 0.0409 (0.0471)  loss_box_reg: 0.0889 (0.1008)  loss_objectness: 0.0034 (0.0044)  loss_rpn_box_reg: 0.0088 (0.0109)  time: 0.7710  data: 0.0057  max mem: 9188\nEpoch: [9]  [59/60]  eta: 0:00:00  lr: 0.000005  loss: 0.1608 (0.1608)  loss_classifier: 0.0415 (0.0465)  loss_box_reg: 0.0889 (0.0990)  loss_objectness: 0.0028 (0.0044)  loss_rpn_box_reg: 0.0113 (0.0109)  time: 0.7794  data: 0.0059  max mem: 9188\nEpoch: [9] Total time: 0:00:46 (0.7830 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:25  model_time: 0.3008 (0.3008)  evaluator_time: 0.0039 (0.0039)  time: 0.5143  data: 0.2076  max mem: 9188\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2704 (0.2700)  evaluator_time: 0.0016 (0.0018)  time: 0.2762  data: 0.0031  max mem: 9188\nTest: Total time: 0:00:14 (0.2838 s / it)\nAveraged stats: model_time: 0.2704 (0.2700)  evaluator_time: 0.0016 (0.0018)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.516\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.954\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.508\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.004\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.295\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.542\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.264\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.590\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.596\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.067\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.400\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.619\n\n\nNow to convert model and save the model. Make sure to put the model on CPU before conversion or you will get an error. After conversion you should see quantized modules like QuantizedConvReLU2d.\n(backbone): QuantBackboneWithFPN(\n    (body): QuantLayers(\n      (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([56]), dtype=torch.quint8)\n      (dequant): DeQuantize()\n      (layers): IntermediateLayerGetter(\n        (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.3137107789516449, zero_point=0, padding=(3, 3))\n        (bn1): Identity()\n        (relu): Identity()\n        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n        (layer1): Sequential(\n          (0): BottleneckFloatFunctional(\n            (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.15175238251686096, zero_point=0)\n            (bn1): Identity()\n            (relu1): Identity()\n            ...\n\nquant_rcnn_res101_prepared.eval()\nquant_rcnn_res101_prepared.to(torch.device('cpu'))\n\nquant_rcnn_res101_converted = torch.ao.quantization.convert(quant_rcnn_res101_prepared, inplace=False)\n\nquant_model_101_path = \"/content/quant_model_101.pth\"\ntorch.save(quant_rcnn_res101_converted.state_dict(), quant_model_101_path)\n\nFor comparison I’ll generate the same network without any modifications made for quantization (including fusion). Then we can compare model sizes and latency. Note that this is just comparing latency on the CPU, if the float model was on GPU it could be significantly faster depending upon the hardware.\n\n%%capture\nfrom torchvision.models.detection.backbone_utils import BackboneWithFPN\nfrom torchvision.models.resnet import Bottleneck\n\n\nres101_backbone = ResNet(block=Bottleneck, layers=[3, 4, 23, 3])\nres101_backbone.load_state_dict(ResNet101_Weights.DEFAULT.get_state_dict(progress=False))\nrcnn_res101 = FasterRCNN(\n    BackboneWithFPN(\n        backbone=res101_backbone,\n        return_layers=return_layers,\n        in_channels_list=in_channels_list,\n        out_channels=256,\n        extra_blocks=None,\n        norm_layer=None,\n        ),\n    num_classes=2\n)\n\nrcnn_res101.eval()\nrcnn_res101.to(torch.device('cpu'))\nmodel_101_path = \"/content/model_101.pth\"\ntorch.save(rcnn_res101.state_dict(), model_101_path)\n\n\nprint(f'size of quantized model: {round(os.path.getsize(\"/content/quant_model_101.pth\") / 1e6)} MB')\nprint(f'size of float model: {round(os.path.getsize(\"/content/model_101.pth\") / 1e6)} MB')\n\nsize of quantized model: 115 MB\nsize of float model: 242 MB\n\n\n\nfrom time import perf_counter\n\n# just grab one test image/batch\nimages, targets = next(iter(data_loader_test))\nimages = list(img.to(torch.device('cpu')) for img in images)\nn = 10\n\nstart = perf_counter()\nfor _ in range(n):\n    __ = quant_rcnn_res101_converted(images)\nprint(f\"quant model avg time: {(perf_counter() - start) / n:.2f}\")\n\nstart = perf_counter()\nfor _ in range(n):\n    __ = rcnn_res101(images)\nprint(f\"float model (cpu) avg time: {(perf_counter() - start) / n:.2f}\")\n\nquant model avg time: 1.84\nfloat model (cpu) avg time: 2.39\n\n\nI believe a fully quantized model would be even smaller and faster by comparison. In this case, while we did quantize the backbone for the RCNN, it only accounted for roughly 70% of the model parameters. So a significant number of float operations still occur after the quantized backbone.\n\nnum_model_params = sum(p.numel() for p in rcnn_res101.parameters() if p.requires_grad)\nnum_backbone_params = sum(p.numel() for p in rcnn_res101.backbone.body.parameters() if p.requires_grad)\n\nprint(f\"total number of parameters in model: {num_model_params}\")\nprint(f\"total number of parameters in backbone: {num_backbone_params}\")\nprint(f\"ratio of quantized parameters: {num_backbone_params / num_model_params:.2f}\")\n\ntotal number of parameters in model: 60344409\ntotal number of parameters in backbone: 42500160\nratio of quantized parameters: 0.70\n\n\nWe can also profile each model to see where each spends the most time during a forward pass.\n\nfrom torch.profiler import profile, record_function, ProfilerActivity\n\n\nwith profile(activities=[ProfilerActivity.CPU], record_shapes=False) as prof:\n    with record_function(\"model_inference\"):\n        quant_rcnn_res101_converted(images)\n\nprint(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n\nwith profile(activities=[ProfilerActivity.CPU], record_shapes=False) as prof:\n    with record_function(\"model_inference\"):\n        rcnn_res101(images)\n\nprint(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n\n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                  model_inference         1.55%      31.980ms       100.00%        2.068s        2.068s             1  \n           quantized::conv2d_relu        34.88%     721.174ms        35.12%     726.108ms      10.837ms            67  \n                     aten::conv2d         0.01%     277.000us        22.65%     468.216ms      20.357ms            23  \n                aten::convolution         0.02%     463.000us        22.64%     468.117ms      20.353ms            23  \n               aten::_convolution         0.02%     478.000us        22.62%     467.654ms      20.333ms            23  \n         aten::mkldnn_convolution        22.44%     463.959ms        22.48%     464.897ms      20.213ms            23  \n           torchvision::roi_align        11.18%     231.104ms        14.49%     299.518ms      74.879ms             4  \n                quantized::conv2d        14.12%     291.885ms        14.17%     292.962ms       7.918ms            37  \n                      aten::relu_         3.83%      79.098ms         3.93%      81.327ms       2.140ms            38  \n                      aten::clone         0.07%       1.494ms         3.40%      70.212ms       1.463ms            48  \n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 2.068s\n\n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                  model_inference         3.11%      72.709ms       100.00%        2.341s        2.341s             1  \n                     aten::conv2d         0.03%     765.000us        70.93%        1.660s      13.073ms           127  \n                aten::convolution         0.12%       2.885ms        70.90%        1.660s      13.067ms           127  \n               aten::_convolution         0.09%       2.153ms        70.78%        1.657s      13.045ms           127  \n         aten::mkldnn_convolution        70.41%        1.648s        70.68%        1.655s      13.028ms           127  \n           torchvision::roi_align        10.81%     253.069ms        10.89%     254.852ms      63.713ms             4  \n                 aten::batch_norm         0.02%     443.000us         4.26%      99.738ms     959.019us           104  \n     aten::_batch_norm_impl_index         0.04%     942.000us         4.24%      99.295ms     954.760us           104  \n          aten::native_batch_norm         4.04%      94.653ms         4.19%      98.097ms     943.240us           104  \n                     aten::linear         0.00%      76.000us         2.97%      69.608ms      17.402ms             4  \n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 2.341s\n\n\n\nThe following loads the saved quantized model. It’s important that the same process of fusing, preparing, and converting be done before loading weights since quantization significantly alters the network. For sake of completeness, we can look at a prediction from the partially quantized RCNN.\n\n%%capture\n\nquant_model_loaded = FasterRCNN(\n    QuantBackboneWithFPN(\n        backbone=resnet101_ff(),\n        return_layers=return_layers,\n        in_channels_list=in_channels_list,\n        out_channels=256,\n        extra_blocks=None,\n        norm_layer=None\n        ),\n    num_classes=2\n    )\n\nquant_model_loaded.eval()\nfuse_modules(quant_model_loaded.backbone.body.layers, [['conv1', 'bn1', 'relu']], inplace=True)\nfor k1, m1 in quant_model_loaded.backbone.body.layers.named_children():\n    if \"layer\" in k1:  # in sequential layer with blocks\n        for k2, m2 in m1.named_children():\n            fuse_modules(m2, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\", \"relu2\"], [\"conv3\", \"bn3\"]], inplace=True)\n            for k3, m3 in m2.named_children():\n                if \"downsample\" in k3:  # fuse downsample\n                    fuse_modules(m3, [[\"0\", \"1\"]], inplace=True)\n\nquant_model_loaded.train()\nquant_model_loaded.backbone.body.qconfig = torch.quantization.get_default_qconfig('fbgemm')\ntorch.quantization.prepare_qat(quant_model_loaded, inplace=True)\ntorch.quantization.convert(quant_model_loaded, inplace=True)\n\nquant_model_loaded.eval()\nquant_model_loaded.load_state_dict(torch.load(quant_model_101_path, map_location=torch.device('cpu')))\n\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\nimage = read_image(\"PennFudanPed/PNGImages/FudanPed00022.png\")  # 7, 22\neval_transform = get_transform(train=False)\n\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -&gt; RGB and move to device\n    x = x[:3, ...].to(torch.device('cpu'))\n    predictions = quant_model_loaded([x, ])\n    pred = predictions[0]\n\nthreshold = 0.80\nimage = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\nimage = image[:3, ...]\npred_labels = [f\"pedestrian: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"]) if score &gt; threshold]\npred_boxes = pred[\"boxes\"].long()[pred[\"scores\"] &gt; threshold]\n\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n\n# masks = (pred[\"masks\"] &gt; 0.7).squeeze(1)\n# output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))\n\n&lt;matplotlib.image.AxesImage at 0x7e43de5482e0&gt;"
  },
  {
    "objectID": "posts/bayes-classifier/bayes_classifier.html",
    "href": "posts/bayes-classifier/bayes_classifier.html",
    "title": "Bayesian Classifiers",
    "section": "",
    "text": "Even though I’ve studied (and revisited, and revisited..) Bayesian statistics several times over the years, I always felt that, over time, my understanding would lose it’s sharpness. In my opinion, the Bayesian paradigm isn’t very intuitive. So I created this post as future reference to myself, but also as a way to dive deeper into things like the naive assumption, maximum a posterior vs maximum likelihood, and decision boundaries."
  },
  {
    "objectID": "posts/bayes-classifier/bayes_classifier.html#maximum-a-posteriori",
    "href": "posts/bayes-classifier/bayes_classifier.html#maximum-a-posteriori",
    "title": "Bayesian Classifiers",
    "section": "maximum a posteriori",
    "text": "maximum a posteriori\nLet’s assume there is a random variable \\(X\\) that follows a Gaussian distribution\n\\[\nX \\sim N(\\mu, \\sigma)\n\\]\nand a variable \\(Y\\) which is discrete\n\\[\nY\\in\\{0, 1\\}\n\\]\nSuppose we know that the value of \\(Y\\) is dependent upon \\(X\\), but that the relationship is not deterministic. We can model this relationship using conditional probability\n\\[\nP(Y=y|X=x)\n\\]\nBut say we want to assign \\(Y\\) a definitive value (i.e., classify). In that case we can simply select the value of \\(Y\\) with the highest probability\n\\[\n\\arg\\max_ y P(Y|X)\n\\]\nAnd because we are selecting a value for \\(Y\\) when there is uncertainty, this means we are making an estimate. The above is known as the maximum a posteriori (MAP) estimate of \\(Y\\) given \\(X\\), and \\(P(Y|X)\\) is commonly referred to as the posterior.\nMost likely we won’t have knowledge of the posterior (\\(Y\\) is unknown afterall), so we use Bayes theorem to derive an equivalence\n\\[\nP(Y|X) = {P(Y \\cap X) \\over P(X)} = {P(X|Y) \\cdot P(Y) \\over P(X)}\n\\]\nwhere\n\n\\(P(X|Y)\\) is the likelhood (i.e., probability of the data given the class)\n\\(P(Y)\\) is the prior (i.e., probability of the class)\n\\(P(X)\\) is the marginal (i.e., probability of the data)\n\nWhen performing the MAP estimate, we are given some value of \\(X\\) and then calculate the posterior probability for each possible value of \\(Y\\). This means that the marginal is the same for all values of \\(Y\\) and is just a constant that can be factored out\n\\[\nP(Y|X) \\propto {P(X|Y) \\cdot P(Y)}\n\\]\nwhich simplifies the MAP classifier to\n\\[\n\\arg\\max_y {P(X|Y)  \\cdot P(Y)}\n\\]\nAs far as the likelihood function, we made an assumption on the distribution of \\(X\\) so we can use the Gaussian probability density function\n\\[\np(x|y) = \\frac{1}{\\sigma_y\\sqrt2\\pi} e ^ {- \\frac{1}{2} ( \\frac{x - \\mu_y}{\\sigma_y} ) ^2}\n\\]\nIf we don’t know the Gaussian parameters above, we just estimate them using the empirical mean and variance of the training data for each class which is a maximum likelihood estimate.\n\\[\n\\mu_y = \\frac{1}{n}\\sum_{i}^{n}x_i\n\\]\n\\[\n\\sigma_y^2 = \\frac{1}{n}\\sum_{i}^{n}(x_i - \\mu_y)^2\n\\]\nWe don’t know the distribution of the prior, so we have to estimate it. In practice, we simply use the prevalence of each class in the training data which is again a maximum likelihood estimate.\n\\[\np(y) = \\frac{1}{n}\\sum_{i}^{n} \\mathbb{1}(y_i = y)\n\\]\nIt’s worth noting that there is also a maximum likelihood estimate (MLE) that could be used for the classifier. As the name suggest we would just use the likelihood term and remove the prior\n\\[\n\\arg\\max_y {P(X|Y)}\n\\]\nbut this ignores the prior distribution if we have that information.\nWith some basic theory out of the way, let’s build a classifer."
  },
  {
    "objectID": "posts/bayes-classifier/bayes_classifier.html#univariate-classifier",
    "href": "posts/bayes-classifier/bayes_classifier.html#univariate-classifier",
    "title": "Bayesian Classifiers",
    "section": "univariate classifier",
    "text": "univariate classifier\nLet’s simulate univariate Gaussian data for the two classes. For simplicity, the data will have different means but the same variance.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\nsns.set()\n%matplotlib inline\n\n\nn = 1000\nmu_1 = 40\nmu_2 = 80\nstd = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std, size=n)\nx_2 = np.random.normal(loc=mu_2, scale=std, size=n)\n\ndf = pd.DataFrame({'x': np.concatenate([x_1, x_2]), 'y': [1] * n + [2] * n})\n\nsns.displot(df, kind='kde', x='x', hue='y',\n            fill=True, linewidth=0, palette='dark', alpha=0.5)\n\n\n\n\nTime to estimate priors, means, and standard deviations. This is trivial since we generated the data but let’s pretend that we didn’t :)\n\npriors = {k: df[df.y == k].size / df.size for k in df.y.unique()}\npriors\n\n{1: 0.5, 2: 0.5}\n\n\n\nmeans = {k: df[df.y == k].x.mean() for k in df.y.unique()}\nmeans\n\n{1: 39.74917237733038, 2: 80.16187136171098}\n\n\n\nstdevs = {k: df[df.y == k].x.std() for k in df.y.unique()}\n# .std(ddof=0) if not sample\nstdevs\n\n{1: 10.004638113965834, 2: 10.265729149219876}\n\n\nNow that the data is fit, we can build a classifier and predict new instances.\n\n# scipy actually has gaussian pdf: from scipy.stats import norm\n\ndef uni_gaussian_pdf(x, mean, stdev):\n    scalar = 1.0 / (stdev * np.sqrt(2 * np.pi))\n    exponential = np.exp(-0.5 * ((x - mean) / stdev) ** 2)\n    return scalar * exponential\n\nclasses = df.y.unique().tolist()  \n\ndef gbayes_uni_classify(x):\n    probas = []\n    for c in classes:\n        likelihood = uni_gaussian_pdf(x, means[c], stdevs[c])\n        # likelihood = norm.pdf(x, means[c], stdevs[c])\n        probas.append(likelihood * priors[c])\n    return classes[np.argmax(probas)]\n\nIt’s important to mention here that the priors are the same since we generated equal amounts of data for both classes. Mathematically this means that the prior is a constant and can be factored out in the original MAP equation (for this case) giving\n\\[\n\\arg\\max_y {P(X|Y)}\n\\]\nSo in the case where priors are the same, the MAP is equivalent to the MLE.\nAnd now to visualize the decision boundary.\n\nsim_data = np.arange(0, 150, 1)  # uniform sequence\nsim_class_preds = [gbayes_uni_classify(x) for x in sim_data]\n\ndecision_boundary = np.where(np.array(sim_class_preds[:-1]) - np.array(sim_class_preds[1:]) != 0)[0]\nprint(decision_boundary)\n\nsns.displot(df, kind='kde', x='x', hue='y', fill=True, linewidth=0, palette='dark', alpha=0.5)\nfor v in sim_data[decision_boundary]:\n    plt.axvline(v, color='black', linestyle='--')\n\n[59]\n\n\n\n\n\nThe decision boundary is roughly halfway between means, as expected. This isn’t super interesting but what if the variances are different?\n\nn = 1000\nmu_1 = 40\nmu_2 = 80\nstd_1 = 20\nstd_2 = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std_1, size=n)\nx_2 = np.random.normal(loc=mu_2, scale=std_2, size=n)\n\ndf = pd.DataFrame({'x': np.concatenate([x_1, x_2]), 'y': [1] * n + [2] * n})\n\nsns.displot(df, kind='kde', x='x', hue='y', fill=True, linewidth=0, palette='dark', alpha=0.5)\n\n\n\n\n\n# class so we don't repeat same spiel\n\nclass GBUniClf:\n    \n    def __init__(self):\n        self.classes = None\n        self.priors = None\n        self.means = None\n        self.stdevs = None\n        \n    def fit(self, df):\n        self.classes = df.y.unique().tolist()\n        self.priors = {k: df[df.y == k].size / df.size for k in self.classes}\n        self.means = {k: df[df.y == k].x.mean() for k in self.classes}\n        self.stdevs = {k: df[df.y == k].x.std() for k in self.classes}\n        \n    def likelihood(self, x, mean, stdev):\n        scalar = 1.0 / (stdev * np.sqrt(2 * np.pi))\n        exponential = np.exp(-0.5 * ((x - mean) / stdev) ** 2)\n        return scalar * exponential\n    \n    def predict(self, x):\n        probas = []\n        for c in self.classes:\n            likelihood = self.likelihood(x, self.means[c], self.stdevs[c])\n            probas.append(likelihood * self.priors[c])\n        return self.classes[np.argmax(probas)]\n\n\nclf = GBUniClf()\nclf.fit(df)\n\nsim_data = np.arange(-50, 200, 1)\nsim_class_preds = [clf.predict(x) for x in sim_data]\n\ndecision_boundary = np.where(np.array(sim_class_preds[:-1]) - np.array(sim_class_preds[1:]) != 0)[0]\nprint(decision_boundary)\n\ndf_preds = pd.DataFrame({'x': sim_data, 'y': sim_class_preds})\n\nsns.displot(df, kind='kde', x='x', hue='y', fill=True, linewidth=0, palette='dark', alpha=0.3)\nrug_plot = sns.rugplot(df_preds, x=\"x\", hue=\"y\", palette='dark', alpha=0.7)\nrug_plot.get_legend().remove()\nfor v in sim_data[decision_boundary]:\n    plt.axvline(v, color='black', linestyle='--')\n\n[113 174]\n\n\n\n\n\nBecause class 1 has a larger variance, there is now a second decision boundary. Instances with high values of \\(x\\) (far right) are less likely to belong to class 2 even though they are closer to its’ mean. Instead they get classified as 1.\nWhat if the priors are different?\n\nn_1 = 2000\nn_2 = 500\nmu_1 = 40\nmu_2 = 80\nstd_1 = 20\nstd_2 = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std_1, size=n_1)\nx_2 = np.random.normal(loc=mu_2, scale=std_2, size=n_2)\n\ndf = pd.DataFrame({'x': np.concatenate([x_1, x_2]), 'y': [1] * n_1 + [2] * n_2})\n\nclf = GBUniClf()\nclf.fit(df)\n\nsim_data = np.arange(-50, 200, 1)\nsim_class_preds = [clf.predict(x) for x in sim_data]\n\ndecision_boundary = np.where(np.array(sim_class_preds[:-1]) - np.array(sim_class_preds[1:]) != 0)[0]\nprint(decision_boundary)\n\ndf_preds = pd.DataFrame({'x': sim_data, 'y': sim_class_preds})\n\nsns.displot(df, kind='kde', x='x', hue='y', fill=True, linewidth=0, palette='dark', alpha=0.3)\nrug_plot = sns.rugplot(df_preds, x=\"x\", hue=\"y\", palette='dark', alpha=0.7)\nrug_plot.get_legend().remove()\nfor v in sim_data[decision_boundary]:\n    plt.axvline(v, color='black', linestyle='--')\n\n[120 164]\n\n\n\n\n\nIt simply makes the more prevalent class more likely as expected.\n\nmultivariate\nNow we can look at bivariate data where covariance between features come into play. The naive assumption ignores covariance so we can compare classifiers that do and do not make that assumption.\nMathematically, the posterior is now conditioned on multiple features\n\\[\nP(Y|X) = P(Y|x_1, x_2, \\ldots , x_i)\n\\]\nand the MAP classifier in the multivariate case is\n\\[\n\\arg\\max_y {P(Y) \\cdot P(x_1, x_2, \\ldots , x_i|Y)}\n\\]\nTherefore we use the multivariate likelihood function which makes use of covariance\n\\[\np(x|y) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma_y|}} e^{ - \\frac{1}{2} (x - \\mu_y)^T \\Sigma_y^{-1} (x - \\mu_y)}\n\\]\nThis is a drop-in replacement though, and the rest of the classifier is the same.\n\nn = 1000\nmu_1 = 40\nmu_2 = 80\nstd = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std, size=(n, 2))\nx_2 = np.random.normal(loc=mu_2, scale=std, size=(n, 2))\ndata = np.concatenate([x_1, x_2])\n\ndf = pd.DataFrame({'x1': data[:, 0], 'x2': data[:, 1], 'y': [1] * n + [2] * n})\n\n# s = sns.scatterplot(df, x='x1', y='x2', hue='y', hue_order=classes, palette='dark', alpha=0.25)\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\n\nclass GBBiClf:\n    \n    def __init__(self):\n        self.classes = None\n        self.priors = None\n        self.means = None\n        self.covars = None\n        self.covar_dets = None\n        self.covar_invs = None\n\n    def fit(self, df):\n        self.classes = df.y.unique().tolist()\n        self.priors = {k: df[df.y == k].shape[0] / df.shape[0] for k in self.classes}\n        self.means = {k: df[['x1', 'x2']][df.y == k].mean(axis=0) for k in self.classes}\n        self.covars = {k: np.cov(df[['x1', 'x2']][df.y == k], rowvar=False, bias=True) for k in self.classes}\n        self.covar_dets = {k: np.linalg.det(self.covars[k]) for k in self.classes}\n        self.covar_invs = {k: np.linalg.inv(self.covars[k]) for k in self.classes}\n\n    def likelihood(self, x, c):\n        dims = 2\n        scalar = 1.0 / np.sqrt(((2 * np.pi) ** dims) * self.covar_dets[c])\n        exponential = np.exp(-0.5 * (x - self.means[c]).T @ self.covar_invs[c] @ (x - self.means[c]))\n        return scalar * exponential\n\n    def predict(self, x):\n        probas = []\n        for c in self.classes:\n            likelihood = self.likelihood(x, c)\n            probas.append(likelihood * self.priors[c])\n        return self.classes[np.argmax(probas)]\n\n\nclf = GBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(0, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\n# sns.scatterplot(plot_df, x='x1', y=\"x2\", hue=\"y\", hue_order=classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\nThe variance was same for both distributions and the features were sampled independently, so the decision boundary isn’t complex. Slight curvature is due to the estimate of covariance which is different from the true value.\n\nclf.covars\n\n{1: array([[97.13589025,  3.99602431],\n        [ 3.99602431, 99.21513485]]),\n 2: array([[104.87159824,  -2.09309889],\n        [ -2.09309889, 102.99262222]])}\n\n\nEven though this data is uninteresting, let’s compare the decision boundary of a naive classifier. The naive assumption is that all features are independent, so we can use the chain rule of probability for a simpler calculation of the likelihood.\n\\[\nP(X|Y) = P(x_1, x_2, \\ldots , x_i|Y) = \\prod\\limits_{i}P(x_i|Y)\n\\]\nThe MAP classifier under the naive assumption then becomes\n\\[\n\\arg\\max_y {P(Y) \\cdot P(x_1|Y) \\cdot P(x_2|Y) \\cdot \\ldots \\cdot P(x_m|Y)}\n\\]\nFor this case though, since the features were generated independently, the decision boundary should be roughly the same.\n\nclass GNBBiClf:\n    \n    def __init__(self):\n        self.classes = None\n        self.priors = None\n        self.means = None\n\n    def fit(self, df):\n        self.classes = df.y.unique().tolist()\n        self.priors = {k: df[df.y == k].shape[0] / df.shape[0] for k in self.classes}\n        self.means = {k: df[['x1', 'x2']][df.y == k].mean(axis=0) for k in self.classes}\n        self.stdevs = {k: np.std(df[['x1', 'x2']][df.y == k], axis=0) for k in self.classes}\n\n    def likelihood(self, x, mean, stdev):\n        scalar = 1.0 / (stdev * np.sqrt(2 * np.pi))\n        exponential = np.exp(-0.5 * ((x - mean) / stdev) ** 2)\n        return scalar * exponential\n    \n    def predict(self, x):\n        probas = []\n        for c in self.classes:\n            joint_likelihood = 1\n            for i, v in enumerate(x):\n                likelihood = self.likelihood(v, self.means[c][i], self.stdevs[c][i])\n                joint_likelihood *= likelihood\n            probas.append(joint_likelihood * self.priors[c])\n        return self.classes[np.argmax(probas)]\n\n\nclf = GNBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(0, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\nWhat if just the covariance are different? Let’s draw random data were the features are still independent (i.e. covariance matrix is symmetic) but the variance of features is different for each class.\n\nn = 1000\nmu_1 = 40\nmu_2 = 100\nstd_1 = 20\nstd_2 = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std_1, size=(n, 2))\nx_2 = np.random.normal(loc=mu_2, scale=std_2, size=(n, 2))\ndata = np.concatenate([x_1, x_2])\n\ndf = pd.DataFrame({'x1': data[:, 0], 'x2': data[:, 1], 'y': [1] * n + [2] * n})\n\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\n\nclf = GBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(-10, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\nAs expected, the decision boundary favors class 1 since it had larger variance. Without any correlation between features, I would again expect the decision boundary to be the same for the naive classifier.\n\nclf = GNBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(-10, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\n# sanity check\n# from sklearn.naive_bayes import GaussianNB\n# clf = GaussianNB()\n# clf.fit(df[['x1', 'x2']].values, df[['y']].values.ravel())\n# sim_data_range = np.arange(-10, 140, 1)\n# sim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\n# sim_classes = [clf.predict(x.reshape(1, -1)) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\nThe decision boundary for the naive classifier is roughly identical. Zooming out, we can see the classifier has similar behavior as the univariate case for different variance.\n\nclf = GNBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(-10, 200, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 200), ylim=(-10, 200))\n\n[(-10.0, 200.0), (-10.0, 200.0)]\n\n\n\n\n\nLet’s finally simulate data with correlation between features. There should be a noticeable difference in the decision boundary for the naive classifier.\n\nn = 1000\nmu_1 = 40\nmu_2 = 100\n\nx_1 = np.random.multivariate_normal(mean=[mu_1, mu_1], cov=[[50, 70], [70, 200]], size=n)\nx_2 = np.random.multivariate_normal(mean=[mu_2, mu_2], cov=[[100, 1], [1, 100]], size=n)  # no correlation\ndata = np.concatenate([x_1, x_2])\n\ndf = pd.DataFrame({'x1': data[:, 0], 'x2': data[:, 1], 'y': [1] * n + [2] * n})\n\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\n\nclf = GBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(-10, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\n\nclf = GNBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(-10, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\nThe difference between the naive and non-naive classifier is more noticeable when there is correlation between features (class 1). The naive classifier clearly ignores the covariance and the decision boundary is much smoother.\nOne obvious advantage of the naive assumption is computational efficiency. Predictions for the naive classifier ran in faster time compared to the non-naive by an order of magnitude. Fit times were roughly the same.\n\nfrom time import perf_counter\n\n# TODO memory reqs\n\nm = 100\n\nstart_time = perf_counter()\nfor i in range(m):\n    clf = GBBiClf()\n    clf.fit(df)\nprint(f\"GB avg fit time: {(perf_counter() - start_time) / m:.6f}\")\nstart_time = perf_counter()\nfor i in range(m):\n    clf.predict(sim_data[i])\nprint(f\"GB avg predict time: {(perf_counter() - start_time) / m:.6f}\")\n\nstart_time = perf_counter()\nfor i in range(m):\n    clf = GNBBiClf()\n    clf.fit(df)\nprint(f\"GNB avg fit time: {(perf_counter() - start_time) / m:.6f}\")\nstart_time = perf_counter()\nfor i in range(m):\n    clf.predict(sim_data[i])\nprint(f\"GNB avg predict time: {(perf_counter() - start_time) / m:.6f}\")\n\nGB avg fit time: 0.004047\nGB avg predict time: 0.000654\nGNB avg fit time: 0.005097\nGNB avg predict time: 0.000047"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cbhyphen",
    "section": "",
    "text": "Quantizing an Object Detection Model in PyTorch\n\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nme\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Classifiers\n\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\nme\n\n\n\n\n\n\nNo matching items"
  }
]