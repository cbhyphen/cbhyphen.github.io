[
  {
    "objectID": "posts/bayes-classifier/bayes_classifier.html",
    "href": "posts/bayes-classifier/bayes_classifier.html",
    "title": "Bayesian Classifiers",
    "section": "",
    "text": "Even though I’ve studied (and revisited, and revisited..) Bayesian statistics several times over the years, I always felt that, over time, my understanding would lose it’s sharpness. In my opinion, the Bayesian paradigm isn’t very intuitive. So I created this post as future reference to myself, but also as a way to dive deeper into things like the naive assumption, maximum a posterior vs maximum likelihood, and decision boundaries."
  },
  {
    "objectID": "posts/bayes-classifier/bayes_classifier.html#maximum-a-posteriori",
    "href": "posts/bayes-classifier/bayes_classifier.html#maximum-a-posteriori",
    "title": "Bayesian Classifiers",
    "section": "maximum a posteriori",
    "text": "maximum a posteriori\nLet’s assume there is a random variable \\(X\\) that follows a Gaussian distribution\n\\[\nX \\sim N(\\mu, \\sigma)\n\\]\nand a variable \\(Y\\) which is discrete\n\\[\nY\\in\\{0, 1\\}\n\\]\nSuppose we know that the value of \\(Y\\) is dependent upon \\(X\\), but that the relationship is not deterministic. We can model this relationship using conditional probability\n\\[\nP(Y=y|X=x)\n\\]\nBut say we want to assign \\(Y\\) a definitive value (i.e., classify). In that case we can simply select the value of \\(Y\\) with the highest probability\n\\[\n\\arg\\max_ y P(Y|X)\n\\]\nAnd because we are selecting a value for \\(Y\\) when there is uncertainty, this means we are making an estimate. The above is known as the maximum a posteriori (MAP) estimate of \\(Y\\) given \\(X\\), and \\(P(Y|X)\\) is commonly referred to as the posterior.\nMost likely we won’t have knowledge of the posterior (\\(Y\\) is unknown afterall), so we use Bayes theorem to derive an equivalence\n\\[\nP(Y|X) = {P(Y \\cap X) \\over P(X)} = {P(X|Y) \\cdot P(Y) \\over P(X)}\n\\]\nwhere\n\n\\(P(X|Y)\\) is the likelhood (i.e., probability of the data given the class)\n\\(P(Y)\\) is the prior (i.e., probability of the class)\n\\(P(X)\\) is the marginal (i.e., probability of the data)\n\nWhen performing the MAP estimate, we are given some value of \\(X\\) and then calculate the posterior probability for each possible value of \\(Y\\). This means that the marginal is the same for all values of \\(Y\\) and is just a constant that can be factored out\n\\[\nP(Y|X) \\propto {P(X|Y) \\cdot P(Y)}\n\\]\nwhich simplifies the MAP classifier to\n\\[\n\\arg\\max_y {P(X|Y)  \\cdot P(Y)}\n\\]\nAs far as the likelihood function, we made an assumption on the distribution of \\(X\\) so we can use the Gaussian probability density function\n\\[\np(x|y) = \\frac{1}{\\sigma_y\\sqrt2\\pi} e ^ {- \\frac{1}{2} ( \\frac{x - \\mu_y}{\\sigma_y} ) ^2}\n\\]\nIf we don’t know the Gaussian parameters above, we just estimate them using the empirical mean and variance of the training data for each class which is a maximum likelihood estimate.\n\\[\n\\mu_y = \\frac{1}{n}\\sum_{i}^{n}x_i\n\\]\n\\[\n\\sigma_y^2 = \\frac{1}{n}\\sum_{i}^{n}(x_i - \\mu_y)^2\n\\]\nWe don’t know the distribution of the prior, so we have to estimate it. In practice, we simply use the prevalence of each class in the training data which is again a maximum likelihood estimate.\n\\[\np(y) = \\frac{1}{n}\\sum_{i}^{n} \\mathbb{1}(y_i = y)\n\\]\nIt’s worth noting that there is also a maximum likelihood estimate (MLE) that could be used for the classifier. As the name suggest we would just use the likelihood term and remove the prior\n\\[\n\\arg\\max_y {P(X|Y)}\n\\]\nbut this ignores the prior distribution if we have that information.\nWith some basic theory out of the way, let’s build a classifer."
  },
  {
    "objectID": "posts/bayes-classifier/bayes_classifier.html#univariate-classifier",
    "href": "posts/bayes-classifier/bayes_classifier.html#univariate-classifier",
    "title": "Bayesian Classifiers",
    "section": "univariate classifier",
    "text": "univariate classifier\nLet’s simulate univariate Gaussian data for the two classes. For simplicity, the data will have different means but the same variance.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\nsns.set()\n%matplotlib inline\n\n\nn = 1000\nmu_1 = 40\nmu_2 = 80\nstd = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std, size=n)\nx_2 = np.random.normal(loc=mu_2, scale=std, size=n)\n\ndf = pd.DataFrame({'x': np.concatenate([x_1, x_2]), 'y': [1] * n + [2] * n})\n\nsns.displot(df, kind='kde', x='x', hue='y',\n            fill=True, linewidth=0, palette='dark', alpha=0.5)\n\n\n\n\nTime to estimate priors, means, and standard deviations. This is trivial since we generated the data but let’s pretend that we didn’t :)\n\npriors = {k: df[df.y == k].size / df.size for k in df.y.unique()}\npriors\n\n{1: 0.5, 2: 0.5}\n\n\n\nmeans = {k: df[df.y == k].x.mean() for k in df.y.unique()}\nmeans\n\n{1: 39.74917237733038, 2: 80.16187136171098}\n\n\n\nstdevs = {k: df[df.y == k].x.std() for k in df.y.unique()}\n# .std(ddof=0) if not sample\nstdevs\n\n{1: 10.004638113965834, 2: 10.265729149219876}\n\n\nNow that the data is fit, we can build a classifier and predict new instances.\n\n# scipy actually has gaussian pdf: from scipy.stats import norm\n\ndef uni_gaussian_pdf(x, mean, stdev):\n    scalar = 1.0 / (stdev * np.sqrt(2 * np.pi))\n    exponential = np.exp(-0.5 * ((x - mean) / stdev) ** 2)\n    return scalar * exponential\n\nclasses = df.y.unique().tolist()  \n\ndef gbayes_uni_classify(x):\n    probas = []\n    for c in classes:\n        likelihood = uni_gaussian_pdf(x, means[c], stdevs[c])\n        # likelihood = norm.pdf(x, means[c], stdevs[c])\n        probas.append(likelihood * priors[c])\n    return classes[np.argmax(probas)]\n\nIt’s important to mention here that the priors are the same since we generated equal amounts of data for both classes. Mathematically this means that the prior is a constant and can be factored out in the original MAP equation (for this case) giving\n\\[\n\\arg\\max_y {P(X|Y)}\n\\]\nSo in the case where priors are the same, the MAP is equivalent to the MLE.\nAnd now to visualize the decision boundary.\n\nsim_data = np.arange(0, 150, 1)  # uniform sequence\nsim_class_preds = [gbayes_uni_classify(x) for x in sim_data]\n\ndecision_boundary = np.where(np.array(sim_class_preds[:-1]) - np.array(sim_class_preds[1:]) != 0)[0]\nprint(decision_boundary)\n\nsns.displot(df, kind='kde', x='x', hue='y', fill=True, linewidth=0, palette='dark', alpha=0.5)\nfor v in sim_data[decision_boundary]:\n    plt.axvline(v, color='black', linestyle='--')\n\n[59]\n\n\n\n\n\nThe decision boundary is roughly halfway between means, as expected. This isn’t super interesting but what if the variances are different?\n\nn = 1000\nmu_1 = 40\nmu_2 = 80\nstd_1 = 20\nstd_2 = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std_1, size=n)\nx_2 = np.random.normal(loc=mu_2, scale=std_2, size=n)\n\ndf = pd.DataFrame({'x': np.concatenate([x_1, x_2]), 'y': [1] * n + [2] * n})\n\nsns.displot(df, kind='kde', x='x', hue='y', fill=True, linewidth=0, palette='dark', alpha=0.5)\n\n\n\n\n\n# class so we don't repeat same spiel\n\nclass GBUniClf:\n    \n    def __init__(self):\n        self.classes = None\n        self.priors = None\n        self.means = None\n        self.stdevs = None\n        \n    def fit(self, df):\n        self.classes = df.y.unique().tolist()\n        self.priors = {k: df[df.y == k].size / df.size for k in self.classes}\n        self.means = {k: df[df.y == k].x.mean() for k in self.classes}\n        self.stdevs = {k: df[df.y == k].x.std() for k in self.classes}\n        \n    def likelihood(self, x, mean, stdev):\n        scalar = 1.0 / (stdev * np.sqrt(2 * np.pi))\n        exponential = np.exp(-0.5 * ((x - mean) / stdev) ** 2)\n        return scalar * exponential\n    \n    def predict(self, x):\n        probas = []\n        for c in self.classes:\n            likelihood = self.likelihood(x, self.means[c], self.stdevs[c])\n            probas.append(likelihood * self.priors[c])\n        return self.classes[np.argmax(probas)]\n\n\nclf = GBUniClf()\nclf.fit(df)\n\nsim_data = np.arange(-50, 200, 1)\nsim_class_preds = [clf.predict(x) for x in sim_data]\n\ndecision_boundary = np.where(np.array(sim_class_preds[:-1]) - np.array(sim_class_preds[1:]) != 0)[0]\nprint(decision_boundary)\n\ndf_preds = pd.DataFrame({'x': sim_data, 'y': sim_class_preds})\n\nsns.displot(df, kind='kde', x='x', hue='y', fill=True, linewidth=0, palette='dark', alpha=0.3)\nrug_plot = sns.rugplot(df_preds, x=\"x\", hue=\"y\", palette='dark', alpha=0.7)\nrug_plot.get_legend().remove()\nfor v in sim_data[decision_boundary]:\n    plt.axvline(v, color='black', linestyle='--')\n\n[113 174]\n\n\n\n\n\nBecause class 1 has a larger variance, there is now a second decision boundary. Instances with high values of \\(x\\) (far right) are less likely to belong to class 2 even though they are closer to its’ mean. Instead they get classified as 1.\nWhat if the priors are different?\n\nn_1 = 2000\nn_2 = 500\nmu_1 = 40\nmu_2 = 80\nstd_1 = 20\nstd_2 = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std_1, size=n_1)\nx_2 = np.random.normal(loc=mu_2, scale=std_2, size=n_2)\n\ndf = pd.DataFrame({'x': np.concatenate([x_1, x_2]), 'y': [1] * n_1 + [2] * n_2})\n\nclf = GBUniClf()\nclf.fit(df)\n\nsim_data = np.arange(-50, 200, 1)\nsim_class_preds = [clf.predict(x) for x in sim_data]\n\ndecision_boundary = np.where(np.array(sim_class_preds[:-1]) - np.array(sim_class_preds[1:]) != 0)[0]\nprint(decision_boundary)\n\ndf_preds = pd.DataFrame({'x': sim_data, 'y': sim_class_preds})\n\nsns.displot(df, kind='kde', x='x', hue='y', fill=True, linewidth=0, palette='dark', alpha=0.3)\nrug_plot = sns.rugplot(df_preds, x=\"x\", hue=\"y\", palette='dark', alpha=0.7)\nrug_plot.get_legend().remove()\nfor v in sim_data[decision_boundary]:\n    plt.axvline(v, color='black', linestyle='--')\n\n[120 164]\n\n\n\n\n\nIt simply makes the more prevalent class more likely as expected.\n\nmultivariate\nNow we can look at bivariate data where covariance between features come into play. The naive assumption ignores covariance so we can compare classifiers that do and do not make that assumption.\nMathematically, the posterior is now conditioned on multiple features\n\\[\nP(Y|X) = P(Y|x_1, x_2, \\ldots , x_i)\n\\]\nand the MAP classifier in the multivariate case is\n\\[\n\\arg\\max_y {P(Y) \\cdot P(x_1, x_2, \\ldots , x_i|Y)}\n\\]\nTherefore we use the multivariate likelihood function which makes use of covariance\n\\[\np(x|y) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma_y|}} e^{ - \\frac{1}{2} (x - \\mu_y)^T \\Sigma_y^{-1} (x - \\mu_y)}\n\\]\nThis is a drop-in replacement though, and the rest of the classifier is the same.\n\nn = 1000\nmu_1 = 40\nmu_2 = 80\nstd = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std, size=(n, 2))\nx_2 = np.random.normal(loc=mu_2, scale=std, size=(n, 2))\ndata = np.concatenate([x_1, x_2])\n\ndf = pd.DataFrame({'x1': data[:, 0], 'x2': data[:, 1], 'y': [1] * n + [2] * n})\n\n# s = sns.scatterplot(df, x='x1', y='x2', hue='y', hue_order=classes, palette='dark', alpha=0.25)\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\n\nclass GBBiClf:\n    \n    def __init__(self):\n        self.classes = None\n        self.priors = None\n        self.means = None\n        self.covars = None\n        self.covar_dets = None\n        self.covar_invs = None\n\n    def fit(self, df):\n        self.classes = df.y.unique().tolist()\n        self.priors = {k: df[df.y == k].shape[0] / df.shape[0] for k in self.classes}\n        self.means = {k: df[['x1', 'x2']][df.y == k].mean(axis=0) for k in self.classes}\n        self.covars = {k: np.cov(df[['x1', 'x2']][df.y == k], rowvar=False, bias=True) for k in self.classes}\n        self.covar_dets = {k: np.linalg.det(self.covars[k]) for k in self.classes}\n        self.covar_invs = {k: np.linalg.inv(self.covars[k]) for k in self.classes}\n\n    def likelihood(self, x, c):\n        dims = 2\n        scalar = 1.0 / np.sqrt(((2 * np.pi) ** dims) * self.covar_dets[c])\n        exponential = np.exp(-0.5 * (x - self.means[c]).T @ self.covar_invs[c] @ (x - self.means[c]))\n        return scalar * exponential\n\n    def predict(self, x):\n        probas = []\n        for c in self.classes:\n            likelihood = self.likelihood(x, c)\n            probas.append(likelihood * self.priors[c])\n        return self.classes[np.argmax(probas)]\n\n\nclf = GBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(0, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\n# sns.scatterplot(plot_df, x='x1', y=\"x2\", hue=\"y\", hue_order=classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\nThe variance was same for both distributions and the features were sampled independently, so the decision boundary isn’t complex. Slight curvature is due to the estimate of covariance which is different from the true value.\n\nclf.covars\n\n{1: array([[97.13589025,  3.99602431],\n        [ 3.99602431, 99.21513485]]),\n 2: array([[104.87159824,  -2.09309889],\n        [ -2.09309889, 102.99262222]])}\n\n\nEven though this data is uninteresting, let’s compare the decision boundary of a naive classifier. The naive assumption is that all features are independent, so we can use the chain rule of probability for a simpler calculation of the likelihood.\n\\[\nP(X|Y) = P(x_1, x_2, \\ldots , x_i|Y) = \\prod\\limits_{i}P(x_i|Y)\n\\]\nThe MAP classifier under the naive assumption then becomes\n\\[\n\\arg\\max_y {P(Y) \\cdot P(x_1|Y) \\cdot P(x_2|Y) \\cdot \\ldots \\cdot P(x_m|Y)}\n\\]\nFor this case though, since the features were generated independently, the decision boundary should be roughly the same.\n\nclass GNBBiClf:\n    \n    def __init__(self):\n        self.classes = None\n        self.priors = None\n        self.means = None\n\n    def fit(self, df):\n        self.classes = df.y.unique().tolist()\n        self.priors = {k: df[df.y == k].shape[0] / df.shape[0] for k in self.classes}\n        self.means = {k: df[['x1', 'x2']][df.y == k].mean(axis=0) for k in self.classes}\n        self.stdevs = {k: np.std(df[['x1', 'x2']][df.y == k], axis=0) for k in self.classes}\n\n    def likelihood(self, x, mean, stdev):\n        scalar = 1.0 / (stdev * np.sqrt(2 * np.pi))\n        exponential = np.exp(-0.5 * ((x - mean) / stdev) ** 2)\n        return scalar * exponential\n    \n    def predict(self, x):\n        probas = []\n        for c in self.classes:\n            joint_likelihood = 1\n            for i, v in enumerate(x):\n                likelihood = self.likelihood(v, self.means[c][i], self.stdevs[c][i])\n                joint_likelihood *= likelihood\n            probas.append(joint_likelihood * self.priors[c])\n        return self.classes[np.argmax(probas)]\n\n\nclf = GNBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(0, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\nWhat if just the covariance are different? Let’s draw random data were the features are still independent (i.e. covariance matrix is symmetic) but the variance of features is different for each class.\n\nn = 1000\nmu_1 = 40\nmu_2 = 100\nstd_1 = 20\nstd_2 = 10\n\nx_1 = np.random.normal(loc=mu_1, scale=std_1, size=(n, 2))\nx_2 = np.random.normal(loc=mu_2, scale=std_2, size=(n, 2))\ndata = np.concatenate([x_1, x_2])\n\ndf = pd.DataFrame({'x1': data[:, 0], 'x2': data[:, 1], 'y': [1] * n + [2] * n})\n\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\n\nclf = GBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(-10, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\nAs expected, the decision boundary favors class 1 since it had larger variance. Without any correlation between features, I would again expect the decision boundary to be the same for the naive classifier.\n\nclf = GNBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(-10, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\n# sanity check\n# from sklearn.naive_bayes import GaussianNB\n# clf = GaussianNB()\n# clf.fit(df[['x1', 'x2']].values, df[['y']].values.ravel())\n# sim_data_range = np.arange(-10, 140, 1)\n# sim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\n# sim_classes = [clf.predict(x.reshape(1, -1)) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\nThe decision boundary for the naive classifier is roughly identical. Zooming out, we can see the classifier has similar behavior as the univariate case for different variance.\n\nclf = GNBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(-10, 200, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 200), ylim=(-10, 200))\n\n[(-10.0, 200.0), (-10.0, 200.0)]\n\n\n\n\n\nLet’s finally simulate data with correlation between features. There should be a noticeable difference in the decision boundary for the naive classifier.\n\nn = 1000\nmu_1 = 40\nmu_2 = 100\n\nx_1 = np.random.multivariate_normal(mean=[mu_1, mu_1], cov=[[50, 70], [70, 200]], size=n)\nx_2 = np.random.multivariate_normal(mean=[mu_2, mu_2], cov=[[100, 1], [1, 100]], size=n)  # no correlation\ndata = np.concatenate([x_1, x_2])\n\ndf = pd.DataFrame({'x1': data[:, 0], 'x2': data[:, 1], 'y': [1] * n + [2] * n})\n\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\n\nclf = GBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(-10, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\n\nclf = GNBBiClf()\nclf.fit(df)\n\nsim_data_range = np.arange(-10, 140, 1)\nsim_data = np.array([np.array([x1, x2]) for x1 in sim_data_range for x2 in sim_data_range])\nsim_classes = [clf.predict(x) for x in sim_data]\n\nplot_df = pd.DataFrame(np.hstack([sim_data, np.array(sim_classes).reshape(-1, 1)]), columns=['x1', 'x2', 'y'])\n\nplt_points = sns.relplot(plot_df, x='x1', y='x2', hue='y', hue_order=clf.classes, palette='dark', marker=\".\", alpha=0.15)\nplt_points._legend.remove()\ns = sns.kdeplot(df, x='x1', y=\"x2\", hue=\"y\", palette='dark', fill=True, alpha=1)\nsns.move_legend(s, \"upper left\", bbox_to_anchor=(1, 1))\ns.set(xlim=(-10, 140), ylim=(-10, 140))\n\n[(-10.0, 140.0), (-10.0, 140.0)]\n\n\n\n\n\nThe difference between the naive and non-naive classifier is more noticeable when there is correlation between features (class 1). The naive classifier clearly ignores the covariance and the decision boundary is much smoother.\nOne obvious advantage of the naive assumption is computational efficiency. Predictions for the naive classifier ran in faster time compared to the non-naive by an order of magnitude. Fit times were roughly the same.\n\nfrom time import perf_counter\n\n# TODO memory reqs\n\nm = 100\n\nstart_time = perf_counter()\nfor i in range(m):\n    clf = GBBiClf()\n    clf.fit(df)\nprint(f\"GB avg fit time: {(perf_counter() - start_time) / m:.6f}\")\nstart_time = perf_counter()\nfor i in range(m):\n    clf.predict(sim_data[i])\nprint(f\"GB avg predict time: {(perf_counter() - start_time) / m:.6f}\")\n\nstart_time = perf_counter()\nfor i in range(m):\n    clf = GNBBiClf()\n    clf.fit(df)\nprint(f\"GNB avg fit time: {(perf_counter() - start_time) / m:.6f}\")\nstart_time = perf_counter()\nfor i in range(m):\n    clf.predict(sim_data[i])\nprint(f\"GNB avg predict time: {(perf_counter() - start_time) / m:.6f}\")\n\nGB avg fit time: 0.004047\nGB avg predict time: 0.000654\nGNB avg fit time: 0.005097\nGNB avg predict time: 0.000047"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cbhyphen",
    "section": "",
    "text": "Eager Mode Quantization in PyTorch\n\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nme\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Classifiers\n\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\nme\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/pytorch-eager-qat/pytorch_eager_qat.html",
    "href": "posts/pytorch-eager-qat/pytorch_eager_qat.html",
    "title": "Eager Mode Quantization in PyTorch",
    "section": "",
    "text": "At the time of writing this quantization in PyTorch was relatively new to me, and I wanted a deep dive on the topic for something non-trivial. After reading about different quantization modes (eager vs graph fx) as well as quantization methods (dynamic, static post-training, static aware-training), I decide to try eager mode quantization aware training (QAT).\nIn my research for a project, I came across multiple discussions online requesting either help or a tutorial for quantizing the backbone of an object detection model (faster R-CNN in this case). As far as I could tell there was nothing available so this was the perfect excuse.\nSo in this post, I will go through that process of quantization-aware training and include some analysis on the benefits of quantization.\n\nResNet and Feature Pyramid Network\nThis assumes some familiarity with the R-CNN architecture, but to refresh, the feature extraction backbone consists of two components; the resnet and the feature pyramid network. The FPN combines output from consecutive layers of the resnet (via upsampling) which allows it to extract semantic information at higher resolutions. These two components of the backbone can be quantized while the rest of the network still uses floating point precision.\nFrom an implementation standpoint, there is a utility class IntermediateLayerGetter for extracting each layer output (no fully connected) from the resnet. And another convenience class for the FPN which takes the layer ouputs as input. Combining these two is BackboneWithFPN which is mostly just a thin wrapper around both.\nAs we are doing eager mode static quantization, we’ll need to prepare the model before we can train it and subsequently quantize it.\n\n\nModel Preparation\nThe first step in preparing the network for quantization is to create a modified bottleneck block. This isn’t obvious until you try to quantize the ResNet without it. You will get an error .. out += identity .. Could not run 'aten::add.out' .. which means that PyTorch isn’t able to quantize the skip connection using the += operator in eager mode. This discussion on the pytorch forums was helpful for describing the error as well as how to fix it. The modified bottleneck block just uses FloatFunctional which has a quantized addition operator. I’m using ResNet 101 here but for much smaller networks you would want to modify the basic block. Also, the original bottleneck class reuses the ReLU layer which won’t work when fusing. Finding this blog post about quantizing ResNet was helpful for realizing and avoiding that pitfall.\n\nfrom typing import Optional, Callable\n\nimport torch.nn as nn\nfrom torch import Tensor\nfrom torch.ao.nn.quantized import FloatFunctional\n\nfrom torchvision.models.resnet import conv1x1, conv3x3\n\n\nclass BottleneckQuantizeable(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.0)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.ff_add = FloatFunctional()\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu2(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = self.ff_add.add(out, identity)\n        out = self.relu3(out)\n\n        return out\n\nNow that we have a quantizeable bottleneck, we can simply reference it when generating the ResNet. Even though the float functional operator was added, we can still load pretrained imagenet weights since the trainable submodules didn’t change. Note that the number of classes for the ResNet don’t matter here because we will extract intermediate layers and ignore the final fully connected layer.\n\nfrom torchvision.models.resnet import ResNet, ResNet50_Weights, ResNet101_Weights\n\n\ndef resnet_101():\n    resnet = ResNet(block=BottleneckQuantizeable, layers=[3, 4, 23, 3])\n    resnet.load_state_dict(ResNet101_Weights.DEFAULT.get_state_dict(progress=True))\n    return resnet\n\n\nresnet = resnet_101()\n\nThe next step would be to pass the resnet to the IntermediateLayerGetter. In addition to the resnet we created, this class also requires a dictionary of the layer names (to know what to extract). It returns an OrderedDict of those layer outputs. Here’s an example using a toy image.\n\nimport torch\nfrom torchvision.models._utils import IntermediateLayerGetter\n\n\nreturned_layers = [1, 2, 3, 4]  # get all 4 layers\nreturn_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}  # {'layer1': 0, 'layer2': 1, ...}\n\nresnet_layers = IntermediateLayerGetter(resnet, return_layers=return_layers)\n\nout = resnet_layers(torch.rand(1, 3, 200, 200))  # e.g. 200 x 200 image with 3 channels\n[(k, v.shape) for k, v in out.items()]\n\n[('0', torch.Size([1, 256, 50, 50])),\n ('1', torch.Size([1, 512, 25, 25])),\n ('2', torch.Size([1, 1024, 13, 13])),\n ('3', torch.Size([1, 2048, 7, 7]))]\n\n\nAs mentioned, the output of the resnet layers will be fed to the feature pyramid network. Before we can do this, the FPN also needs to be modified as it uses the addition operator +. There is also a functional F.interpolate that doesn’t need to be replaced, however it does need to be referenced differently as importing torch.nn.functional as F causes a namespace issue later with torchvision and QAT will fail.\n\nfrom collections import OrderedDict\nfrom typing import List, Dict\n\n# importing as 'F' causes namespace collision with torchvision and QAT fails later\n# import torch.nn.functional as F\nimport torch\n\nfrom torchvision.ops.misc import Conv2dNormActivation\nfrom torchvision.utils import _log_api_usage_once\nfrom torchvision.ops.feature_pyramid_network import ExtraFPNBlock\n\n\nclass FeaturePyramidNetworkQuantizeable(nn.Module):\n    \"\"\"\n    Module that adds a FPN from on top of a set of feature maps. This is based on\n    `\"Feature Pyramid Network for Object Detection\" &lt;https://arxiv.org/abs/1612.03144&gt;`_.\n\n    The feature maps are currently supposed to be in increasing depth\n    order.\n\n    The input to the model is expected to be an OrderedDict[Tensor], containing\n    the feature maps on top of which the FPN will be added.\n\n    Args:\n        in_channels_list (list[int]): number of channels for each feature map that\n            is passed to the module\n        out_channels (int): number of channels of the FPN representation\n        extra_blocks (ExtraFPNBlock or None): if provided, extra operations will\n            be performed. It is expected to take the fpn features, the original\n            features and the names of the original features as input, and returns\n            a new list of feature maps and their corresponding names\n        norm_layer (callable, optional): Module specifying the normalization layer to use. Default: None\n\n    Examples::\n\n        &gt;&gt;&gt; m = torchvision.ops.FeaturePyramidNetwork([10, 20, 30], 5)\n        &gt;&gt;&gt; # get some dummy data\n        &gt;&gt;&gt; x = OrderedDict()\n        &gt;&gt;&gt; x['feat0'] = torch.rand(1, 10, 64, 64)\n        &gt;&gt;&gt; x['feat2'] = torch.rand(1, 20, 16, 16)\n        &gt;&gt;&gt; x['feat3'] = torch.rand(1, 30, 8, 8)\n        &gt;&gt;&gt; # compute the FPN on top of x\n        &gt;&gt;&gt; output = m(x)\n        &gt;&gt;&gt; print([(k, v.shape) for k, v in output.items()])\n        &gt;&gt;&gt; # returns\n        &gt;&gt;&gt;   [('feat0', torch.Size([1, 5, 64, 64])),\n        &gt;&gt;&gt;    ('feat2', torch.Size([1, 5, 16, 16])),\n        &gt;&gt;&gt;    ('feat3', torch.Size([1, 5, 8, 8]))]\n\n    \"\"\"\n\n    _version = 2\n\n    def __init__(\n        self,\n        in_channels_list: List[int],\n        out_channels: int,\n        extra_blocks: Optional[ExtraFPNBlock] = None,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ):\n        super().__init__()\n        _log_api_usage_once(self)\n        self.inner_blocks = nn.ModuleList()\n        self.layer_blocks = nn.ModuleList()\n        for in_channels in in_channels_list:\n            if in_channels == 0:\n                raise ValueError(\"in_channels=0 is currently not supported\")\n            inner_block_module = Conv2dNormActivation(\n                in_channels, out_channels, kernel_size=1, padding=0, norm_layer=norm_layer, activation_layer=None\n            )\n            layer_block_module = Conv2dNormActivation(\n                out_channels, out_channels, kernel_size=3, norm_layer=norm_layer, activation_layer=None\n            )\n            self.inner_blocks.append(inner_block_module)\n            self.layer_blocks.append(layer_block_module)\n\n        # initialize parameters now to avoid modifying the initialization of top_blocks\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_uniform_(m.weight, a=1)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n        if extra_blocks is not None:\n            if not isinstance(extra_blocks, ExtraFPNBlock):\n                raise TypeError(f\"extra_blocks should be of type ExtraFPNBlock not {type(extra_blocks)}\")\n        self.extra_blocks = extra_blocks\n        self.ff_add = FloatFunctional()\n\n    def _load_from_state_dict(\n        self,\n        state_dict,\n        prefix,\n        local_metadata,\n        strict,\n        missing_keys,\n        unexpected_keys,\n        error_msgs,\n    ):\n        version = local_metadata.get(\"version\", None)\n\n        if version is None or version &lt; 2:\n            num_blocks = len(self.inner_blocks)\n            for block in [\"inner_blocks\", \"layer_blocks\"]:\n                for i in range(num_blocks):\n                    for type in [\"weight\", \"bias\"]:\n                        old_key = f\"{prefix}{block}.{i}.{type}\"\n                        new_key = f\"{prefix}{block}.{i}.0.{type}\"\n                        if old_key in state_dict:\n                            state_dict[new_key] = state_dict.pop(old_key)\n\n        super()._load_from_state_dict(\n            state_dict,\n            prefix,\n            local_metadata,\n            strict,\n            missing_keys,\n            unexpected_keys,\n            error_msgs,\n        )\n\n    def get_result_from_inner_blocks(self, x: Tensor, idx: int) -&gt; Tensor:\n        \"\"\"\n        This is equivalent to self.inner_blocks[idx](x),\n        but torchscript doesn't support this yet\n        \"\"\"\n        num_blocks = len(self.inner_blocks)\n        if idx &lt; 0:\n            idx += num_blocks\n        out = x\n        for i, module in enumerate(self.inner_blocks):\n            if i == idx:\n                out = module(x)\n        return out\n\n    def get_result_from_layer_blocks(self, x: Tensor, idx: int) -&gt; Tensor:\n        \"\"\"\n        This is equivalent to self.layer_blocks[idx](x),\n        but torchscript doesn't support this yet\n        \"\"\"\n        num_blocks = len(self.layer_blocks)\n        if idx &lt; 0:\n            idx += num_blocks\n        out = x\n        for i, module in enumerate(self.layer_blocks):\n            if i == idx:\n                out = module(x)\n        return out\n\n    def forward(self, x: Dict[str, Tensor]) -&gt; Dict[str, Tensor]:\n        \"\"\"\n        Computes the FPN for a set of feature maps.\n\n        Args:\n            x (OrderedDict[Tensor]): feature maps for each feature level.\n\n        Returns:\n            results (OrderedDict[Tensor]): feature maps after FPN layers.\n                They are ordered from the highest resolution first.\n        \"\"\"\n        # unpack OrderedDict into two lists for easier handling\n        names = list(x.keys())\n        x = list(x.values())\n\n        last_inner = self.get_result_from_inner_blocks(x[-1], -1)\n        results = []\n        results.append(self.get_result_from_layer_blocks(last_inner, -1))\n\n        for idx in range(len(x) - 2, -1, -1):\n            inner_lateral = self.get_result_from_inner_blocks(x[idx], idx)\n            feat_shape = inner_lateral.shape[-2:]\n            inner_top_down = torch.nn.functional.interpolate(last_inner, size=feat_shape, mode=\"nearest\")\n            # last_inner = inner_lateral + inner_top_down\n            last_inner = self.ff_add.add(inner_lateral, inner_top_down)\n            results.insert(0, self.get_result_from_layer_blocks(last_inner, idx))\n\n        if self.extra_blocks is not None:\n            results, names = self.extra_blocks(results, x, names)\n\n        # make it back an OrderedDict\n        out = OrderedDict([(k, v) for k, v in zip(names, results)])\n\n        return out\n\nAs you can see from the signature of the modified FPN, it also needs to know input dimensions of each layer from the resnet. There are several ways to get this but one way is to simply get the number of features in the final module of each layer.\n\n# from backbone_utils.py\n# https://github.com/pytorch/vision/blob/main/torchvision/models/detection/backbone_utils.py#L145\n# in_channels_stage2 = res101_layers.inplanes // 8\n# in_channels_list = [in_channels_stage2 * 2 ** (i - 1) for i in returned_layers]\n\nin_channels_list = []\nfor k1, m1 in resnet.named_children():\n  if 'layer' in k1:\n    in_channels_list.append((m1[-1].bn3.num_features))\n\nin_channels_list\n\n[256, 512, 1024, 2048]\n\n\nNext step is to create a modified BackboneWithFPN that uses our FeaturePyramidNetworkQuantizeable. Here we’ll also make sure that the inputs are quantized and the outputs subsequently dequantized so that they can be fed to the R-CNN.\nOne important note is that regular BatchNorm2d is the default and used here instead of FrozenBatchNorm2d. Frozen batch norm is the recommended layer because batches are generally too small for good estimates of mean and variance statistics but it isn’t quantizeable. So using regular batch norm could be unstable and less performant if those layers aren’t frozen before training.\n\nfrom torchvision.ops.feature_pyramid_network import LastLevelMaxPool\nfrom torchvision.models._utils import IntermediateLayerGetter\nfrom torch.ao.quantization import QuantStub, DeQuantStub\n\n\nclass BackboneWithFPNQuantizeable(nn.Module):\n\n    def __init__(\n        self,\n        backbone: nn.Module,\n        return_layers: Dict[str, str],\n        in_channels_list: List[int],\n        out_channels: int,\n        extra_blocks: Optional[ExtraFPNBlock] = None,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -&gt; None:\n        super().__init__()\n\n        self.quant = QuantStub()\n        self.dequant = DeQuantStub()\n\n        if extra_blocks is None:\n            extra_blocks = LastLevelMaxPool()\n\n        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n        self.fpn = FeaturePyramidNetworkQuantizeable(\n            in_channels_list=in_channels_list,\n            out_channels=out_channels,\n            extra_blocks=extra_blocks,\n            norm_layer=norm_layer\n        )\n        self.out_channels = out_channels\n\n    def forward(self, x: Tensor) -&gt; Dict[str, Tensor]:\n        x = self.quant(x)\n        x = self.body(x)\n        x = self.fpn(x)\n        for k, v in x.items():\n            x[k] = self.dequant(v)\n        return x\n\nNow we can crate the modified backbone with FPN. Once created, there should be quant/dequant stubs visible in the network like so\nBackboneWithFPNQuantizeable(\n  (quant): QuantStub()\n  (dequant): DeQuantStub()\n  (body): IntermediateLayerGetter(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): BottleneckQuantizeable(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        ...\n\n# resnet = resnet_101()\n# returned_layers = [1, 2, 3, 4]\n# return_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}\n# in_channels_list = []\n# for k1, m1 in resnet.named_children():\n#   if 'layer' in k1:\n#     in_channels_list.append((m1[-1].bn3.num_features))\n\nbbfpn = BackboneWithFPNQuantizeable(\n    backbone=resnet,\n    return_layers=return_layers,\n    in_channels_list=in_channels_list,\n    out_channels=256,\n    extra_blocks=None,\n    norm_layer=None,\n)\n# bbfpn\n\nNow the backbone with FPN can be plugged into the FasterRCNN. The number of classes is set to 2 for either object or background which is specific to the dataset used here.\n\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN\n\n\nquant_rcnn = FasterRCNN(bbfpn, num_classes=2)\n\n\n\nLayer Fusion and Quantization Config\nBefore training and subsequently converting the model, we can fuse specific sequences of modules in the backbone. Fusing compresses the model making it smaller and run faster by combining modules like Conv2d-BatchNorm2d-ReLU and Conv2d-BatchNorm2d. After fusing you should see new fused modules in the network like ConvReLU2d as well as Identity where previous modules were.\nFasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPNQuantizeable(\n    (body): IntermediateLayerGetter(\n      (conv1): ConvReLU2d(\n        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n        (1): ReLU(inplace=True)\n      )\n      (bn1): Identity()\n      (relu): Identity()\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): BottleneckQuantizeable(\n          (conv1): ConvReLU2d(\n            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n            (1): ReLU(inplace=True)\n          )\n          (bn1): Identity()\n          (relu1): Identity()\n          ...\n\nfrom torch.ao.quantization import fuse_modules\n\n\nquant_rcnn.eval()\n# fuse stem\nfuse_modules(quant_rcnn.backbone.body, [['conv1', 'bn1', 'relu']], inplace=True)\n# fuse blocks\nfor k1, m1 in quant_rcnn.backbone.body.named_children():\n    if \"layer\" in k1:  # in sequential layer with blocks\n        for k2, m2 in m1.named_children():\n            fuse_modules(m2, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\", \"relu2\"], [\"conv3\", \"bn3\"]], inplace=True)\n            for k3, m3 in m2.named_children():\n                if \"downsample\" in k3:  # fuse downsample\n                    fuse_modules(m3, [[\"0\", \"1\"]], inplace=True)\n\nBefore training the quantization config needs to be set (on the backbone only). And again, because the batches are so small, batch norm gets frozen (see this pytorch tutorial for another example). Last, I’ll freeze the stem and the first layer in the backbone since the pretrained imagenet weights were loaded. After preparation you should be able to see the observers in the network.\nFasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPNQuantizeable(\n    (body): IntermediateLayerGetter(\n      (conv1): ConvReLU2d(\n        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n        )\n        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n        )\n      )\n      (bn1): Identity()\n      ...\n\nimport re\nimport torch\nfrom torch.ao.quantization import get_default_qat_qconfig, prepare_qat\n\n\nquant_rcnn.train()\nquant_rcnn.backbone.qconfig = get_default_qat_qconfig('fbgemm')\nquant_rcnn_prepared = prepare_qat(quant_rcnn, inplace=False)\n\nquant_rcnn_prepared = quant_rcnn_prepared.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n\nquant_rcnn_prepared.backbone.body.conv1.weight.requires_grad = False\nfor name, parameter in quant_rcnn_prepared.backbone.named_parameters():\n    if re.search(r\".layer1\", name):\n        parameter.requires_grad = False\n\n\n\nDataset, Training, and Conversion\nI’ll be using the PennFudan dataset from the Torchvision object detection finetuning tutorial for QAT. Most of the code below is borrowed from that tutorial with slight modifications and no segmentation.\n\nimport os\nimport torch\n\nfrom torchvision.io import read_image\nfrom torchvision.ops.boxes import masks_to_boxes\nfrom torchvision import tv_tensors\nfrom torchvision.transforms.v2 import functional as F  # careful namespace 'F'\nfrom torchvision.transforms import v2 as T\n\n\nclass PennFudanDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = read_image(img_path)\n        mask = read_image(mask_path)\n        # instances are encoded as different colors\n        obj_ids = torch.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n        num_objs = len(obj_ids)\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n\n        # get bounding box coordinates for each mask\n        boxes = masks_to_boxes(masks)\n\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n\n        image_id = idx\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        # Wrap sample and targets into torchvision tv_tensors:\n        img = tv_tensors.Image(img)\n\n        target = {}\n        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n        target[\"masks\"] = tv_tensors.Mask(masks)\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)\n\n\ndef get_transform(train):\n    transforms = []\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    transforms.append(T.ToDtype(torch.float, scale=True))\n    transforms.append(T.ToPureTensor())\n    return T.Compose(transforms)\n\n\n%%capture\n\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")\n\n!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n!unzip PennFudanPed.zip -d ./\n\n\nimport utils\nfrom engine import train_one_epoch, evaluate\n\n\n# train on the GPU or on the CPU, if a GPU is not available\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n# use our dataset and defined transformations\ndataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\ndataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n\n# split the dataset in train and test set\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=2,\n    shuffle=True,\n    num_workers=1,\n    collate_fn=utils.collate_fn\n)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test,\n    batch_size=1,\n    shuffle=False,\n    num_workers=1,\n    collate_fn=utils.collate_fn\n)\n\n\n# move model to the right device\nquant_rcnn_prepared.to(device)\n\n# construct an optimizer\nparams = [p for p in quant_rcnn_prepared.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(\n    params,\n    lr=0.005,\n    momentum=0.9,\n    weight_decay=0.0005\n)\n\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=3,\n    gamma=0.1\n)\n\n# let's train it for 10 epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 20 iterations\n    train_one_epoch(quant_rcnn_prepared, optimizer, data_loader, device, epoch, print_freq=20)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(quant_rcnn_prepared, data_loader_test, device=device)\n\nEpoch: [0]  [ 0/60]  eta: 0:00:49  lr: 0.000090  loss: 1.5364 (1.5364)  loss_classifier: 0.8895 (0.8895)  loss_box_reg: 0.0007 (0.0007)  loss_objectness: 0.6395 (0.6395)  loss_rpn_box_reg: 0.0069 (0.0069)  time: 0.8299  data: 0.1936  max mem: 6367\nEpoch: [0]  [20/60]  eta: 0:00:25  lr: 0.001783  loss: 0.6043 (0.7210)  loss_classifier: 0.1105 (0.2343)  loss_box_reg: 0.0665 (0.0648)  loss_objectness: 0.3679 (0.3956)  loss_rpn_box_reg: 0.0270 (0.0263)  time: 0.6163  data: 0.0064  max mem: 6367\nEpoch: [0]  [40/60]  eta: 0:00:12  lr: 0.003476  loss: 0.3219 (0.5298)  loss_classifier: 0.0957 (0.1841)  loss_box_reg: 0.1185 (0.0904)  loss_objectness: 0.0582 (0.2321)  loss_rpn_box_reg: 0.0194 (0.0233)  time: 0.6057  data: 0.0056  max mem: 6367\nEpoch: [0]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2767 (0.4805)  loss_classifier: 0.0943 (0.1721)  loss_box_reg: 0.1098 (0.0993)  loss_objectness: 0.0459 (0.1849)  loss_rpn_box_reg: 0.0160 (0.0242)  time: 0.6120  data: 0.0057  max mem: 6367\nEpoch: [0] Total time: 0:00:37 (0.6183 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:23  model_time: 0.2870 (0.2870)  evaluator_time: 0.0120 (0.0120)  time: 0.4753  data: 0.1745  max mem: 6367\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2425 (0.2465)  evaluator_time: 0.0037 (0.0049)  time: 0.2517  data: 0.0030  max mem: 6367\nTest: Total time: 0:00:13 (0.2624 s / it)\nAveraged stats: model_time: 0.2425 (0.2465)  evaluator_time: 0.0037 (0.0049)\nAccumulating evaluation results...\nDONE (t=0.04s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.111\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.432\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.013\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.119\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.074\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.238\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.299\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.013\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.322\nEpoch: [1]  [ 0/60]  eta: 0:00:50  lr: 0.005000  loss: 0.2406 (0.2406)  loss_classifier: 0.0861 (0.0861)  loss_box_reg: 0.1205 (0.1205)  loss_objectness: 0.0287 (0.0287)  loss_rpn_box_reg: 0.0053 (0.0053)  time: 0.8447  data: 0.1997  max mem: 6367\nEpoch: [1]  [20/60]  eta: 0:00:25  lr: 0.005000  loss: 0.2323 (0.2870)  loss_classifier: 0.0756 (0.1012)  loss_box_reg: 0.1083 (0.1261)  loss_objectness: 0.0416 (0.0443)  loss_rpn_box_reg: 0.0130 (0.0154)  time: 0.6140  data: 0.0056  max mem: 6367\nEpoch: [1]  [40/60]  eta: 0:00:12  lr: 0.005000  loss: 0.3195 (0.3082)  loss_classifier: 0.1031 (0.1040)  loss_box_reg: 0.1658 (0.1469)  loss_objectness: 0.0288 (0.0396)  loss_rpn_box_reg: 0.0207 (0.0177)  time: 0.6341  data: 0.0056  max mem: 6983\nEpoch: [1]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2555 (0.2919)  loss_classifier: 0.0822 (0.0958)  loss_box_reg: 0.1283 (0.1443)  loss_objectness: 0.0088 (0.0319)  loss_rpn_box_reg: 0.0179 (0.0199)  time: 0.6182  data: 0.0055  max mem: 6983\nEpoch: [1] Total time: 0:00:37 (0.6288 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:24  model_time: 0.3046 (0.3046)  evaluator_time: 0.0079 (0.0079)  time: 0.4994  data: 0.1854  max mem: 6983\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2479 (0.2522)  evaluator_time: 0.0022 (0.0028)  time: 0.2579  data: 0.0033  max mem: 6983\nTest: Total time: 0:00:13 (0.2663 s / it)\nAveraged stats: model_time: 0.2479 (0.2522)  evaluator_time: 0.0022 (0.0028)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.831\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.139\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.172\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.321\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.190\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.413\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.437\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.300\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.448\nEpoch: [2]  [ 0/60]  eta: 0:00:51  lr: 0.005000  loss: 0.2004 (0.2004)  loss_classifier: 0.0508 (0.0508)  loss_box_reg: 0.1245 (0.1245)  loss_objectness: 0.0078 (0.0078)  loss_rpn_box_reg: 0.0173 (0.0173)  time: 0.8501  data: 0.1844  max mem: 6983\nEpoch: [2]  [20/60]  eta: 0:00:26  lr: 0.005000  loss: 0.2482 (0.2507)  loss_classifier: 0.0578 (0.0676)  loss_box_reg: 0.1521 (0.1533)  loss_objectness: 0.0069 (0.0085)  loss_rpn_box_reg: 0.0191 (0.0213)  time: 0.6400  data: 0.0056  max mem: 6983\nEpoch: [2]  [40/60]  eta: 0:00:12  lr: 0.005000  loss: 0.1892 (0.2265)  loss_classifier: 0.0588 (0.0633)  loss_box_reg: 0.1038 (0.1351)  loss_objectness: 0.0061 (0.0092)  loss_rpn_box_reg: 0.0143 (0.0189)  time: 0.6334  data: 0.0054  max mem: 6983\nEpoch: [2]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.1695 (0.2197)  loss_classifier: 0.0545 (0.0631)  loss_box_reg: 0.0872 (0.1279)  loss_objectness: 0.0054 (0.0102)  loss_rpn_box_reg: 0.0138 (0.0186)  time: 0.6422  data: 0.0060  max mem: 6983\nEpoch: [2] Total time: 0:00:38 (0.6448 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:23  model_time: 0.2871 (0.2871)  evaluator_time: 0.0039 (0.0039)  time: 0.4719  data: 0.1796  max mem: 6983\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2536 (0.2540)  evaluator_time: 0.0017 (0.0021)  time: 0.2615  data: 0.0031  max mem: 6983\nTest: Total time: 0:00:13 (0.2672 s / it)\nAveraged stats: model_time: 0.2536 (0.2540)  evaluator_time: 0.0017 (0.0021)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.949\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.386\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.324\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.496\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.272\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.564\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.572\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.412\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.584\nEpoch: [3]  [ 0/60]  eta: 0:00:53  lr: 0.000500  loss: 0.1430 (0.1430)  loss_classifier: 0.0370 (0.0370)  loss_box_reg: 0.0875 (0.0875)  loss_objectness: 0.0089 (0.0089)  loss_rpn_box_reg: 0.0096 (0.0096)  time: 0.8995  data: 0.2071  max mem: 6983\nEpoch: [3]  [20/60]  eta: 0:00:26  lr: 0.000500  loss: 0.1757 (0.1837)  loss_classifier: 0.0432 (0.0492)  loss_box_reg: 0.1194 (0.1130)  loss_objectness: 0.0066 (0.0068)  loss_rpn_box_reg: 0.0141 (0.0147)  time: 0.6443  data: 0.0057  max mem: 6983\nEpoch: [3]  [40/60]  eta: 0:00:13  lr: 0.000500  loss: 0.1488 (0.1730)  loss_classifier: 0.0427 (0.0477)  loss_box_reg: 0.0932 (0.1054)  loss_objectness: 0.0046 (0.0062)  loss_rpn_box_reg: 0.0106 (0.0136)  time: 0.6576  data: 0.0057  max mem: 6983\nEpoch: [3]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1673 (0.1781)  loss_classifier: 0.0469 (0.0508)  loss_box_reg: 0.0972 (0.1080)  loss_objectness: 0.0045 (0.0059)  loss_rpn_box_reg: 0.0096 (0.0134)  time: 0.6530  data: 0.0053  max mem: 6983\nEpoch: [3] Total time: 0:00:39 (0.6593 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:23  model_time: 0.2835 (0.2835)  evaluator_time: 0.0039 (0.0039)  time: 0.4670  data: 0.1781  max mem: 6983\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2530 (0.2555)  evaluator_time: 0.0013 (0.0018)  time: 0.2578  data: 0.0032  max mem: 6983\nTest: Total time: 0:00:13 (0.2685 s / it)\nAveraged stats: model_time: 0.2530 (0.2555)  evaluator_time: 0.0013 (0.0018)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.493\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.942\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.419\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.362\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.285\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.571\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.574\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.585\nEpoch: [4]  [ 0/60]  eta: 0:00:54  lr: 0.000500  loss: 0.1610 (0.1610)  loss_classifier: 0.0388 (0.0388)  loss_box_reg: 0.1063 (0.1063)  loss_objectness: 0.0052 (0.0052)  loss_rpn_box_reg: 0.0107 (0.0107)  time: 0.9067  data: 0.2171  max mem: 6983\nEpoch: [4]  [20/60]  eta: 0:00:27  lr: 0.000500  loss: 0.1496 (0.1722)  loss_classifier: 0.0469 (0.0516)  loss_box_reg: 0.0883 (0.1041)  loss_objectness: 0.0025 (0.0046)  loss_rpn_box_reg: 0.0117 (0.0120)  time: 0.6716  data: 0.0062  max mem: 6983\nEpoch: [4]  [40/60]  eta: 0:00:13  lr: 0.000500  loss: 0.1522 (0.1744)  loss_classifier: 0.0459 (0.0516)  loss_box_reg: 0.1082 (0.1082)  loss_objectness: 0.0038 (0.0043)  loss_rpn_box_reg: 0.0068 (0.0103)  time: 0.6789  data: 0.0058  max mem: 6983\nEpoch: [4]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1792 (0.1787)  loss_classifier: 0.0513 (0.0527)  loss_box_reg: 0.1076 (0.1115)  loss_objectness: 0.0029 (0.0041)  loss_rpn_box_reg: 0.0100 (0.0105)  time: 0.6734  data: 0.0055  max mem: 6983\nEpoch: [4] Total time: 0:00:40 (0.6817 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:23  model_time: 0.2894 (0.2894)  evaluator_time: 0.0040 (0.0040)  time: 0.4672  data: 0.1722  max mem: 6983\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2520 (0.2579)  evaluator_time: 0.0014 (0.0018)  time: 0.2610  data: 0.0030  max mem: 6983\nTest: Total time: 0:00:13 (0.2706 s / it)\nAveraged stats: model_time: 0.2520 (0.2579)  evaluator_time: 0.0014 (0.0018)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.528\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.952\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.546\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.338\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.542\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.293\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.605\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.615\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.463\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.626\nEpoch: [5]  [ 0/60]  eta: 0:00:52  lr: 0.000500  loss: 0.1348 (0.1348)  loss_classifier: 0.0358 (0.0358)  loss_box_reg: 0.0862 (0.0862)  loss_objectness: 0.0018 (0.0018)  loss_rpn_box_reg: 0.0110 (0.0110)  time: 0.8712  data: 0.1820  max mem: 6983\nEpoch: [5]  [20/60]  eta: 0:00:27  lr: 0.000500  loss: 0.1626 (0.1758)  loss_classifier: 0.0444 (0.0485)  loss_box_reg: 0.0996 (0.1133)  loss_objectness: 0.0029 (0.0039)  loss_rpn_box_reg: 0.0098 (0.0101)  time: 0.6826  data: 0.0058  max mem: 6983\nEpoch: [5]  [40/60]  eta: 0:00:13  lr: 0.000500  loss: 0.1343 (0.1651)  loss_classifier: 0.0395 (0.0468)  loss_box_reg: 0.0837 (0.1052)  loss_objectness: 0.0023 (0.0037)  loss_rpn_box_reg: 0.0089 (0.0095)  time: 0.6910  data: 0.0059  max mem: 7287\nEpoch: [5]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1644 (0.1699)  loss_classifier: 0.0499 (0.0484)  loss_box_reg: 0.0954 (0.1076)  loss_objectness: 0.0026 (0.0038)  loss_rpn_box_reg: 0.0098 (0.0100)  time: 0.6861  data: 0.0062  max mem: 7287\nEpoch: [5] Total time: 0:00:41 (0.6929 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:24  model_time: 0.3069 (0.3069)  evaluator_time: 0.0041 (0.0041)  time: 0.4943  data: 0.1818  max mem: 7287\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2673 (0.2650)  evaluator_time: 0.0014 (0.0018)  time: 0.2721  data: 0.0030  max mem: 7287\nTest: Total time: 0:00:13 (0.2780 s / it)\nAveraged stats: model_time: 0.2673 (0.2650)  evaluator_time: 0.0014 (0.0018)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.560\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.957\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.602\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.451\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.570\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.311\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.644\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.647\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.550\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.655\nEpoch: [6]  [ 0/60]  eta: 0:00:55  lr: 0.000050  loss: 0.1775 (0.1775)  loss_classifier: 0.0405 (0.0405)  loss_box_reg: 0.1216 (0.1216)  loss_objectness: 0.0029 (0.0029)  loss_rpn_box_reg: 0.0125 (0.0125)  time: 0.9273  data: 0.2030  max mem: 7287\nEpoch: [6]  [20/60]  eta: 0:00:28  lr: 0.000050  loss: 0.1606 (0.1758)  loss_classifier: 0.0500 (0.0530)  loss_box_reg: 0.0945 (0.1087)  loss_objectness: 0.0032 (0.0039)  loss_rpn_box_reg: 0.0090 (0.0102)  time: 0.6964  data: 0.0055  max mem: 7287\nEpoch: [6]  [40/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1137 (0.1597)  loss_classifier: 0.0357 (0.0480)  loss_box_reg: 0.0660 (0.0997)  loss_objectness: 0.0018 (0.0034)  loss_rpn_box_reg: 0.0057 (0.0086)  time: 0.6934  data: 0.0057  max mem: 7287\nEpoch: [6]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1633 (0.1669)  loss_classifier: 0.0410 (0.0493)  loss_box_reg: 0.0954 (0.1050)  loss_objectness: 0.0024 (0.0035)  loss_rpn_box_reg: 0.0081 (0.0091)  time: 0.6901  data: 0.0058  max mem: 7287\nEpoch: [6] Total time: 0:00:41 (0.6998 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:24  model_time: 0.3050 (0.3050)  evaluator_time: 0.0035 (0.0035)  time: 0.4857  data: 0.1758  max mem: 7287\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2615 (0.2624)  evaluator_time: 0.0013 (0.0018)  time: 0.2673  data: 0.0031  max mem: 7287\nTest: Total time: 0:00:13 (0.2757 s / it)\nAveraged stats: model_time: 0.2615 (0.2624)  evaluator_time: 0.0013 (0.0018)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.537\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.948\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.534\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.438\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.547\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.294\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.628\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.642\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.550\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.649\nEpoch: [7]  [ 0/60]  eta: 0:00:55  lr: 0.000050  loss: 0.0931 (0.0931)  loss_classifier: 0.0252 (0.0252)  loss_box_reg: 0.0566 (0.0566)  loss_objectness: 0.0020 (0.0020)  loss_rpn_box_reg: 0.0093 (0.0093)  time: 0.9269  data: 0.2053  max mem: 7287\nEpoch: [7]  [20/60]  eta: 0:00:28  lr: 0.000050  loss: 0.1182 (0.1405)  loss_classifier: 0.0339 (0.0406)  loss_box_reg: 0.0713 (0.0877)  loss_objectness: 0.0017 (0.0043)  loss_rpn_box_reg: 0.0049 (0.0079)  time: 0.6926  data: 0.0066  max mem: 7462\nEpoch: [7]  [40/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1324 (0.1550)  loss_classifier: 0.0367 (0.0454)  loss_box_reg: 0.0773 (0.0970)  loss_objectness: 0.0020 (0.0038)  loss_rpn_box_reg: 0.0087 (0.0088)  time: 0.6966  data: 0.0055  max mem: 7462\nEpoch: [7]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1809 (0.1661)  loss_classifier: 0.0493 (0.0488)  loss_box_reg: 0.1123 (0.1040)  loss_objectness: 0.0038 (0.0042)  loss_rpn_box_reg: 0.0103 (0.0092)  time: 0.6901  data: 0.0055  max mem: 7462\nEpoch: [7] Total time: 0:00:42 (0.7001 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:24  model_time: 0.2994 (0.2994)  evaluator_time: 0.0039 (0.0039)  time: 0.4838  data: 0.1787  max mem: 7462\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2554 (0.2593)  evaluator_time: 0.0013 (0.0017)  time: 0.2629  data: 0.0029  max mem: 7462\nTest: Total time: 0:00:13 (0.2721 s / it)\nAveraged stats: model_time: 0.2554 (0.2593)  evaluator_time: 0.0013 (0.0017)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.530\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.951\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.491\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.451\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.540\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.283\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.616\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.618\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.562\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.623\nEpoch: [8]  [ 0/60]  eta: 0:00:58  lr: 0.000050  loss: 0.3004 (0.3004)  loss_classifier: 0.0767 (0.0767)  loss_box_reg: 0.2012 (0.2012)  loss_objectness: 0.0051 (0.0051)  loss_rpn_box_reg: 0.0173 (0.0173)  time: 0.9718  data: 0.2255  max mem: 7462\nEpoch: [8]  [20/60]  eta: 0:00:28  lr: 0.000050  loss: 0.1537 (0.1833)  loss_classifier: 0.0469 (0.0556)  loss_box_reg: 0.0927 (0.1127)  loss_objectness: 0.0018 (0.0034)  loss_rpn_box_reg: 0.0098 (0.0116)  time: 0.7003  data: 0.0058  max mem: 7462\nEpoch: [8]  [40/60]  eta: 0:00:14  lr: 0.000050  loss: 0.1291 (0.1668)  loss_classifier: 0.0407 (0.0491)  loss_box_reg: 0.0915 (0.1050)  loss_objectness: 0.0029 (0.0032)  loss_rpn_box_reg: 0.0055 (0.0094)  time: 0.6929  data: 0.0056  max mem: 7462\nEpoch: [8]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1634 (0.1668)  loss_classifier: 0.0514 (0.0498)  loss_box_reg: 0.1034 (0.1045)  loss_objectness: 0.0024 (0.0032)  loss_rpn_box_reg: 0.0083 (0.0094)  time: 0.6999  data: 0.0056  max mem: 7462\nEpoch: [8] Total time: 0:00:42 (0.7050 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:24  model_time: 0.3030 (0.3030)  evaluator_time: 0.0037 (0.0037)  time: 0.4906  data: 0.1825  max mem: 7462\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2565 (0.2625)  evaluator_time: 0.0012 (0.0017)  time: 0.2623  data: 0.0030  max mem: 7462\nTest: Total time: 0:00:13 (0.2753 s / it)\nAveraged stats: model_time: 0.2565 (0.2625)  evaluator_time: 0.0012 (0.0017)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.534\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.951\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.526\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.293\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.550\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.289\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.618\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.619\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.450\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.632\nEpoch: [9]  [ 0/60]  eta: 0:00:57  lr: 0.000005  loss: 0.2058 (0.2058)  loss_classifier: 0.0578 (0.0578)  loss_box_reg: 0.1311 (0.1311)  loss_objectness: 0.0029 (0.0029)  loss_rpn_box_reg: 0.0140 (0.0140)  time: 0.9631  data: 0.2252  max mem: 7462\nEpoch: [9]  [20/60]  eta: 0:00:28  lr: 0.000005  loss: 0.1616 (0.1713)  loss_classifier: 0.0448 (0.0518)  loss_box_reg: 0.0901 (0.1049)  loss_objectness: 0.0042 (0.0042)  loss_rpn_box_reg: 0.0101 (0.0105)  time: 0.7072  data: 0.0055  max mem: 7462\nEpoch: [9]  [40/60]  eta: 0:00:14  lr: 0.000005  loss: 0.1449 (0.1669)  loss_classifier: 0.0415 (0.0500)  loss_box_reg: 0.1006 (0.1033)  loss_objectness: 0.0021 (0.0038)  loss_rpn_box_reg: 0.0069 (0.0098)  time: 0.7147  data: 0.0056  max mem: 7462\nEpoch: [9]  [59/60]  eta: 0:00:00  lr: 0.000005  loss: 0.1712 (0.1657)  loss_classifier: 0.0517 (0.0498)  loss_box_reg: 0.0998 (0.1030)  loss_objectness: 0.0019 (0.0034)  loss_rpn_box_reg: 0.0084 (0.0094)  time: 0.7149  data: 0.0065  max mem: 7462\nEpoch: [9] Total time: 0:00:43 (0.7191 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:24  model_time: 0.3057 (0.3057)  evaluator_time: 0.0038 (0.0038)  time: 0.4838  data: 0.1729  max mem: 7462\nTest:  [49/50]  eta: 0:00:00  model_time: 0.2585 (0.2639)  evaluator_time: 0.0013 (0.0018)  time: 0.2670  data: 0.0030  max mem: 7462\nTest: Total time: 0:00:13 (0.2766 s / it)\nAveraged stats: model_time: 0.2585 (0.2639)  evaluator_time: 0.0013 (0.0018)\nAccumulating evaluation results...\nDONE (t=0.02s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.535\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.950\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.550\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.550\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.297\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.624\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.634\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.475\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.646\n\n\nNow to convert and save the model. Make sure to put the model on CPU before conversion or you will get an error. After conversion you should see quantized modules like QuantizedConvReLU2d.\nFasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPNQuantizeable(\n    (quant): Quantize(scale=tensor([0.0375]), zero_point=tensor([57]), dtype=torch.quint8)\n    (dequant): DeQuantize()\n    (body): IntermediateLayerGetter(\n      (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.2553767263889313, zero_point=0, padding=(3, 3))\n      (bn1): Identity()\n      (relu): Identity()\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): BottleneckQuantizeable(\n          (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.12423195689916611, zero_point=0)\n          (bn1): Identity()\n          (relu1): Identity()\n          ...\n\nfrom torch.ao.quantization import convert\n\n\nquant_rcnn_prepared.eval()\nquant_rcnn_prepared.to(torch.device('cpu'))\n\nquant_rcnn_converted = convert(quant_rcnn_prepared, inplace=False)\n\nquant_model_path = \"/content/quant_model.pth\"\ntorch.save(quant_rcnn_converted.state_dict(), quant_model_path)\n\nFor comparison I’ll generate the same network without any modifications made for quantization (including fusion). Then we can compare model sizes and latency. Note that this is just comparing latency on the CPU, if the float model was on GPU it could be significantly faster depending upon the hardware.\n\n%%capture\nfrom torchvision.models.detection.backbone_utils import BackboneWithFPN\nfrom torchvision.models.resnet import Bottleneck\n\n\nresnet_bb = resnet_101()\nrcnn = FasterRCNN(\n    BackboneWithFPN(\n        backbone=resnet_bb,\n        return_layers=return_layers,\n        in_channels_list=in_channels_list,\n        out_channels=256,\n        extra_blocks=None,\n        norm_layer=None,\n        ),\n    num_classes=2\n)\n\nrcnn.eval()\nrcnn.to(torch.device('cpu'))\nmodel_path = \"/content/float_model.pth\"\ntorch.save(rcnn.state_dict(), model_path)\n\n\nprint(f'size of quantized model: {round(os.path.getsize(\"/content/quant_model.pth\") / 1e6)} MB')\nprint(f'size of float model: {round(os.path.getsize(\"/content/float_model.pth\") / 1e6)} MB')\n\nsize of quantized model: 105 MB\nsize of float model: 242 MB\n\n\n\nfrom time import perf_counter\n\n\nquant_rcnn_converted.to(torch.device('cpu'))\n# just grab one test image/batch\nimages, targets = next(iter(data_loader_test))\nimages = list(img.to(torch.device('cpu')) for img in images)\nn = 10\n\nstart = perf_counter()\nfor _ in range(n):\n    __ = quant_rcnn_converted(images)\nprint(f\"quant model avg time: {(perf_counter() - start) / n:.2f}\")\n\nstart = perf_counter()\nfor _ in range(n):\n    __ = rcnn(images)\nprint(f\"float model avg time (cpu): {(perf_counter() - start) / n:.2f}\")\n\nquant model avg time: 1.42\nfloat model avg time (cpu): 2.20\n\n\nI believe a fully quantized model would be even smaller and faster by comparison. In this case, while we did quantize the backbone for the RCNN, it only accounted for roughly 75% of the model parameters. So a significant number of float operations still occur after the quantized backbone.\n\nnum_model_params = sum(p.numel() for p in rcnn.parameters() if p.requires_grad)\nnum_backbone_params = sum(p.numel() for p in rcnn.backbone.parameters() if p.requires_grad)\n\nprint(f\"total number of parameters in model: {num_model_params}\")\nprint(f\"total number of parameters in backbone: {num_backbone_params}\")\nprint(f\"ratio of quantized parameters: {num_backbone_params / num_model_params:.2f}\")\n\ntotal number of parameters in model: 60344409\ntotal number of parameters in backbone: 45844544\nratio of quantized parameters: 0.76\n\n\nWe can also profile each model to see where each spends the most time during a forward pass.\n\nfrom torch.profiler import profile, record_function, ProfilerActivity\n\n\nwith profile(activities=[ProfilerActivity.CPU], record_shapes=False) as prof:\n    with record_function(\"model_inference\"):\n        quant_rcnn_converted(images)\n\nprint(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n\nwith profile(activities=[ProfilerActivity.CPU], record_shapes=False) as prof:\n    with record_function(\"model_inference\"):\n        rcnn(images)\n\nprint(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n\n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                  model_inference         2.15%      29.904ms       100.00%        1.388s        1.388s             1  \n           quantized::conv2d_relu        26.61%     369.449ms        26.96%     374.209ms       5.585ms            67  \n                quantized::conv2d        23.33%     323.889ms        23.44%     325.335ms       7.230ms            45  \n           torchvision::roi_align        17.54%     243.538ms        20.78%     288.475ms      72.119ms             4  \n                     aten::conv2d         0.00%      64.000us        13.45%     186.652ms      12.443ms            15  \n                aten::convolution         0.03%     400.000us        13.44%     186.588ms      12.439ms            15  \n               aten::_convolution         0.02%     280.000us        13.41%     186.188ms      12.413ms            15  \n         aten::mkldnn_convolution        13.25%     183.875ms        13.29%     184.541ms      12.303ms            15  \n                     aten::linear         0.01%     115.000us         5.54%      76.905ms      19.226ms             4  \n                      aten::addmm         5.50%      76.300ms         5.52%      76.641ms      19.160ms             4  \n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 1.388s\n\n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                  model_inference         3.80%      79.209ms       100.00%        2.086s        2.086s             1  \n                     aten::conv2d         0.04%     759.000us        66.00%        1.377s      10.841ms           127  \n                aten::convolution         0.14%       3.015ms        65.96%        1.376s      10.835ms           127  \n               aten::_convolution         0.11%       2.234ms        65.82%        1.373s      10.811ms           127  \n         aten::mkldnn_convolution        65.44%        1.365s        65.71%        1.371s      10.793ms           127  \n           torchvision::roi_align        14.10%     294.166ms        14.37%     299.835ms      59.967ms             5  \n                 aten::batch_norm         0.02%     462.000us         3.95%      82.490ms     793.173us           104  \n     aten::_batch_norm_impl_index         0.05%     948.000us         3.93%      82.028ms     788.731us           104  \n          aten::native_batch_norm         3.73%      77.820ms         3.87%      80.832ms     777.231us           104  \n                     aten::linear         0.00%      41.000us         3.54%      73.933ms      18.483ms             4  \n---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 2.086s\n\n\n\nThe following loads the saved quantized model. It’s important that the same process of fusing, preparing, and converting be done before loading weights since quantization significantly alters the network. For sake of completeness, we can look at a prediction from the partially quantized RCNN.\n\n%%capture\n\nquant_model_loaded = FasterRCNN(\n    BackboneWithFPNQuantizeable(\n        backbone=resnet_101(),\n        return_layers=return_layers,\n        in_channels_list=in_channels_list,\n        out_channels=256,\n        extra_blocks=None,\n        norm_layer=None\n        ),\n    num_classes=2\n    )\n\nquant_model_loaded.eval()\nfuse_modules(quant_model_loaded.backbone.body, [['conv1', 'bn1', 'relu']], inplace=True)\nfor k1, m1 in quant_model_loaded.backbone.body.named_children():\n    if \"layer\" in k1:  # in sequential layer with blocks\n        for k2, m2 in m1.named_children():\n            fuse_modules(m2, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\", \"relu2\"], [\"conv3\", \"bn3\"]], inplace=True)\n            for k3, m3 in m2.named_children():\n                if \"downsample\" in k3:  # fuse downsample\n                    fuse_modules(m3, [[\"0\", \"1\"]], inplace=True)\n\nquant_model_loaded.train()\nquant_model_loaded.backbone.qconfig = torch.quantization.get_default_qconfig('fbgemm')\ntorch.quantization.prepare_qat(quant_model_loaded, inplace=True)\ntorch.quantization.convert(quant_model_loaded, inplace=True)\n\nquant_model_loaded.eval()\nquant_model_loaded.load_state_dict(torch.load(quant_model_path, map_location=torch.device('cpu')))\n\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n\n\nimage = read_image(\"PennFudanPed/PNGImages/FudanPed00022.png\")  # 7, 22\neval_transform = get_transform(train=False)\n\nwith torch.no_grad():\n    x = eval_transform(image)\n    # convert RGBA -&gt; RGB and move to device\n    x = x[:3, ...].to(torch.device('cpu'))\n    predictions = quant_model_loaded([x, ])\n    pred = predictions[0]\n\nthreshold = 0.50\nimage = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\nimage = image[:3, ...]\npred_labels = [f\"pedestrian: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"]) if score &gt; threshold]\npred_boxes = pred[\"boxes\"].long()[pred[\"scores\"] &gt; threshold]\n\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n\n# masks = (pred[\"masks\"] &gt; 0.7).squeeze(1)\n# output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))\n\n&lt;matplotlib.image.AxesImage at 0x781f0969da80&gt;\n\n\n\n\n\nIt’s worth mentioning that I ran into all sorts of issues on my early attempts. This post is polished and makes the whole process look linear but it wasn’t. There were many attempts, breaks to research bugs, figuring out how to do something, reverting to simpler models, etc. Things failed and failed until finally they didn’t."
  }
]