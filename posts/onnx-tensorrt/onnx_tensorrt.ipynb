{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "title: \"Speeding up Inference with Onnx and TensorRT\"\n",
        "author: \"chris\"\n",
        "date: 2024-05-27\n",
        "draft: false\n",
        "---"
      ],
      "metadata": {
        "id": "A9dd8Ap9hTgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "follow up post to pytorch quantization ... can we make it faster with GPU and TensorRT"
      ],
      "metadata": {
        "id": "BT5j7sb7gPJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "get FasterRCNN as before with a resnet101 backbone..."
      ],
      "metadata": {
        "id": "QDiHrmX7GOtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import torch\n",
        "from torchvision.models.resnet import ResNet, Bottleneck, ResNet101_Weights\n",
        "from torchvision.models._utils import IntermediateLayerGetter\n",
        "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
        "\n",
        "\n",
        "def resnet_101():\n",
        "    resnet = ResNet(block=Bottleneck, layers=[3, 4, 23, 3])\n",
        "    resnet.load_state_dict(ResNet101_Weights.DEFAULT.get_state_dict(progress=True))\n",
        "    return resnet\n",
        "\n",
        "\n",
        "resnet = resnet_101()\n",
        "\n",
        "# same as before, get intermediate layers and their output dimensions\n",
        "returned_layers = [1, 2, 3, 4]\n",
        "return_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}\n",
        "in_channels_list = []\n",
        "for k1, m1 in resnet.named_children():\n",
        "  if 'layer' in k1:\n",
        "    in_channels_list.append((m1[-1].bn3.num_features))\n",
        "\n",
        "rcnn = FasterRCNN(\n",
        "    BackboneWithFPN(\n",
        "        backbone=resnet,\n",
        "        return_layers=return_layers,\n",
        "        in_channels_list=in_channels_list,\n",
        "        out_channels=256,\n",
        "        extra_blocks=None,\n",
        "        norm_layer=None,\n",
        "        ),\n",
        "    num_classes=2\n",
        ")\n",
        "\n",
        "rcnn.eval()"
      ],
      "metadata": {
        "id": "d4JoXjgZyB_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "time the RCNN on both CPU and GPU.  I don't recall what the specs were the last time I used colab to profile the inference time so I'll document that here as well.  I'm using a T4 GPU and the following CPU"
      ],
      "metadata": {
        "id": "CpEzZqORGVmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !cat /proc/cpuinfo  | grep 'name' | uniq\n",
        "!lscpu | grep 'name'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OLr1uWrGsct",
        "outputId": "97a14e4d-cd2f-448c-abb1-d620fb5814fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEOPFQatLmMy",
        "outputId": "4cf2360b-f073-4d64-ac36-e5dd8c478b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-8e4bcaf6-e88c-6df2-f337-32e33d96a494)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# random image\n",
        "image = torch.rand(3, 200, 200)\n",
        "# put on CPU\n",
        "rcnn.to(torch.device('cpu'))\n",
        "image_cpu = image.to(torch.device('cpu'))\n",
        "\n",
        "with torch.no_grad():\n",
        "    cpu_time = %timeit -o rcnn([image_cpu])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwJ7sfCI7sjl",
        "outputId": "dc6685f2-08a1-408b-93f0-2bba090baa95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.86 s ± 190 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "# on GPU\n",
        "rcnn_gpu = deepcopy(rcnn).to(torch.device('cuda'))\n",
        "# rcnn.to(torch.device('cuda'))\n",
        "image_gpu = image.to(torch.device('cuda'))\n",
        "\n",
        "with torch.no_grad():\n",
        "    gpu_time = %timeit -o rcnn_gpu([image_gpu])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ksm0gZ6p72xB",
        "outputId": "1456e86b-ddbe-4b50-c747-f6d46bb1e1d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98.8 ms ± 260 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can also test with half precision..."
      ],
      "metadata": {
        "id": "bg5cr_ZWNu1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rcnn_gpu_half = rcnn_gpu.half().to(torch.device('cuda'))\n",
        "input_half = image_gpu.half()\n",
        "\n",
        "with torch.no_grad():\n",
        "    gpu_half_time = %timeit -o rcnn_gpu_half([input_half])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmnqW54iM_l7",
        "outputId": "22d6676e-9f9d-4160-f539-415c2bb8edde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42.7 ms ± 168 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "also re-clock the quantized model using FX Graph Mode since it's performance is also CPU specific"
      ],
      "metadata": {
        "id": "1u_V_fCuFmLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from torch.ao.quantization import quantize_fx\n",
        "from torch.ao.quantization.qconfig_mapping import get_default_qconfig_mapping\n",
        "\n",
        "\n",
        "quant_rcnn = deepcopy(rcnn)\n",
        "\n",
        "qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")  # \"qnnpack\"\n",
        "# assume calibrated already\n",
        "quant_rcnn.eval()\n",
        "quant_rcnn.to(torch.device('cpu'))\n",
        "# prepare and quantize\n",
        "example_input = torch.randn(1, 3, 200, 200)\n",
        "quant_rcnn.backbone = quantize_fx.prepare_fx(quant_rcnn.backbone, qconfig_mapping, example_input)\n",
        "quant_rcnn.backbone = quantize_fx.convert_fx(quant_rcnn.backbone)\n",
        "\n",
        "script_module = torch.jit.script(quant_rcnn)\n",
        "script_module.save(\"./quant_rcnn.pt\")\n",
        "quant_rcnn_jit = torch.jit.load(\"./quant_rcnn.pt\", map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "wbJ0CJBaBmav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# warmup\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore')\n",
        "    for _ in range(3):\n",
        "        __ = quant_rcnn_jit([image_cpu])\n",
        "\n",
        "with torch.no_grad():\n",
        "    quant_time = %timeit -o quant_rcnn_jit([image_cpu])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuUDsHb9Dx0c",
        "outputId": "97608558-e3e9-48af-ff09-366282fec91d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.35 s ± 223 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert float model to onnx..."
      ],
      "metadata": {
        "id": "PJGIDCMxH6MU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "!pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak4nk7AmzI6h",
        "outputId": "69d2a860-f0a0-4a8c-8b67-0f13a4bf6a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.16.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.18.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "# onnx runs on cpu\n",
        "rcnn.to(torch.device('cpu'))\n",
        "# hack:\n",
        "# onnx wants a tuple of 2 or bombs, but for some reason is ok with a none type\n",
        "# known issue https://github.com/zhiqwang/yolort/issues/485\n",
        "torch.onnx.export(rcnn, ([image], None), \"rcnn.onnx\", opset_version = 11)\n",
        "# make sure the onnx proto is valid\n",
        "rcnn_onnx = onnx.load(\"rcnn.onnx\")\n",
        "onnx.checker.check_model(rcnn_onnx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeVuNMi685OO",
        "outputId": "101146a3-e9d8-40ac-fb99-96c2d7e4a2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4009: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  (torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i], dtype=torch.float32)).float()))\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/ops/boxes.py:166: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/ops/boxes.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1559: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert condition, message\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/transform.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/transform.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  / torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset9.py:5858: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "run inference on onnx model, make sure outputs are as expected, then clock-it..."
      ],
      "metadata": {
        "id": "zBYYNDkcVeVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "ort_session = onnxruntime.InferenceSession(\"rcnn.onnx\", providers=[\"CPUExecutionProvider\"])\n",
        "# good to make sure inputs are as expected with: 'ort_session.get_inputs()'\n",
        "\n",
        "# onnx wants numpy tensor not torch tensor\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "# get a prediction.  note onnx doesn't need a list input like torch model did...\n",
        "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(image)}\n",
        "ort_outs = ort_session.run(None, ort_inputs)"
      ],
      "metadata": {
        "id": "1sM1p_37QgFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# onxx outputs are list of three arrays corresponding to 'boxes', 'labels', and 'scores'\n",
        "print(\"onnx out shapes: \", [arr.shape for arr in ort_outs])\n",
        "# quant model out is tuple of (losses, outputs)\n",
        "torch_outs = __[1][0]\n",
        "print(\"torch out shapes: \", [torch_outs[k].shape for k in torch_outs])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9Oy1RW0VUnm",
        "outputId": "f0a344b8-1532-437b-c0eb-12e69730fdd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "onnx out shapes:  [(100, 4), (100,), (100,)]\n",
            "torch out shapes:  [torch.Size([100, 4]), torch.Size([100]), torch.Size([100])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_time = %timeit -o ort_session.run(None, ort_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSUs1zDVWdVt",
        "outputId": "b4a01af1-742b-4304-9eee-dee67b159249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.62 s ± 148 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert from onnx to tensorRT..."
      ],
      "metadata": {
        "id": "muQk6vpsXMQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO ...\n",
        "# import tensorrt"
      ],
      "metadata": {
        "id": "GFryu0M3zI1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#| code-fold: true\n",
        "# plot latency for all methods (bar chart)"
      ],
      "metadata": {
        "id": "22ODTOhgdl_s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}