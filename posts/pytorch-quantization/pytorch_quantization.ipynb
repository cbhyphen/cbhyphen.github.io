{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d70e73cb",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"quantizing explorations in pytorch\"\n",
    "description: \"quantization...\"\n",
    "author: \"me\"\n",
    "date: 2023-11-01\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0064702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 3.11 torch 2.1 torchvision 0.16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62af8033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo simple QAT\n",
    "# https://pytorch.org/docs/stable/quantization.html#quantization-aware-training-for-static-quantization\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ba2bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a floating point model where some layers could benefit from QAT\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 1, 1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(1)\n",
    "        # self.bn1 = torchvision.ops.FrozenBatchNorm2d(1)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        # add another layer for perf comparison\n",
    "        self.conv2 = torch.nn.Conv2d(1, 1, 1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(1)\n",
    "        # self.bn2 = torchvision.ops.FrozenBatchNorm2d(1)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11a089d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub()\n",
       "  (conv1): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a model instance\n",
    "model_fp32 = M()\n",
    "model_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96c52374",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = deepcopy(model_fp32)\n",
    "\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')  # QAT config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d91aa27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub()\n",
       "  (conv1): ConvReLU2d(\n",
       "    (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (bn1): Identity()\n",
       "  (relu1): Identity()\n",
       "  (conv2): ConvReLU2d(\n",
       "    (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (bn2): Identity()\n",
       "  (relu2): Identity()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model must be set to eval for fusion to work\n",
    "model.eval()\n",
    "\n",
    "# fusions: [Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]\n",
    "torch.quantization.fuse_modules(model, [['conv1', 'bn1', 'relu1'], ['conv2', 'bn2', 'relu2']], inplace=True)\n",
    "model  # note 'Identity()'' where 'bn' and 'relu' modules were"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65fe3a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (conv1): QuantizedConvReLU2d(1, 1, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
       "  (bn1): Identity()\n",
       "  (relu1): Identity()\n",
       "  (conv2): QuantizedConvReLU2d(1, 1, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
       "  (bn2): Identity()\n",
       "  (relu2): Identity()\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# back to train for QAT\n",
    "model.train()\n",
    "\n",
    "torch.quantization.prepare_qat(model, inplace=True)\n",
    "\n",
    "# train ...\n",
    "\n",
    "model.eval()\n",
    "torch.quantization.convert(model, inplace=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56fb83b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float model avg time: 0.000531349700000078\n",
      "quant model avg time: 0.00043306860000006964\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "n = 1000\n",
    "\n",
    "start = perf_counter()\n",
    "for _ in range(n):\n",
    "    model_fp32(torch.rand(8, 1, 32, 32))\n",
    "print(f\"float model avg time: {(perf_counter() - start) / n}\")\n",
    "\n",
    "start = perf_counter()\n",
    "for _ in range(n):\n",
    "    model(torch.rand(8, 1, 32, 32))\n",
    "print(f\"quant model avg time: {(perf_counter() - start) / n}\")\n",
    "\n",
    "# roughly 66%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2b458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fc3d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08196d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4d10e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPH FX MODE QUANTIZATION !!!\n",
    "# https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html\n",
    "# post-training-static\n",
    "# can transform syntax '+=' to proper quantized method which normally causes eager mode to fail\n",
    "# need pytorch 1.11+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "caa0482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "resnet_18 = resnet18()\n",
    "\n",
    "dataset = datasets.CIFAR10(root=\"data\", download=False, train=False, transform=transforms.ToTensor())  # smaller test data\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "926e077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization import get_default_qconfig\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import QConfigMapping\n",
    "\n",
    "resnet_18.eval()\n",
    "# The old 'fbgemm' is still available but 'x86' is the recommended default.\n",
    "qconfig = get_default_qconfig(\"fbgemm\")\n",
    "qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "\n",
    "def calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            model(image)\n",
    "            \n",
    "example_inputs = (next(iter(data_loader))[0]) # get an example input\n",
    "prepared_model = prepare_fx(resnet_18, qconfig_mapping, example_inputs)  # fuse modules and insert observers\n",
    "calibrate(prepared_model, data_loader)  # run calibration on sample data\n",
    "quantized_model = convert_fx(prepared_model)  # convert the calibrated model to a quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "6a6ad187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quant model avg time: 0.005655732270001317\n",
      "float model avg time: 0.00878128740999964\n"
     ]
    }
   ],
   "source": [
    "# compare with original resnet\n",
    "\n",
    "resnet_18_original = resnet18()\n",
    "resnet_18_original.eval()\n",
    "\n",
    "n = 1\n",
    "\n",
    "start = perf_counter()\n",
    "for _ in range(n):\n",
    "    for img, label in data_loader:\n",
    "        resnet_18_original(img)\n",
    "print(f\"float model avg time: {(perf_counter() - start) / (n * len(data_loader))}\")\n",
    "\n",
    "start = perf_counter()\n",
    "for _ in range(n):\n",
    "    for img, label in data_loader:\n",
    "        quantized_model(img)\n",
    "print(f\"quant model avg time: {(perf_counter() - start) / (n * len(data_loader))}\")\n",
    "\n",
    "# roughly 66%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "d7f9c027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with torch jit\n",
    "\n",
    "torch.jit.save(torch.jit.script(quantized_model), \"./data/quant_jit_model.pth\")\n",
    "\n",
    "quantized_jit_model = torch.jit.load(\"./data/quant_jit_model.pth\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "d245cf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quant jit model avg time: 0.0025585005400003864\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "for _ in range(n):\n",
    "    for img, label in data_loader:\n",
    "        quantized_jit_model(img)\n",
    "print(f\"quant jit model avg time: {(perf_counter() - start) / (n * len(data_loader))}\")\n",
    "\n",
    "# 29%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e60dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
