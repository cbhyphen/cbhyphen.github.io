{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "title: \"Speeding up Inference with TensorRT\"\n",
        "author: \"chris\"\n",
        "date: 2024-05-27\n",
        "draft: false\n",
        "---"
      ],
      "metadata": {
        "id": "A9dd8Ap9hTgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "follow up post to pytorch quantization ... can we make it faster with GPU and TensorRT"
      ],
      "metadata": {
        "id": "BT5j7sb7gPJp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "62hA1w2emFWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random colab error: \"A UTF-8 locale is required. Got ANSI_X3.4-1968\"\n",
        "# https://github.com/googlecolab/colabtools/issues/3409\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "n--1mEV8YsiY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get FasterRCNN as before with a resnet101 backbone..."
      ],
      "metadata": {
        "id": "QDiHrmX7GOtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "import torch\n",
        "from torchvision.models.resnet import ResNet, Bottleneck, ResNet101_Weights\n",
        "from torchvision.models._utils import IntermediateLayerGetter\n",
        "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
        "\n",
        "\n",
        "def resnet_101():\n",
        "    resnet = ResNet(block=Bottleneck, layers=[3, 4, 23, 3])\n",
        "    resnet.load_state_dict(ResNet101_Weights.DEFAULT.get_state_dict(progress=True))\n",
        "    return resnet\n",
        "\n",
        "\n",
        "resnet = resnet_101()\n",
        "\n",
        "# same as before, get intermediate layers and their output dimensions\n",
        "returned_layers = [1, 2, 3, 4]\n",
        "return_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}\n",
        "in_channels_list = []\n",
        "for k1, m1 in resnet.named_children():\n",
        "    if 'layer' in k1:\n",
        "        in_channels_list.append((m1[-1].bn3.num_features))\n",
        "\n",
        "rcnn = FasterRCNN(\n",
        "    BackboneWithFPN(\n",
        "        backbone=resnet,\n",
        "        return_layers=return_layers,\n",
        "        in_channels_list=in_channels_list,\n",
        "        out_channels=256,\n",
        "        extra_blocks=None,\n",
        "        norm_layer=None,\n",
        "        ),\n",
        "    num_classes=2\n",
        ")\n",
        "\n",
        "rcnn.eval()"
      ],
      "metadata": {
        "id": "d4JoXjgZyB_D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "time the RCNN on both CPU and GPU.  I don't recall what the specs were the last time I used colab to profile the inference time so I'll document that here as well.  I'm using a T4 GPU and the following CPU"
      ],
      "metadata": {
        "id": "CpEzZqORGVmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !cat /proc/cpuinfo  | grep 'name' | uniq\n",
        "!lscpu | grep 'name'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OLr1uWrGsct",
        "outputId": "fa2236d5-0840-479b-abff-b1bae1ea82d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEOPFQatLmMy",
        "outputId": "fc5a4674-a2ca-49d8-dbc9-db204c3a9e6f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: NVIDIA L4 (UUID: GPU-393b8fe1-1ca8-7aaf-94b9-04eef8e2fda5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# random image\n",
        "image = torch.rand(3, 200, 200)\n",
        "# put on CPU\n",
        "rcnn.to(torch.device('cpu'))\n",
        "image_cpu = image.to(torch.device('cpu'))\n",
        "\n",
        "with torch.no_grad():\n",
        "    cpu_time = %timeit -o rcnn([image_cpu])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwJ7sfCI7sjl",
        "outputId": "0f904955-6634-4ed8-cd69-3f27840ada77"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.47 s ± 137 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "# on GPU\n",
        "rcnn_gpu = deepcopy(rcnn).to(torch.device('cuda'))\n",
        "# rcnn.to(torch.device('cuda'))\n",
        "image_gpu = image.to(torch.device('cuda'))\n",
        "\n",
        "with torch.no_grad():\n",
        "    gpu_time = %timeit -o rcnn_gpu([image_gpu])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ksm0gZ6p72xB",
        "outputId": "26a679b0-e7f7-4185-f34c-bc6b294e984e"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37.9 ms ± 235 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can also test with half precision..."
      ],
      "metadata": {
        "id": "bg5cr_ZWNu1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rcnn_gpu_half = rcnn_gpu.half().to(torch.device('cuda'))\n",
        "input_half = image_gpu.half()\n",
        "\n",
        "with torch.no_grad():\n",
        "    gpu_half_time = %timeit -o rcnn_gpu_half([input_half])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmnqW54iM_l7",
        "outputId": "fc71979d-2d72-4c39-f0fd-4e5d22769230"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29.1 ms ± 1.13 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "also re-clock the quantized model using FX Graph Mode since it's performance is also CPU specific"
      ],
      "metadata": {
        "id": "1u_V_fCuFmLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from torch.ao.quantization import quantize_fx\n",
        "from torch.ao.quantization.qconfig_mapping import get_default_qconfig_mapping\n",
        "\n",
        "\n",
        "quant_rcnn = deepcopy(rcnn)\n",
        "\n",
        "qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")  # \"qnnpack\"\n",
        "# assume calibrated already\n",
        "quant_rcnn.eval()\n",
        "quant_rcnn.to(torch.device('cpu'))\n",
        "# prepare and quantize\n",
        "example_input = torch.randn(1, 3, 200, 200)\n",
        "quant_rcnn.backbone = quantize_fx.prepare_fx(quant_rcnn.backbone, qconfig_mapping, example_input)\n",
        "quant_rcnn.backbone = quantize_fx.convert_fx(quant_rcnn.backbone)\n",
        "\n",
        "script_module = torch.jit.script(quant_rcnn)\n",
        "script_module.save(\"./quant_rcnn.pt\")\n",
        "quant_rcnn_jit = torch.jit.load(\"./quant_rcnn.pt\", map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "wbJ0CJBaBmav"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# warmup\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore')\n",
        "    for _ in range(3):\n",
        "        __ = quant_rcnn_jit([image_cpu])\n",
        "\n",
        "with torch.no_grad():\n",
        "    quant_time = %timeit -o quant_rcnn_jit([image_cpu])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuUDsHb9Dx0c",
        "outputId": "da6c8f97-c132-47c0-d677-b81839aa1003"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "652 ms ± 81 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eLbX86tVZCg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below I convert the float model to onnx.  I went through onnx because that used to be the preferred way of converting to TensorRT.  However, the onnx conversion didn't play well with the `trtexec` command line utility for TensorRT regardless of the torch to onnx exporter used.  Below the [old torch script onnx converter](https://pytorch.org/docs/stable/onnx_torchscript.html) is used but the [newer 'dynamo' converter](https://pytorch.org/docs/stable/onnx_dynamo.html) also had issues.  Thankfully PyTorch has a very easy TensorRT API now, but I keep the ONNX model and evaluate it to see if a simple conversion offers any benefits."
      ],
      "metadata": {
        "id": "PJGIDCMxH6MU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install onnx\n",
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "Ak4nk7AmzI6h"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "torch.onnx.export(\n",
        "    deepcopy(rcnn),\n",
        "    # onnx wants a tuple of 2 or bombs:  https://github.com/zhiqwang/yolort/issues/485\n",
        "    ([torch.randn(3, 200, 200)], ),\n",
        "    \"rcnn.onnx\",\n",
        "    # do_constant_folding=True,\n",
        "    opset_version = 11,\n",
        "    verbose=False\n",
        "    )\n",
        "# make sure the onnx proto is valid\n",
        "rcnn_onnx = onnx.load(\"rcnn.onnx\")\n",
        "onnx.checker.check_model(rcnn_onnx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSVGaGV7fj69",
        "outputId": "bba7b721-159f-4c2b-ce53-ae5ca800489b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4009: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  (torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i], dtype=torch.float32)).float()))\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/ops/boxes.py:166: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/ops/boxes.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1559: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert condition, message\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/transform.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/transform.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  / torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset9.py:5858: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "run inference on onnx model, make sure outputs are as expected, then clock-it..."
      ],
      "metadata": {
        "id": "zBYYNDkcVeVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "ort_session = onnxruntime.InferenceSession(\"rcnn.onnx\", providers=[\"CPUExecutionProvider\"])\n",
        "# good to make sure inputs are as expected: '[i.name for i in ort_session.get_inputs()]'\n",
        "\n",
        "# onnx wants numpy tensor not torch tensor\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "# get a prediction.  onnx doesn't need a list input like torch model does\n",
        "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(image)}\n",
        "ort_outs = ort_session.run(None, ort_inputs)"
      ],
      "metadata": {
        "id": "1sM1p_37QgFn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# onxx outputs are list of three arrays corresponding to 'boxes', 'labels', and 'scores'\n",
        "print(\"onnx out shapes: \", [arr.shape for arr in ort_outs])\n",
        "# quant model out is tuple of (losses, outputs)\n",
        "torch_outs = __[1][0]\n",
        "print(\"torch out shapes: \", [torch_outs[k].shape for k in torch_outs])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9Oy1RW0VUnm",
        "outputId": "c63ab32c-123a-421b-e5ed-2695362647f0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "onnx out shapes:  [(100, 4), (100,), (100,)]\n",
            "torch out shapes:  [torch.Size([100, 4]), torch.Size([100]), torch.Size([100])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_time = %timeit -o ort_session.run(None, ort_inputs)\n",
        "\n",
        "# sess = onnxruntime.InferenceSession('rcnn.onnx', providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider'])\n",
        "# onnx_trt_time = %timeit -o sess.run(None, ort_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSUs1zDVWdVt",
        "outputId": "45740ebf-76f6-47eb-8f1c-dfec0fa29f55"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.05 s ± 114 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# more steps for using trtexec which has issues with rcnn input shape\n",
        "# !sudo apt-get install tensorrt\n",
        "# !pip install tensorrt\n",
        "# !ls /usr/src/tensorrt/bin  # make sure trtexec is there\n",
        "# !/usr/src/tensorrt/bin/trtexec --onnx=rcnn.onnx --saveEngine=rcnn_engine_pytorch.trt"
      ],
      "metadata": {
        "id": "QXijN-ogNnd6"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "use the handy `torch-tensorrt` package..."
      ],
      "metadata": {
        "id": "RYHMkM1ViZCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!python -m pip install torch-tensorrt"
      ],
      "metadata": {
        "id": "VDRRz-Fgj2D3"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "rcnn.to(device)"
      ],
      "metadata": {
        "id": "kIJbkDBVnrc4"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_tensorrt\n",
        "\n",
        "# need to wrap rcnn inputs in list\n",
        "inputs = [[torch.randn(3, 200, 200).to(\"cuda\")]]  # .half()]\n",
        "\n",
        "trt_model = torch_tensorrt.compile(\n",
        "    deepcopy(rcnn),\n",
        "    ir=\"torch_compile\",\n",
        "    # frontend api below complains about input shape\n",
        "    # backend=\"torch_tensorrt\",\n",
        "    inputs=inputs,\n",
        "    enabled_precisions={torch.float32},  #  {torch.half}\n",
        "    debug=True,\n",
        "    workspace_size=20 << 30,\n",
        "    min_block_size=7,\n",
        "    torch_executed_ops={},\n",
        ")"
      ],
      "metadata": {
        "id": "hkLAvXMEXBRj"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# contrary to docs, first run actually compiles model\n",
        "# https://pytorch.org/TensorRT/tutorials/_rendered_examples/dynamo/torch_compile_resnet_example.html#torch-compile-resnet\n",
        "outputs = trt_model(*inputs)"
      ],
      "metadata": {
        "id": "V8cKcrb-XO74"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trt_time = %timeit -o trt_model(*inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez1TrWKBG_tR",
        "outputId": "95cc8134-36e8-486e-8bb6-232dfb42117c"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26.1 ms ± 207 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "runtime = [\n",
        "    'cpu',\n",
        "    'quant',\n",
        "    'onnx',\n",
        "    'gpu',\n",
        "    'gpu half',\n",
        "    'tensorrt'\n",
        "    ]\n",
        "latency = [\n",
        "    cpu_time.average,\n",
        "    quant_time.average,\n",
        "    onnx_time.average,\n",
        "    gpu_time.average,\n",
        "    gpu_half_time.average,\n",
        "    trt_time.average\n",
        "    ]\n",
        "latency = [round(n, 3) for n in latency]\n",
        "\n",
        "ax.bar(runtime, latency)\n",
        "\n",
        "ax.set_ylabel('latency (ms)')\n",
        "ax.set_yscale('log')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "U94DlIWMG_le",
        "outputId": "90f3c99e-36be-4f17-b4f5-1d8a49d32e76"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "... half precision on the GPU is nearly as fast as TensorRT.. with TensorRT can also use half-precision to improve latency even more ..."
      ],
      "metadata": {
        "id": "q8omBOEnltfg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9XRpoZxVXYWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#| code-fold: true\n",
        "# plot latency for all methods (bar chart)"
      ],
      "metadata": {
        "id": "22ODTOhgdl_s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}