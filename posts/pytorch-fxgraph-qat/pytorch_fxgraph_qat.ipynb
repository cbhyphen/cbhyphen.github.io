{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "title: \"FX Graph Mode Quantization in PyTorch\"\n",
        "author: \"chris\"\n",
        "date: 2023-12-04\n",
        "date-modified: 2023-12-04\n",
        "draft: false\n",
        "---"
      ],
      "metadata": {
        "id": "XFetZnVQpei_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this post I'll be using pytorch's FX graph mode quantization to quantize an R-CNN.  in the [previous post](https://cbhyphen.github.io/posts/pytorch-eager-qat/pytorch_eager_qat.html), I used eager mode quantization which resulted in model compression as well as latency gains compared to the same model on CPU.  there are significant differences between the two quantization methods and each has pros and cons.  so here I will touch on those differences as well as demonstrate how to quantize using FX graph mode.\n",
        "\n",
        "at the time of writing this FX graph mode quantization is still a prototype feature.  this means that it's not as mature as eager mode which is a beta feature, although there does appear to be more effort on graphfx and it's even [encouraged over eager mode](https://pytorch.org/docs/stable/quantization.html#quantization-api-summary) for first time users.\n",
        "\n",
        "one major difference is that FX graph requires the network to be [symbolically traceable](https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html).  this requirement can result in [hacky modifications](https://ppwwyyxx.com/blog/2022/TorchScript-Tracing-vs-Scripting/#Recommendation) to code in the network that would otherwise be unnecessary and users have complained about this [complained about this](https://github.com/pytorch/pytorch/issues/48108).\n",
        "\n",
        "according to documentation, the biggest advantages of FX graph mode quantization are:\n",
        "\n",
        "- module fusion occurs automatically, something that could otherwise be tedious or error prone depending upon the complexity and size of your network\n",
        "- functionals and torch ops also get converted automagically.  in this case that means no need to modify the bottleneck block to use float functional as done in the last post\n",
        "- no requirement to insert quant/dequant stubs in the network which means you can avoid creating those additional wrapper classes\n",
        "\n",
        "with that out of the way, let's dive into FX graph and QAT.  as before, we'll start with creating the resnet backbone but without having to modify the bottleneck to use float functional operator."
      ],
      "metadata": {
        "id": "JqooPjZaMTPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.resnet import ResNet, Bottleneck, ResNet101_Weights\n",
        "\n",
        "\n",
        "def resnet_101():\n",
        "    resnet = ResNet(block=Bottleneck, layers=[3, 4, 23, 3])\n",
        "    resnet.load_state_dict(ResNet101_Weights.DEFAULT.get_state_dict(progress=True))\n",
        "    return resnet\n",
        "\n",
        "\n",
        "resnet = resnet_101()"
      ],
      "metadata": {
        "id": "ZE1Wc9lhVHlW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "at this point, the resnet is fully traceable.  [tracing it with an example input](https://pytorch.org/docs/stable/generated/torch.jit.trace.html) will return a `ScriptModule` which can be used to get a [representation of graph's forward method](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.code)."
      ],
      "metadata": {
        "id": "iky8pHpj41Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "traced_module = torch.jit.trace(resnet, torch.rand(1, 3, 200, 200))\n",
        "print(traced_module.code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIPvULhtLnSw",
        "outputId": "da5e316c-f773-4315-d845-17d08823d147"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def forward(self,\n",
            "    x: Tensor) -> Tensor:\n",
            "  fc = self.fc\n",
            "  avgpool = self.avgpool\n",
            "  layer4 = self.layer4\n",
            "  layer3 = self.layer3\n",
            "  layer2 = self.layer2\n",
            "  layer1 = self.layer1\n",
            "  maxpool = self.maxpool\n",
            "  relu = self.relu\n",
            "  bn1 = self.bn1\n",
            "  conv1 = self.conv1\n",
            "  _0 = (relu).forward((bn1).forward((conv1).forward(x, ), ), )\n",
            "  _1 = (layer1).forward((maxpool).forward(_0, ), )\n",
            "  _2 = (layer3).forward((layer2).forward(_1, ), )\n",
            "  _3 = (avgpool).forward((layer4).forward(_2, ), )\n",
            "  input = torch.flatten(_3, 1)\n",
            "  return (fc).forward(input, )\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "same as the eager mode preparation, the next step is to use torchvision's helper method `IntermediateLayerGetter` to extract layer outputs from the resnet to feed to the FPN."
      ],
      "metadata": {
        "id": "vsorX1lE6XaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models._utils import IntermediateLayerGetter\n",
        "\n",
        "\n",
        "returned_layers = [1, 2, 3, 4]\n",
        "return_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}\n",
        "resnet_layers = IntermediateLayerGetter(resnet, return_layers=return_layers)"
      ],
      "metadata": {
        "id": "qYGKfRvVVM-r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the result of the layer getter is a module dict which returns an ordered dict from its forward method.  if we attempt to trace using strict mode, JIT will complain because of the mutable output type\n",
        "\n",
        "```\n",
        "AttributeError\n",
        "...\n",
        "AttributeError: expected a dictionary of (method_name, input) pairs\n",
        "```\n",
        "\n",
        "this can be ignored if we [\"are sure that the container you are using in your problem is a constant structure and does not get used as control flow (if, for) conditions.\"](https://pytorch.org/docs/master/generated/torch.jit.trace.html)  since we know that the output won't change we can safely ignore this and set `strict=False`.  note that this isn't necessary for QAT preparation but it's helpful to know apriori if the parts of the model that we intend to quantize are indeed traceable."
      ],
      "metadata": {
        "id": "m-71ZJ4R6VuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "traced_module = torch.jit.trace(resnet_layers, torch.rand(1, 3, 200, 200), strict=False)\n",
        "print(traced_module.code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofSZJnPfPD72",
        "outputId": "806bf3e8-6a87-43c2-a2c8-e0beb6f8b677"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def forward(self,\n",
            "    x: Tensor) -> Dict[str, Tensor]:\n",
            "  layer4 = self.layer4\n",
            "  layer3 = self.layer3\n",
            "  layer2 = self.layer2\n",
            "  layer1 = self.layer1\n",
            "  maxpool = self.maxpool\n",
            "  relu = self.relu\n",
            "  bn1 = self.bn1\n",
            "  conv1 = self.conv1\n",
            "  _0 = (relu).forward((bn1).forward((conv1).forward(x, ), ), )\n",
            "  _1 = (layer1).forward((maxpool).forward(_0, ), )\n",
            "  _2 = (layer2).forward(_1, )\n",
            "  _3 = (layer3).forward(_2, )\n",
            "  _4 = {\"0\": _1, \"1\": _2, \"2\": _3, \"3\": (layer4).forward(_3, )}\n",
            "  return _4\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as before, we can create the backbone with FPN, however this time without any modifications.  after this, we can trace the module.  because the output is a mutable type (ordered dict) and we know it's structure will not change we again need to set strict mode to false.  note that if you don't there is a slightly different error (shown below) but the reason is the same.\n",
        "\n",
        "```\n",
        "RuntimeError: Encountering a dict at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior.\n",
        "```"
      ],
      "metadata": {
        "id": "WfdZJEqVdynn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
        "\n",
        "\n",
        "in_channels_stage2 = resnet.inplanes // 8\n",
        "in_channels_list = [in_channels_stage2 * 2 ** (i - 1) for i in returned_layers]\n",
        "out_channels = 256\n",
        "returned_layers = [1, 2, 3, 4]\n",
        "return_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}\n",
        "\n",
        "bb_fpn = BackboneWithFPN(\n",
        "    backbone=resnet,\n",
        "    return_layers=return_layers,\n",
        "    in_channels_list=in_channels_list,\n",
        "    out_channels=out_channels\n",
        ")\n",
        "\n",
        "traced_module = torch.jit.trace(bb_fpn, torch.rand(1, 3, 200, 200), strict=False)\n",
        "print(traced_module.code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a-_3X6beODd",
        "outputId": "eee2a697-18ff-47b1-9d3e-74228d8d2b0f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def forward(self,\n",
            "    x: Tensor) -> Dict[str, Tensor]:\n",
            "  fpn = self.fpn\n",
            "  body = self.body\n",
            "  _0, _1, _2, _3, = (body).forward(x, )\n",
            "  _4, _5, _6, _7, _8, = (fpn).forward(_0, _1, _2, _3, )\n",
            "  _9 = {\"0\": _4, \"1\": _5, \"2\": _6, \"3\": _7, \"pool\": _8}\n",
            "  return _9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now that we've verified the backbone with FPN is indeed traceable, we can create the R-CNN and prepare the model for QAT.  during preparation, FX graph mode will automatically insert observers and fuse modules.  the returned model is also now graph module.\n",
        "\n",
        "```\n",
        "GraphModule(\n",
        "  (activation_post_process_0): HistogramObserver(min_val=inf, max_val=-inf)\n",
        "  (body): Module(\n",
        "    (conv1): ConvBnReLU2d(\n",
        "      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
        "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
        "    )\n",
        "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
        "    (layer1): Module(\n",
        "      (0): Module(\n",
        "        (conv1): ConvBnReLU2d(\n",
        "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
        "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
        "        )\n",
        "        (conv2): ConvBnReLU2d(\n",
        "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
        "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
        "          ...\n",
        "```\n",
        "\n",
        "note that preparation now [requires an example input to determine the output types](https://pytorch.org/docs/stable/generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html).  as before, I'll freeze the first layer as well as batch norm stats"
      ],
      "metadata": {
        "id": "2K28zuB1f-QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import re\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
        "from torch.ao.quantization import quantize_fx\n",
        "from torch.ao.quantization.qconfig_mapping import get_default_qconfig_mapping\n",
        "\n",
        "\n",
        "quant_rcnn = FasterRCNN(bb_fpn, num_classes=2)\n",
        "\n",
        "example_input = torch.randn(1, 3, 200, 200)\n",
        "quant_rcnn.train()\n",
        "qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n",
        "quant_rcnn.backbone = quantize_fx.prepare_qat_fx(quant_rcnn .backbone, qconfig_mapping, example_input)\n",
        "\n",
        "quant_rcnn = quant_rcnn.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
        "\n",
        "for name, parameter in quant_rcnn.named_parameters():\n",
        "    if re.search(r\"body.conv1\", name) or re.search(r\"body.layer1\", name):\n",
        "        parameter.requires_grad = False"
      ],
      "metadata": {
        "id": "q1TL707jdlHd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as in the previous post, I'll use the PennFudan dataset from the Torchvision object detection finetuning tutorial..."
      ],
      "metadata": {
        "id": "VlJAZBq5F-pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "from torchvision.transforms import v2 as T\n",
        "\n",
        "\n",
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = read_image(img_path)\n",
        "        mask = read_image(mask_path)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = torch.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "        num_objs = len(obj_ids)\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        boxes = masks_to_boxes(masks)\n",
        "\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        image_id = idx\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Wrap sample and targets into torchvision tv_tensors:\n",
        "        img = tv_tensors.Image(img)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
        "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
        "    transforms.append(T.ToPureTensor())\n",
        "    return T.Compose(transforms)"
      ],
      "metadata": {
        "id": "t1ayZz3SVNQk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")\n",
        "\n",
        "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
        "!unzip PennFudanPed.zip -d ./"
      ],
      "metadata": {
        "id": "NSwY4qGdSlcE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "from engine import train_one_epoch, evaluate\n",
        "\n",
        "\n",
        "# train on the GPU or on the CPU, if a GPU is not available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has two classes only - background and person\n",
        "num_classes = 2\n",
        "# use our dataset and defined transformations\n",
        "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=1,\n",
        "    collate_fn=utils.collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "shU18MbuHYID"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# move model to the right device\n",
        "quant_rcnn.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in quant_rcnn.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params,\n",
        "    lr=0.005,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=3,\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "# let's train it for 10 epochs\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(quant_rcnn, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(quant_rcnn, data_loader_test, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "pLaEfp_yGe0W",
        "outputId": "cd21b919-1a03-4f27-e8e2-ea94c2023c90"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0]  [  0/120]  eta: 0:16:57  lr: 0.000047  loss: 3.1833 (3.1833)  loss_classifier: 2.4029 (2.4029)  loss_box_reg: 0.0138 (0.0138)  loss_objectness: 0.7201 (0.7201)  loss_rpn_box_reg: 0.0465 (0.0465)  time: 8.4778  data: 0.1050  max mem: 3467\n",
            "Epoch: [0]  [ 10/120]  eta: 0:02:32  lr: 0.000467  loss: 0.7261 (1.0924)  loss_classifier: 0.0578 (0.4519)  loss_box_reg: 0.0027 (0.0079)  loss_objectness: 0.6478 (0.6082)  loss_rpn_box_reg: 0.0206 (0.0244)  time: 1.3850  data: 0.0124  max mem: 4112\n",
            "Epoch: [0]  [ 20/120]  eta: 0:01:43  lr: 0.000886  loss: 0.5282 (0.7883)  loss_classifier: 0.0766 (0.2908)  loss_box_reg: 0.0388 (0.0545)  loss_objectness: 0.3719 (0.4203)  loss_rpn_box_reg: 0.0179 (0.0228)  time: 0.6608  data: 0.0033  max mem: 4112\n",
            "Epoch: [0]  [ 30/120]  eta: 0:01:21  lr: 0.001306  loss: 0.4415 (0.6330)  loss_classifier: 0.0868 (0.2373)  loss_box_reg: 0.1074 (0.0661)  loss_objectness: 0.1051 (0.3091)  loss_rpn_box_reg: 0.0168 (0.0206)  time: 0.6503  data: 0.0034  max mem: 4112\n",
            "Epoch: [0]  [ 40/120]  eta: 0:01:08  lr: 0.001726  loss: 0.2593 (0.5681)  loss_classifier: 0.1061 (0.2116)  loss_box_reg: 0.0974 (0.0813)  loss_objectness: 0.0565 (0.2530)  loss_rpn_box_reg: 0.0171 (0.0222)  time: 0.6693  data: 0.0032  max mem: 4112\n",
            "Epoch: [0]  [ 50/120]  eta: 0:00:56  lr: 0.002146  loss: 0.2338 (0.5130)  loss_classifier: 0.0974 (0.1916)  loss_box_reg: 0.0979 (0.0869)  loss_objectness: 0.0525 (0.2135)  loss_rpn_box_reg: 0.0125 (0.0209)  time: 0.6621  data: 0.0034  max mem: 4112\n",
            "Epoch: [0]  [ 60/120]  eta: 0:00:47  lr: 0.002565  loss: 0.2338 (0.4915)  loss_classifier: 0.0902 (0.1859)  loss_box_reg: 0.0944 (0.0954)  loss_objectness: 0.0451 (0.1887)  loss_rpn_box_reg: 0.0151 (0.0214)  time: 0.6447  data: 0.0035  max mem: 4112\n",
            "Epoch: [0]  [ 70/120]  eta: 0:00:38  lr: 0.002985  loss: 0.1885 (0.4502)  loss_classifier: 0.0733 (0.1716)  loss_box_reg: 0.0657 (0.0924)  loss_objectness: 0.0370 (0.1670)  loss_rpn_box_reg: 0.0073 (0.0192)  time: 0.6372  data: 0.0032  max mem: 4112\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6b67222384a3>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_rcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     )\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<eval_with_key>.8\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mactivation_post_process_59\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_post_process_59\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_layer3_6_conv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mbody_layer3_6_conv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mbody_layer3_6_conv3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"6\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_post_process_59\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mactivation_post_process_59\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mactivation_post_process_60\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_post_process_60\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_layer3_6_conv3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mbody_layer3_6_conv3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0madd_13\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_post_process_60\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mactivation_post_process_57\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mactivation_post_process_60\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_post_process_57\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mbody_layer3_6_relu_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"6\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0madd_13\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_orig)\u001b[0m\n\u001b[1;32m   1180\u001b[0m             )\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcombined_min\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmin_val\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcombined_max\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m                 \u001b[0mcombined_histogram\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 combined_histogram = self._combine_histograms(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1680\u001b[0m     \u001b[0;31m# See full discussion on the problems with returning `Union` here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0;31m# https://github.com/microsoft/pyright/issues/4213\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to quantized\n",
        "quant_rcnn.to(torch.device('cpu'))\n",
        "quant_rcnn.eval()\n",
        "quant_rcnn.backbone = quantize_fx.convert_fx(quant_rcnn.backbone)"
      ],
      "metadata": {
        "id": "sAySJLzdGe2h"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import perf_counter\n",
        "\n",
        "\n",
        "images, targets = next(iter(data_loader_test))\n",
        "images = list(img.to(torch.device('cpu')) for img in images)\n",
        "n = 10\n",
        "\n",
        "start = perf_counter()\n",
        "for _ in range(n):\n",
        "    __ = quant_rcnn(images)\n",
        "print(f\"quant model avg time: {(perf_counter() - start) / n:.2f}\")"
      ],
      "metadata": {
        "id": "Gbf1-vKsljfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5bdacc-a93d-428d-f619-b65f3e84aecd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quant model avg time: 1.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Y05PNkZljiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "because the model is symbolically traceable, we can save it as a torch script.  this has the advantage that it is has no torch dependencies and we don't need to instantiate the model and load the state dict as done with eager mode.  this decouples model creation with usage as long as input types are known.  the graph module is also now a recursive script module\n",
        "\n",
        "```\n",
        "RecursiveScriptModule(\n",
        "  original_name=FasterRCNN\n",
        "  (transform): RecursiveScriptModule(original_name=GeneralizedRCNNTransform)\n",
        "  (backbone): RecursiveScriptModule(\n",
        "    original_name=GraphModule\n",
        "    (body): RecursiveScriptModule(\n",
        "      original_name=Module\n",
        "      (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
        "      (maxpool): RecursiveScriptModule(original_name=MaxPool2d)\n",
        "      (layer1): RecursiveScriptModule(\n",
        "        original_name=Module\n",
        "        (0): RecursiveScriptModule(\n",
        "          original_name=Module\n",
        "          (conv1): RecursiveScriptModule(original_name=ConvReLU2d)\n",
        "          (conv2): RecursiveScriptModule(original_name=ConvReLU2d)\n",
        "          (conv3): RecursiveScriptModule(original_name=Conv2d)\n",
        "          (downsample): RecursiveScriptModule(\n",
        "            original_name=Module\n",
        "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
        "          )\n",
        "        )\n",
        "        ...\n",
        "```\n"
      ],
      "metadata": {
        "id": "RCLT1IJil27F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "script_module = torch.jit.script(quant_rcnn)\n",
        "script_module.save(\"./quant_rcnn_torchscript.pt\")\n",
        "\n",
        "quant_rcnn_jit = torch.jit.load(\"./quant_rcnn_torchscript.pt\", map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "cf9TlT1DlO4d"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO warm-up first\n",
        "\n",
        "start = perf_counter()\n",
        "for _ in range(n):\n",
        "    __ = quant_rcnn_jit(images)\n",
        "print(f\"quant model avg time: {(perf_counter() - start) / n:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Hzg77tulv2F",
        "outputId": "2ffa057e-be0b-49ab-e720-2cf4b30dd9b8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quant model avg time: 1.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fxgraph mode quantization appears to be even faster than the quantized model using eager mode.  the eager mode model had an average inference time of `1.42` and the float model `2.39` seconds."
      ],
      "metadata": {
        "id": "0yWFVJvfCoP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO full train and inference on another image"
      ],
      "metadata": {
        "id": "DC4lM5WWmmqy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}